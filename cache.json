{"2024-03-27T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03683v2","updated":"2024-03-27T16:44:22Z","published":"2023-11-07T03:19:16Z","title":"Preventing Arbitrarily High Confidence on Far-Away Data in\n  Point-Estimated Discriminative Neural Networks","summary":"  Discriminatively trained, deterministic neural networks are the de facto\nchoice for classification problems. However, even though they achieve\nstate-of-the-art results on in-domain test sets, they tend to be overconfident\non out-of-distribution (OOD) data. For instance, ReLU networks - a popular\nclass of neural network architectures - have been shown to almost always yield\nhigh confidence predictions when the test data are far away from the training\nset, even when they are trained with OOD data. We overcome this problem by\nadding a term to the output of the neural network that corresponds to the logit\nof an extra class, that we design to dominate the logits of the original\nclasses as we move away from the training data.This technique provably prevents\narbitrarily high confidence on far-away test data while maintaining a simple\ndiscriminative point-estimate training. Evaluation on various benchmarks\ndemonstrates strong performance against competitive baselines on both far-away\nand realistic OOD data.\n","authors":["Ahmad Rashid","Serena Hacker","Guojun Zhang","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2311.03683v2.pdf","comment":"Accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18735v1","updated":"2024-03-27T16:24:26Z","published":"2024-03-27T16:24:26Z","title":"Nonlinear model reduction for operator learning","summary":"  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n","authors":["Hamidreza Eivazi","Stefan Wittek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2403.18735v1.pdf","comment":"Published as a Tiny Paper at ICLR 2024 (Notable)"},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2402.07868v2","updated":"2024-03-27T16:12:43Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential\nMonte Carlo technique to infer optimal designs, and embed it into a particle\nMarkov chain Monte Carlo framework to perform gradient-based policy\namortization. Our approach is distinct from other amortized experimental design\ntechniques, as it does not rely on contrastive estimators. Numerical validation\non a set of dynamical systems showcases the efficacy of our method in\ncomparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11798v3","updated":"2024-03-27T16:12:18Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17878v2","updated":"2024-03-27T16:01:00Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18710v1","updated":"2024-03-27T15:57:42Z","published":"2024-03-27T15:57:42Z","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based\n  Model and CNN-LSTM architecture","summary":"  Recent works have attempted to use deep learning to predict future states of\ntraffic flow, but have met with mixed results. These approaches face two key\nchallenges. First, training deep learning neural networks requires large\namounts of training data which are not yet easily available for traffic flow\nsystems. Second, even when data is available, the neural networks require\naccess to historical data that covers most possible traffic flow dynamics to\nsuccessfully predict future traffic states. Specifically, these deep learning\napproaches do not fully leverage domain-knowledge about traffic flow dynamics,\ndespite a significant existing knowledge-base. In this work, we propose to\nsolve both issues using a Convolutional Neural Network (CNNs) with Long Short\nTerm Memory (LSTM) deep learning architecture to successfully predict traffic\nflow, while leveraging a cellular automata-based statistical mechanics model of\ntraffic flow to generate training and test data. Another major contribution of\nthis paper is the insight that training data for a large traffic system can\nactually be sampled from the simulations of a much smaller traffic system. This\nis achieved through observing that the normalized energy distribution of the\nstatistical mechanics model is scale invariant, which significantly eases the\nburden of data generation for large scale traffic systems. The resulting\nsimulations indicate good agreement between the predicted and the true traffic\nflow dynamics.\n","authors":["Zhaohui Yang","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.18710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18705v1","updated":"2024-03-27T15:54:55Z","published":"2024-03-27T15:54:55Z","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow\n  Matching","summary":"  In inverse problems, many conditional generative models approximate the\nposterior measure by minimizing a distance between the joint measure and its\nlearned approximation. While this approach also controls the distance between\nthe posterior measures in the case of the Kullback--Leibler divergence, this is\nin general not hold true for the Wasserstein distance. In this paper, we\nintroduce a conditional Wasserstein distance via a set of restricted couplings\nthat equals the expected Wasserstein distance of the posteriors. Interestingly,\nthe dual formulation of the conditional Wasserstein-1 flow resembles losses in\nthe conditional Wasserstein GAN literature in a quite natural way. We derive\ntheoretical properties of the conditional Wasserstein distance, characterize\nthe corresponding geodesics and velocity fields as well as the flow ODEs.\nSubsequently, we propose to approximate the velocity fields by relaxing the\nconditional Wasserstein distance. Based on this, we propose an extension of OT\nFlow Matching for solving Bayesian inverse problems and demonstrate its\nnumerical advantages on an inverse problem and class-conditional image\ngeneration.\n","authors":["Jannis Chemseddine","Paul Hagemann","Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2403.18705v1.pdf","comment":"This paper supersedes arXiv:2310.13433"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18687v1","updated":"2024-03-27T15:34:27Z","published":"2024-03-27T15:34:27Z","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","summary":"  Neural networks were used to classify infrasound data. Two different\napproaches were compared. One based on the direct classification of time series\ndata, using a custom implementation of the InceptionTime network. For the other\napproach, we generated 2D images of the wavelet transformation of the signals,\nwhich were subsequently classified using a ResNet implementation. Choosing\nappropriate hyperparameter settings, both achieve a classification accuracy of\nabove 90 %, with the direct approach reaching 95.2 %.\n","authors":["Daniel Klenkert","Daniel Schaeffer","Julian Stauch"],"pdf_url":"https://arxiv.org/pdf/2403.18687v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.18685v1","updated":"2024-03-27T15:29:08Z","published":"2024-03-27T15:29:08Z","title":"Representatividad Muestral en la Incertidumbre Simétrica Multivariada\n  para la Selección de Atributos","summary":"  In this work, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. In this thesis, through observation of\nresults, it is proposed an heuristic condition that preserves good quality in\nthe MSU under different combinations of these three factors, providing a new\nuseful criterion to help drive the process of dimension reduction.\n  --\n  En el presente trabajo hemos analizado el comportamiento de una versi\\'on\nmultivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de\nsimulaci\\'on estad\\'isticas sobre varias combinaciones de atributos\ninformativos y no-informativos generados de forma aleatoria. Los experimentos\nmuestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no\nmuestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on\nde resultados hemos propuesto una condici\\'on que preserva una buena calidad en\nel MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual\nprovee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on\nde dimensionalidad.\n","authors":["Gustavo Sosa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2403.18685v1.pdf","comment":"52 pages, in Spanish. Advisors: Miguel Garc\\'ia-Torres, Santiago\n  G\\'omez-Guerrero, Christian E. Schaerer Serra"},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12882v2","updated":"2024-03-27T14:47:41Z","published":"2023-08-23T17:42:00Z","title":"LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition","summary":"  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n","authors":["Sayanton V. Dibbo","Juston S. Moore","Garrett T. Kenyon","Michael A. Teti"],"pdf_url":"https://arxiv.org/pdf/2308.12882v2.pdf","comment":"Accepted at 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18635v1","updated":"2024-03-27T14:40:25Z","published":"2024-03-27T14:40:25Z","title":"Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features","summary":"  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n","authors":["Leonardo Pepino","Pablo Riera","Luciana Ferrer","Agustin Gravano"],"pdf_url":"https://arxiv.org/pdf/2403.18635v1.pdf","comment":"5 pages. Accepted in ICASSP 2020"},{"id":"http://arxiv.org/abs/2403.18631v1","updated":"2024-03-27T14:38:02Z","published":"2024-03-27T14:38:02Z","title":"First Experiences with the Identification of People at Risk for Diabetes\n  in Argentina using Machine Learning Techniques","summary":"  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for\nmedicine due to the absence of pathogenic symptoms and the lack of known\nassociated risk factors. Even though some proposals for machine learning models\nenable the identification of people at risk, the nature of the condition makes\nit so that a model suitable for one population may not necessarily be suitable\nfor another. In this article, the development and assessment of predictive\nmodels to identify people at risk for T2D and PD specifically in Argentina are\ndiscussed. First, the database was thoroughly preprocessed and three specific\ndatasets were generated considering a compromise between the number of records\nand the amount of available variables. After applying 5 different\nclassification models, the results obtained show that a very good performance\nwas observed for two datasets with some of these models. In particular, RF, DT,\nand ANN demonstrated great classification power, with good values for the\nmetrics under consideration. Given the lack of this type of tool in Argentina,\nthis work represents the first step towards the development of more\nsophisticated models.\n","authors":["Enzo Rucci","Gonzalo Tittarelli","Franco Ronchetti","Jorge F. Elgart","Laura Lanzarini","Juan José Gagliardino"],"pdf_url":"https://arxiv.org/pdf/2403.18631v1.pdf","comment":"Accepted for publication in Computer Science - CACIC 2023"},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v1","updated":"2024-03-27T14:28:44Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18597v1","updated":"2024-03-27T14:20:11Z","published":"2024-03-27T14:20:11Z","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue\n  Constitutive Law and Microstructure From Digital Image Correlation\n  Measurements","summary":"  Human tissues are highly organized structures with specific collagen fiber\narrangements varying from point to point. The effects of such heterogeneity\nplay an important role for tissue function, and hence it is of critical to\ndiscover and understand the distribution of such fiber orientations from\nexperimental measurements, such as the digital image correlation data. To this\nend, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)\napproach, for data-driven constitutive modeling of heterogeneous anisotropic\nmaterials. The goal is to learn both a nonlocal constitutive law together with\nthe material microstructure, in the form of a heterogeneous fiber orientation\nfield, from loading field-displacement field measurements. To this end, we\npropose a two-phase learning approach. Firstly, we learn a homogeneous\nconstitutive law in the form of a neural network-based kernel function and a\nnonlocal bond force, to capture complex homogeneous material responses from\ndata. Then, in the second phase we reinitialize the learnt bond force and the\nkernel function, and training them together with a fiber orientation field for\neach material point. Owing to the state-based peridynamic skeleton, our\nHeteroPNO-learned material models are objective and have the balance of linear\nand angular momentum guaranteed. Moreover, the effects from heterogeneity and\nnonlinear constitutive relationship are captured by the kernel function and the\nbond force respectively, enabling physical interpretability. As a result, our\nHeteroPNO architecture can learn a constitutive model for a biological tissue\nwith anisotropic heterogeneous response undergoing large deformation regime.\nMoreover, the framework is capable to provide displacement and stress field\npredictions for new and unseen loading instances.\n","authors":["Siavash Jafarzadeh","Stewart Silling","Lu Zhang","Colton Ross","Chung-Hao Lee","S. M. Rakibur Rahman","Shuodao Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2403.18582v1","updated":"2024-03-27T14:03:41Z","published":"2024-03-27T14:03:41Z","title":"One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch","summary":"  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n","authors":["Caio Cesar Daumann","Mauro Donega","Johannes Erdmann","Massimiliano Galli","Jan Lukas Späh","Davide Valsecchi"],"pdf_url":"https://arxiv.org/pdf/2403.18582v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18579v1","updated":"2024-03-27T13:59:09Z","published":"2024-03-27T13:59:09Z","title":"On Optimizing Hyperparameters for Quantum Neural Networks","summary":"  The increasing capabilities of Machine Learning (ML) models go hand in hand\nwith an immense amount of data and computational power required for training.\nTherefore, training is usually outsourced into HPC facilities, where we have\nstarted to experience limits in scaling conventional HPC hardware, as theorized\nby Moore's law. Despite heavy parallelization and optimization efforts, current\nstate-of-the-art ML models require weeks for training, which is associated with\nan enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum\nMachine Learning (QML), can offer significant theoretical speed-ups and\nenhanced expressive power. However, training QML models requires tuning various\nhyperparameters, which is a nontrivial task and suboptimal choices can highly\naffect the trainability and performance of the models. In this study, we\nidentify the most impactful hyperparameters and collect data about the\nperformance of QML models. We compare different configurations and provide\nresearchers with performance data and concrete suggestions for hyperparameter\nselection.\n","authors":["Sabrina Herbst","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.18579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18578v1","updated":"2024-03-27T13:59:05Z","published":"2024-03-27T13:59:05Z","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","summary":"  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n","authors":["Gesine Reinert","Wenkai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12370v2","updated":"2024-03-27T13:44:21Z","published":"2023-10-18T22:34:32Z","title":"No-Regret Learning in Bilateral Trade via Global Budget Balance","summary":"  Bilateral trade models the problem of intermediating between two rational\nagents -- a seller and a buyer -- both characterized by a private valuation for\nan item they want to trade. We study the online learning version of the\nproblem, in which at each time step a new seller and buyer arrive and the\nlearner has to set prices for them without any knowledge about their\n(adversarially generated) valuations.\n  In this setting, known impossibility results rule out the existence of\nno-regret algorithms when budget balanced has to be enforced at each time step.\nIn this paper, we introduce the notion of \\emph{global budget balance}, which\nonly requires the learner to fulfill budget balance over the entire time\nhorizon. Under this natural relaxation, we provide the first no-regret\nalgorithms for adversarial bilateral trade under various feedback models.\nFirst, we show that in the full-feedback model, the learner can guarantee\n$\\tilde O(\\sqrt{T})$ regret against the best fixed prices in hindsight, and\nthat this bound is optimal up to poly-logarithmic terms. Second, we provide a\nlearning algorithm guaranteeing a $\\tilde O(T^{3/4})$ regret upper bound with\none-bit feedback, which we complement with a $\\Omega(T^{5/7})$ lower bound that\nholds even in the two-bit feedback model. Finally, we introduce and analyze an\nalternative benchmark that is provably stronger than the best fixed prices in\nhindsight and is inspired by the literature on bandits with knapsacks.\n","authors":["Martino Bernasconi","Matteo Castiglioni","Andrea Celli","Federico Fusco"],"pdf_url":"https://arxiv.org/pdf/2310.12370v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2403.18560v1","updated":"2024-03-27T13:42:14Z","published":"2024-03-27T13:42:14Z","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","summary":"  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n","authors":["Jacob Mørk","Holger Severin Bovbjerg","Gergely Kiss","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2403.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18540v1","updated":"2024-03-27T13:17:15Z","published":"2024-03-27T13:17:15Z","title":"skscope: Fast Sparsity-Constrained Optimization in Python","summary":"  Applying iterative solvers on sparsity-constrained optimization (SCO)\nrequires tedious mathematical deduction and careful programming/debugging that\nhinders these solvers' broad impact. In the paper, the library skscope is\nintroduced to overcome such an obstacle. With skscope, users can solve the SCO\nby just programming the objective function. The convenience of skscope is\ndemonstrated through two examples in the paper, where sparse linear regression\nand trend filtering are addressed with just four lines of code. More\nimportantly, skscope's efficient implementation allows state-of-the-art solvers\nto quickly attain the sparse solution regardless of the high dimensionality of\nparameter space. Numerical experiments reveal the available solvers in skscope\ncan achieve up to 80x speedup on the competing relaxation solutions obtained\nvia the benchmarked convex solver. skscope is published on the Python Package\nIndex (PyPI) and Conda, and its source code is available at:\nhttps://github.com/abess-team/skscope.\n","authors":["Zezhi Wang","Jin Zhu","Peng Chen","Huiyang Peng","Xiaoke Zhang","Anran Wang","Yu Zheng","Junxian Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18540v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18535v1","updated":"2024-03-27T13:11:34Z","published":"2024-03-27T13:11:34Z","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","summary":"  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18535v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME2024)"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18517v1","updated":"2024-03-27T12:49:14Z","published":"2024-03-27T12:49:14Z","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models","summary":"  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n","authors":["Jeremy E. Cohen","Valentin Leplat"],"pdf_url":"https://arxiv.org/pdf/2403.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18509v1","updated":"2024-03-27T12:39:16Z","published":"2024-03-27T12:39:16Z","title":"Distributed Maximum Consensus over Noisy Links","summary":"  We introduce a distributed algorithm, termed noise-robust distributed maximum\nconsensus (RD-MC), for estimating the maximum value within a multi-agent\nnetwork in the presence of noisy communication links. Our approach entails\nredefining the maximum consensus problem as a distributed optimization problem,\nallowing a solution using the alternating direction method of multipliers.\nUnlike existing algorithms that rely on multiple sets of noise-corrupted\nestimates, RD-MC employs a single set, enhancing both robustness and\nefficiency. To further mitigate the effects of link noise and improve\nrobustness, we apply moving averaging to the local estimates. Through extensive\nsimulations, we demonstrate that RD-MC is significantly more robust to\ncommunication link noise compared to existing maximum-consensus algorithms.\n","authors":["Ehsan Lari","Reza Arablouei","Naveen K. D. Venkategowda","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18509v1.pdf","comment":"5 pages, 7 figures, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08579v2","updated":"2024-03-27T12:28:02Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v2.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18494v1","updated":"2024-03-27T12:10:30Z","published":"2024-03-27T12:10:30Z","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","summary":"  We investigate the learning dynamics of fully-connected neural networks\nthrough the lens of gradient signal-to-noise ratio (SNR), examining the\nbehavior of first-order optimizers like Adam in non-convex objectives. By\ninterpreting the drift/diffusion phases in the information bottleneck theory,\nfocusing on gradient homogeneity, we identify a third phase termed ``total\ndiffusion\", characterized by equilibrium in the learning rates and homogeneous\ngradients. This phase is marked by an abrupt SNR increase, uniform residuals\nacross the sample space and the most rapid training convergence. We propose a\nresidual-based re-weighting scheme to accelerate this diffusion in quadratic\nloss functions, enhancing generalization. We also explore the information\ncompression phenomenon, pinpointing a significant saturation-induced\ncompression of activations at the total diffusion phase, with deeper layers\nexperiencing negligible information loss. Supported by experimental data on\nphysics-informed neural networks (PINNs), which underscore the importance of\ngradient homogeneity due to their PDE-based sample inter-dependence, our\nfindings suggest that recognizing phase transitions could refine ML\noptimization strategies for improved generalization.\n","authors":["Sokratis J. Anagnostopoulos","Juan Diego Toscano","Nikolaos Stergiopulos","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2403.18494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.09267v3","updated":"2024-03-27T11:11:02Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v3.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18444v1","updated":"2024-03-27T11:00:53Z","published":"2024-03-27T11:00:53Z","title":"FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization","summary":"  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n","authors":["Nicolas Mauricio Cuadrado","Roberto Alejandro Gutierrez","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18444v1.pdf","comment":"Tiny Paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2403.18439v1","updated":"2024-03-27T10:47:06Z","published":"2024-03-27T10:47:06Z","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","summary":"  The smart grid domain requires bolstering the capabilities of existing energy\nmanagement systems; Federated Learning (FL) aligns with this goal as it\ndemonstrates a remarkable ability to train models on heterogeneous datasets\nwhile maintaining data privacy, making it suitable for smart grid applications,\nwhich often involve disparate data distributions and interdependencies among\nfeatures that hinder the suitability of linear models. This paper introduces a\nframework that combines FL with a Trust Region Policy Optimization (FL TRPO)\naiming to reduce energy-associated emissions and costs. Our approach reveals\nlatent interconnections and employs personalized encoding methods to capture\nunique insights, understanding the relationships between features and optimal\nstrategies, allowing our model to generalize to previously unseen data.\nExperimental results validate the robustness of our approach, affirming its\nproficiency in effectively learning policy models for smart grid challenges.\n","authors":["Yunxiang Li","Nicolas Mauricio Cuadrado","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18439v1.pdf","comment":"ICLR 2024 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2403.18438v1","updated":"2024-03-27T10:45:16Z","published":"2024-03-27T10:45:16Z","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","summary":"  Accurate vegetation models can produce further insights into the complex\ninteraction between vegetation activity and ecosystem processes. Previous\nresearch has established that long-term trends and short-term variability of\ntemperature and precipitation affect vegetation activity. Motivated by the\nrecent success of Transformer-based Deep Learning models for medium-range\nweather forecasting, we adapt the publicly available pre-trained FourCastNet to\nmodel vegetation activity while accounting for the short-term dynamics of\nclimate variability. We investigate how the learned global representation of\nthe atmosphere's state can be transferred to model the normalized difference\nvegetation index (NDVI). Our model globally estimates vegetation activity at a\nresolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We\ndemonstrate that leveraging pre-trained weather models improves the NDVI\nestimates compared to learning an NDVI model from scratch. Additionally, we\ncompare our results to other recent data-driven NDVI modeling approaches from\nmachine learning and ecology literature. We further provide experimental\nevidence on how much data and training time is necessary to turn FourCastNet\ninto an effective vegetation model. Code and models will be made available upon\npublication.\n","authors":["Pascal Janetzky","Florian Gallusser","Simon Hentschel","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.18438v1.pdf","comment":"Tackling Climate Change with Machine Learning Workshop @ ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18436v1","updated":"2024-03-27T10:40:27Z","published":"2024-03-27T10:40:27Z","title":"Collaborative Active Learning in Conditional Trust Environment","summary":"  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18436v1.pdf","comment":"5 pages, 9 figures, conference"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18415v1","updated":"2024-03-27T10:06:33Z","published":"2024-03-27T10:06:33Z","title":"The Topos of Transformer Networks","summary":"  The transformer neural network has significantly out-shined all other neural\nnetwork architectures as the engine behind large language models. We provide a\ntheoretical analysis of the expressivity of the transformer architecture\nthrough the lens of topos theory. From this viewpoint, we show that many common\nneural network architectures, such as the convolutional, recurrent and graph\nconvolutional networks, can be embedded in a pretopos of piecewise-linear\nfunctions, but that the transformer necessarily lives in its topos completion.\nIn particular, this suggests that the two network families instantiate\ndifferent fragments of logic: the former are first order, whereas transformers\nare higher-order reasoners. Furthermore, we draw parallels with architecture\nsearch and gradient descent, integrating our analysis in the framework of\ncybernetic agents.\n","authors":["Mattia Jacopo Villani","Peter McBurney"],"pdf_url":"https://arxiv.org/pdf/2403.18415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2310.05723v2","updated":"2024-03-27T09:48:34Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm well matched to a real-world RL\ndeployment process. In this scenario, we aim to find the best-performing policy\nwithin a limited budget of online interactions. Previous work in the OtO\nsetting has focused on correcting for bias introduced by the policy-constraint\nmechanisms of offline RL algorithms. Such constraints keep the learned policy\nclose to the behavior policy that collected the dataset, but we show this can\nunnecessarily limit policy performance if the behavior policy is far from\noptimal. Instead, we forgo constraints and frame OtO RL as an exploration\nproblem that aims to maximize the benefit of online data-collection. We first\nstudy the major online RL exploration methods based on intrinsic rewards and\nUCB in the OtO setting, showing that intrinsic rewards add training instability\nthrough reward-function modification, and UCB methods are myopic and it is\nunclear which learned-component's ensemble to use for action selection. We then\nintroduce an algorithm for planning to go out-of-distribution (PTGOOD) that\navoids these issues. PTGOOD uses a non-myopic planning procedure that targets\nexploration in relatively high-reward regions of the state-action space\nunlikely to be visited by the behavior policy. By leveraging concepts from the\nConditional Entropy Bottleneck, PTGOOD encourages data collected online to\nprovide new information relevant to improving the final deployment policy\nwithout altering rewards. We show empirically in several continuous control\ntasks that PTGOOD significantly improves agent returns during online\nfine-tuning and avoids the suboptimal policy convergence that many of our\nbaselines exhibit in several environments.\n","authors":["Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v2.pdf","comment":"10 pages, 17 figures, preprint"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18402v1","updated":"2024-03-27T09:44:50Z","published":"2024-03-27T09:44:50Z","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for\n  Power Grid Classification Using Electric Network Frequency","summary":"  The Electric Network Frequency (ENF) serves as a unique signature inherent to\npower distribution systems. Here, a novel approach for power grid\nclassification is developed, leveraging ENF. Spectrograms are generated from\naudio and power recordings across different grids, revealing distinctive ENF\npatterns that aid in grid classification through a fusion of classifiers. Four\ntraditional machine learning classifiers plus a Convolutional Neural Network\n(CNN), optimized using Neural Architecture Search, are developed for One-vs-All\nclassification. This process generates numerous predictions per sample, which\nare then compiled and used to train a shallow multi-label neural network\nspecifically designed to model the fusion process, ultimately leading to the\nconclusive class prediction for each sample. Experimental findings reveal that\nboth validation and testing accuracy outperform those of current\nstate-of-the-art classifiers, underlining the effectiveness and robustness of\nthe proposed methodology.\n","authors":["Georgios Tzolopoulos","Christos Korgialas","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.18402v1.pdf","comment":"13th International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM)"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18393v1","updated":"2024-03-27T09:30:50Z","published":"2024-03-27T09:30:50Z","title":"Tensor-based Graph Learning with Consistency and Specificity for\n  Multi-view Clustering","summary":"  Graph learning is widely recognized as a crucial technique in multi-view\nclustering. Existing graph learning methods typically involve constructing an\nadaptive neighbor graph based on probabilistic neighbors and then learning a\nconsensus graph to for clustering, however, they are confronted with two\nlimitations. Firstly, they often rely on Euclidean distance to measure\nsimilarity when constructing the adaptive neighbor graph, which proves\ninadequate in capturing the intrinsic structure among data points in many\nreal-world scenarios. Secondly, most of these methods focus solely on consensus\ngraph, ignoring view-specific graph information. In response to the\naforementioned drawbacks, we in this paper propose a novel tensor-based graph\nlearning framework that simultaneously considers consistency and specificity\nfor multi-view clustering. Specifically, we calculate the similarity distance\non the Stiefel manifold to preserve the intrinsic structure among data points.\nBy making an assumption that the learned neighbor graph of each view comprises\nboth a consistent graph and a view-specific graph, we formulate a new\ntensor-based target graph learning paradigm. Owing to the benefits of tensor\nsingular value decomposition (t-SVD) in uncovering high-order correlations,\nthis model is capable of achieving a complete understanding of the target\ngraph. Furthermore, we develop an iterative algorithm to solve the proposed\nobjective optimization problem. Experiments conducted on real-world datasets\nhave demonstrated the superior performance of the proposed method over some\nstate-of-the-art multi-view clustering methods. The source code has been\nreleased on https://github.com/lshi91/CSTGL-Code.\n","authors":["Long Shi","Lei Cao","Yunshan Ye","Yu Zhao","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18375v1","updated":"2024-03-27T09:14:36Z","published":"2024-03-27T09:14:36Z","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via\n  Layer-Wise Model Updates","summary":"  Synchronous federated learning (FL) is a popular paradigm for collaborative\nedge learning. It typically involves a set of heterogeneous devices locally\ntraining neural network (NN) models in parallel with periodic centralized\naggregations. As some of the devices may have limited computational resources\nand varying availability, FL latency is highly sensitive to stragglers.\nConventional approaches discard incomplete intra-model updates done by\nstragglers, alter the amount of local workload and architecture, or resort to\nasynchronous settings; which all affect the trained model performance under\ntight training latency constraints. In this work, we propose straggler-aware\nlayer-wise federated learning (SALF) that leverages the optimization procedure\nof NNs via backpropagation to update the global model in a layer-wise fashion.\nSALF allows stragglers to synchronously convey partial gradients, having each\nlayer of the global model be updated independently with a different\ncontributing set of users. We provide a theoretical analysis, establishing\nconvergence guarantees for the global model under mild assumptions on the\ndistribution of the participating devices, revealing that SALF converges at the\nsame asymptotic rate as FL with no timing limitations. This insight is matched\nwith empirical observations, demonstrating the performance gains of SALF\ncompared to alternative mechanisms mitigating the device heterogeneity gap in\nFL.\n","authors":["Natalie Lang","Alejandro Cohen","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2403.18375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08533v4","updated":"2024-03-27T09:11:48Z","published":"2023-12-13T21:46:09Z","title":"World Models via Policy-Guided Trajectory Diffusion","summary":"  World models are a powerful tool for developing intelligent agents. By\npredicting the outcome of a sequence of actions, world models enable policies\nto be optimised via on-policy reinforcement learning (RL) using synthetic data,\ni.e. in \"in imagination\". Existing world models are autoregressive in that they\ninterleave predicting the next state with sampling the next action from the\npolicy. Prediction error inevitably compounds as the trajectory length grows.\nIn this work, we propose a novel world modelling approach that is not\nautoregressive and generates entire on-policy trajectories in a single pass\nthrough a diffusion model. Our approach, Policy-Guided Trajectory Diffusion\n(PolyGRAD), leverages a denoising model in addition to the gradient of the\naction distribution of the policy to diffuse a trajectory of initially random\nstates and actions into an on-policy synthetic trajectory. We analyse the\nconnections between PolyGRAD, score-based generative models, and\nclassifier-guided diffusion models. Our results demonstrate that PolyGRAD\noutperforms state-of-the-art baselines in terms of trajectory prediction error\nfor short trajectories, with the exception of autoregressive diffusion. For\nshort trajectories, PolyGRAD obtains similar errors to autoregressive\ndiffusion, but with lower computational requirements. For long trajectories,\nPolyGRAD obtains comparable performance to baselines. Our experiments\ndemonstrate that PolyGRAD enables performant policies to be trained via\non-policy RL in imagination for MuJoCo continuous control domains. Thus,\nPolyGRAD introduces a new paradigm for accurate on-policy world modelling\nwithout autoregressive sampling.\n","authors":["Marc Rigter","Jun Yamada","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2312.08533v4.pdf","comment":"Published in TMLR, March 2024"},{"id":"http://arxiv.org/abs/2102.12920v5","updated":"2024-03-27T09:07:29Z","published":"2021-02-25T15:18:13Z","title":"Emerging Trends in Federated Learning: From Model Fusion to Federated X\n  Learning","summary":"  Federated learning is a new learning paradigm that decouples data collection\nand model training via multi-party computation and model aggregation. As a\nflexible learning setting, federated learning has the potential to integrate\nwith other learning frameworks. We conduct a focused survey of federated\nlearning in conjunction with other learning algorithms. Specifically, we\nexplore various learning algorithms to improve the vanilla federated averaging\nalgorithm and review model fusion methods such as adaptive aggregation,\nregularization, clustered methods, and Bayesian methods. Following the emerging\ntrends, we also discuss federated learning in the intersection with other\nlearning paradigms, termed federated X learning, where X includes multitask\nlearning, meta-learning, transfer learning, unsupervised learning, and\nreinforcement learning. In addition to reviewing state-of-the-art studies, this\npaper also identifies key challenges and applications in this field, while also\nhighlighting promising future directions.\n","authors":["Shaoxiong Ji","Yue Tan","Teemu Saravirta","Zhiqin Yang","Yixin Liu","Lauri Vasankari","Shirui Pan","Guodong Long","Anwar Walid"],"pdf_url":"https://arxiv.org/pdf/2102.12920v5.pdf","comment":"To appear in the International Journal of Machine Learning and\n  Cybernetics"},{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18370v1","updated":"2024-03-27T09:06:36Z","published":"2024-03-27T09:06:36Z","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","summary":"  In recent years, remarkable advancements have been achieved in the field of\nimage generation, primarily driven by the escalating demand for high-quality\noutcomes across various image generation subtasks, such as inpainting,\ndenoising, and super resolution. A major effort is devoted to exploring the\napplication of super-resolution techniques to enhance the quality of\nlow-resolution images. In this context, our method explores in depth the\nproblem of ship image super resolution, which is crucial for coastal and port\nsurveillance. We investigate the opportunity given by the growing interest in\ntext-to-image diffusion models, taking advantage of the prior knowledge that\nsuch foundation models have already learned. In particular, we present a\ndiffusion-model-based architecture that leverages text conditioning during\ntraining while being class-aware, to best preserve the crucial details of the\nships during the generation of the super-resoluted image. Since the specificity\nof this task and the scarcity availability of off-the-shelf data, we also\nintroduce a large labeled ship dataset scraped from online ship images, mostly\nfrom ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method\nachieves more robust results than other deep learning models previously\nemployed for super resolution, as proven by the multiple experiments performed.\nMoreover, we investigate how this model can benefit downstream tasks, such as\nclassification and object detection, thus emphasizing practical implementation\nin a real-world scenario. Experimental results show flexibility, reliability,\nand impressive performance of the proposed framework over state-of-the-art\nmethods for different tasks. The code is available at:\nhttps://github.com/LuigiSigillo/ShipinSight .\n","authors":["Luigi Sigillo","Riccardo Fosco Gramaccioni","Alessandro Nicolosi","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.18370v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2307.13352v2","updated":"2024-03-27T09:04:04Z","published":"2023-07-25T09:14:45Z","title":"High Dimensional Distributed Gradient Descent with Arbitrary Number of\n  Byzantine Attackers","summary":"  Robust distributed learning with Byzantine failures has attracted extensive\nresearch interests in recent years. However, most of existing methods suffer\nfrom curse of dimensionality, which is increasingly serious with the growing\ncomplexity of modern machine learning models. In this paper, we design a new\nmethod that is suitable for high dimensional problems, under arbitrary number\nof Byzantine attackers. The core of our design is a direct high dimensional\nsemi-verified mean estimation method. Our idea is to identify a subspace first.\nThe components of mean value perpendicular to this subspace can be estimated\nvia gradient vectors uploaded from worker machines, while the components within\nthis subspace are estimated using auxiliary dataset. We then use our new method\nas the aggregator of distributed learning problems. Our theoretical analysis\nshows that the new method has minimax optimal statistical rates. In particular,\nthe dependence on dimensionality is significantly improved compared with\nprevious works.\n","authors":["Puning Zhao","Zhiguo Wan"],"pdf_url":"https://arxiv.org/pdf/2307.13352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10158v2","updated":"2024-03-27T08:57:20Z","published":"2024-03-15T10:01:19Z","title":"Functional Graph Convolutional Networks: A unified multi-task and\n  multi-modal learning framework to facilitate health and social-care insights","summary":"  This paper introduces a novel Functional Graph Convolutional Network (funGCN)\nframework that combines Functional Data Analysis and Graph Convolutional\nNetworks to address the complexities of multi-task and multi-modal learning in\ndigital health and longitudinal studies. With the growing importance of health\nsolutions to improve health care and social support, ensure healthy lives, and\npromote well-being at all ages, funGCN offers a unified approach to handle\nmultivariate longitudinal data for multiple entities and ensures\ninterpretability even with small sample sizes. Key innovations include\ntask-specific embedding components that manage different data types, the\nability to perform classification, regression, and forecasting, and the\ncreation of a knowledge graph for insightful data interpretation. The efficacy\nof funGCN is validated through simulation experiments and a real-data\napplication.\n","authors":["Tobia Boschi","Francesca Bonin","Rodrigo Ordonez-Hurtado","Cécile Rousseau","Alessandra Pascale","John Dinsmore"],"pdf_url":"https://arxiv.org/pdf/2403.10158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18364v1","updated":"2024-03-27T08:57:15Z","published":"2024-03-27T08:57:15Z","title":"Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR","summary":"  We investigate the problem of supporting Industrial Internet of Things user\nequipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and\nrandom traffic arrival. A deep reinforcement learning (DRL) based centralized\ndynamic scheduler for time-frequency resources is proposed to learn how to\nschedule the available communication resources among the IIoT UEs. The proposed\nscheduler leverages an RL framework to adapt to the dynamic changes in the\nwireless communication system and traffic arrivals. Moreover, a graph-based\nreduction scheme is proposed to reduce the state and action space of the RL\nframework to allow fast convergence and a better learning strategy. Simulation\nresults demonstrate the effectiveness of the proposed intelligent scheduler in\nguaranteeing the expressed intent of IIoT UEs compared to several traditional\nscheduling schemes, such as round-robin, semi-static, and heuristic approaches.\nThe proposed scheduler also outperforms the contention-free and\ncontention-based schemes in maximizing the number of successfully computed\ntasks.\n","authors":["Salwa Mostafa","Mateus P. Mota","Alvaro Valcarce","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2403.18364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17767v2","updated":"2024-03-27T08:49:19Z","published":"2024-03-26T14:54:35Z","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain\n  labeling","summary":"  This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.\n","authors":["Victor Leger","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2403.17767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18355v1","updated":"2024-03-27T08:48:16Z","published":"2024-03-27T08:48:16Z","title":"Supervised Multiple Kernel Learning approaches for multi-omics data\n  integration","summary":"  Advances in high-throughput technologies have originated an ever-increasing\navailability of omics datasets. The integration of multiple heterogeneous data\nsources is currently an issue for biology and bioinformatics. Multiple kernel\nlearning (MKL) has shown to be a flexible and valid approach to consider the\ndiverse nature of multi-omics inputs, despite being an underused tool in\ngenomic data mining.We provide novel MKL approaches based on different kernel\nfusion strategies.To learn from the meta-kernel of input kernels, we\nadaptedunsupervised integration algorithms for supervised tasks with support\nvector machines.We also tested deep learning architectures for kernel fusion\nand classification.The results show that MKL-based models can compete with more\ncomplex, state-of-the-art, supervised multi-omics integrative approaches.\nMultiple kernel learning offers a natural framework for predictive models in\nmulti-omics genomic data. Our results offer a direction for bio-data mining\nresearch and further development of methods for heterogeneous data integration.\n","authors":["Mitja Briscik","Gabriele Tazza","Marie-Agnes Dillies","László Vidács","Sébastien Dejean"],"pdf_url":"https://arxiv.org/pdf/2403.18355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18343v1","updated":"2024-03-27T08:34:39Z","published":"2024-03-27T08:34:39Z","title":"The Artificial Neural Twin -- Process Optimization and Continual\n  Learning in Distributed Process Chains","summary":"  Industrial process optimization and control is crucial to increase economic\nand ecologic efficiency. However, data sovereignty, differing goals, or the\nrequired expert knowledge for implementation impede holistic implementation.\nFurther, the increasing use of data-driven AI-methods in process models and\nindustrial sensory often requires regular fine-tuning to accommodate\ndistribution drifts. We propose the Artificial Neural Twin, which combines\nconcepts from model predictive control, deep learning, and sensor networks to\naddress these issues. Our approach introduces differentiable data fusion to\nestimate the state of distributed process steps and their dependence on input\ndata. By treating the interconnected process steps as a quasi neural-network,\nwe can backpropagate loss gradients for process optimization or model\nfine-tuning to process parameters or AI models respectively. The concept is\ndemonstrated on a virtual machine park simulated in Unity, consisting of bulk\nmaterial processes in plastic recycling.\n","authors":["Johannes Emmert","Ronald Mendez","Houman Mirzaalian Dastjerdi","Christopher Syben","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2403.18343v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.18337v1","updated":"2024-03-27T08:21:41Z","published":"2024-03-27T08:21:41Z","title":"Macroscale fracture surface segmentation via semi-supervised learning\n  considering the structural similarity","summary":"  To this date the safety assessment of materials, used for example in the\nnuclear power sector, commonly relies on a fracture mechanical analysis\nutilizing macroscopic concepts, where a global load quantity K or J is compared\nto the materials fracture toughness curve. Part of the experimental effort\ninvolved in these concepts is dedicated to the quantitative analysis of\nfracture surfaces. Within the scope of this study a methodology for the\nsemi-supervised training of deep learning models for fracture surface\nsegmentation on a macroscopic level was established. Therefore, three distinct\nand unique datasets were created to analyze the influence of structural\nsimilarity on the segmentation capability. The structural similarity differs\ndue to the assessed materials and specimen, as well as imaging-induced variance\ndue to fluctuations in image acquisition in different laboratories. The\ndatasets correspond to typical isolated laboratory conditions, complex\nreal-world circumstances, and a curated subset of the two. We implemented a\nweak-to-strong consistency regularization for semi-supervised learning. On the\nheterogeneous dataset we were able to train robust and well-generalizing models\nthat learned feature representations from images across different domains\nwithout observing a significant drop in prediction quality. Furthermore, our\napproach reduced the number of labeled images required for training by a factor\nof 6. To demonstrate the success of our method and the benefit of our approach\nfor the fracture mechanics assessment, we utilized the models for initial crack\nsize measurements with the area average method. For the laboratory setting, the\ndeep learning assisted measurements proved to have the same quality as manual\nmeasurements. For models trained on the heterogeneous dataset, very good\nmeasurement accuracies with mean deviations smaller than 1 % could be\nachieved...\n","authors":["Johannes Rosenberger","Johannes Tlatlik","Sebastian Münstermann"],"pdf_url":"https://arxiv.org/pdf/2403.18337v1.pdf","comment":"During review title changed to: Deep learning based initial crack\n  size measurements utilizing macroscale fracture surface segmentation"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18330v1","updated":"2024-03-27T08:11:25Z","published":"2024-03-27T08:11:25Z","title":"Tracking-Assisted Object Detection with Event Cameras","summary":"  Event-based object detection has recently garnered attention in the computer\nvision community due to the exceptional properties of event cameras, such as\nhigh dynamic range and no motion blur. However, feature asynchronism and\nsparsity cause invisible objects due to no relative motion to the camera,\nposing a significant challenge in the task. Prior works have studied various\nmemory mechanisms to preserve as many features as possible at the current time,\nguided by temporal clues. While these implicit-learned memories retain some\nshort-term information, they still struggle to preserve long-term features\neffectively. In this paper, we consider those invisible objects as\npseudo-occluded objects and aim to reveal their features. Firstly, we introduce\nvisibility attribute of objects and contribute an auto-labeling algorithm to\nappend additional visibility labels on an existing event camera dataset.\nSecondly, we exploit tracking strategies for pseudo-occluded objects to\nmaintain their permanence and retain their bounding boxes, even when features\nhave not been available for a very long time. These strategies can be treated\nas an explicit-learned memory guided by the tracking objective to record the\ndisplacements of objects across frames. Lastly, we propose a spatio-temporal\nfeature aggregation module to enrich the latent features and a consistency loss\nto increase the robustness of the overall pipeline. We conduct comprehensive\nexperiments to verify our method's effectiveness where still objects are\nretained but real occluded objects are discarded. The results demonstrate that\n(1) the additional visibility labels can assist in supervised training, and (2)\nour method outperforms state-of-the-art approaches with a significant\nimprovement of 7.9% absolute mAP.\n","authors":["Ting-Kang Yen","Igor Morawski","Shusil Dangi","Kai He","Chung-Yi Lin","Jia-Fong Yeh","Hung-Ting Su","Winston Hsu"],"pdf_url":"https://arxiv.org/pdf/2403.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18326v1","updated":"2024-03-27T08:07:07Z","published":"2024-03-27T08:07:07Z","title":"Privacy-Preserving Distributed Nonnegative Matrix Factorization","summary":"  Nonnegative matrix factorization (NMF) is an effective data representation\ntool with numerous applications in signal processing and machine learning.\nHowever, deploying NMF in a decentralized manner over ad-hoc networks\nintroduces privacy concerns due to the conventional approach of sharing raw\ndata among network agents. To address this, we propose a privacy-preserving\nalgorithm for fully-distributed NMF that decomposes a distributed large data\nmatrix into left and right matrix factors while safeguarding each agent's local\ndata privacy. It facilitates collaborative estimation of the left matrix factor\namong agents and enables them to estimate their respective right factors\nwithout exposing raw data. To ensure data privacy, we secure information\nexchanges between neighboring agents utilizing the Paillier cryptosystem, a\nprobabilistic asymmetric algorithm for public-key cryptography that allows\ncomputations on encrypted data without decryption. Simulation results conducted\non synthetic and real-world datasets demonstrate the effectiveness of the\nproposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc\nnetworks.\n","authors":["Ehsan Lari","Reza Arablouei","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18326v1.pdf","comment":"5 pages, 1 figure, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18322v1","updated":"2024-03-27T07:52:10Z","published":"2024-03-27T07:52:10Z","title":"Quantum Algorithms: A New Frontier in Financial Crime Prevention","summary":"  Financial crimes fast proliferation and sophistication require novel\napproaches that provide robust and effective solutions. This paper explores the\npotential of quantum algorithms in combating financial crimes. It highlights\nthe advantages of quantum computing by examining traditional and Machine\nLearning (ML) techniques alongside quantum approaches. The study showcases\nadvanced methodologies such as Quantum Machine Learning (QML) and Quantum\nArtificial Intelligence (QAI) as powerful solutions for detecting and\npreventing financial crimes, including money laundering, financial crime\ndetection, cryptocurrency attacks, and market manipulation. These quantum\napproaches leverage the inherent computational capabilities of quantum\ncomputers to overcome limitations faced by classical methods. Furthermore, the\npaper illustrates how quantum computing can support enhanced financial risk\nmanagement analysis. Financial institutions can improve their ability to\nidentify and mitigate risks, leading to more robust risk management strategies\nby exploiting the quantum advantage. This research underscores the\ntransformative impact of quantum algorithms on financial risk management. By\nembracing quantum technologies, organisations can enhance their capabilities to\ncombat evolving threats and ensure the integrity and stability of financial\nsystems.\n","authors":["Abraham Itzhak Weinberg","Alessio Faccia"],"pdf_url":"https://arxiv.org/pdf/2403.18322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18321v1","updated":"2024-03-27T07:50:45Z","published":"2024-03-27T07:50:45Z","title":"Implementation of the Principal Component Analysis onto High-Performance\n  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and\n  Comparisons","summary":"  Dimensionality reduction represents a critical preprocessing step in order to\nincrease the efficiency and the performance of many hyperspectral imaging\nalgorithms. However, dimensionality reduction algorithms, such as the Principal\nComponent Analysis (PCA), suffer from their computationally demanding nature,\nbecoming advisable for their implementation onto high-performance computer\narchitectures for applications under strict latency constraints. This work\npresents the implementation of the PCA algorithm onto two different\nhigh-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and\na Kalray manycore, uncovering a highly valuable set of tips and tricks in order\nto take full advantage of the inherent parallelism of these high-performance\ncomputing platforms, and hence, reducing the time that is required to process a\ngiven hyperspectral image. Moreover, the achieved results obtained with\ndifferent hyperspectral images have been compared with the ones that were\nobtained with a field programmable gate array (FPGA)-based implementation of\nthe PCA algorithm that has been recently published, providing, for the first\ntime in the literature, a comprehensive analysis in order to highlight the pros\nand cons of each option.\n","authors":["E. Martel","R. Lazcano","J. Lopez","D. Madroñal","R. Salvador","S. Lopez","E. Juarez","R. Guerra","C. Sanz","R. Sarmiento"],"pdf_url":"https://arxiv.org/pdf/2403.18321v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18316v1","updated":"2024-03-27T07:38:36Z","published":"2024-03-27T07:38:36Z","title":"Multi-Modal Contrastive Learning for Online Clinical Time-Series\n  Applications","summary":"  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)\ncontain a diverse set of data modalities. While prior works have successfully\nleveraged multiple modalities in supervised settings, we apply advanced\nself-supervised multi-modal contrastive learning techniques to ICU data,\nspecifically focusing on clinical notes and time-series for clinically relevant\nonline prediction tasks. We introduce a loss function Multi-Modal Neighborhood\nContrastive Loss (MM-NCL), a soft neighborhood function, and showcase the\nexcellent linear probe and zero-shot performance of our approach.\n","authors":["Fabian Baldenweg","Manuel Burger","Gunnar Rätsch","Rita Kuznetsova"],"pdf_url":"https://arxiv.org/pdf/2403.18316v1.pdf","comment":"Accepted as a Workshop Paper at TS4H@ICLR2024"},{"id":"http://arxiv.org/abs/2403.12820v2","updated":"2024-03-27T07:35:47Z","published":"2024-03-19T15:21:00Z","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","summary":"  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n","authors":["Zhiwei Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12820v2.pdf","comment":"A derivation is incomplete, and updations are being processed"},{"id":"http://arxiv.org/abs/2403.18310v1","updated":"2024-03-27T07:22:32Z","published":"2024-03-27T07:22:32Z","title":"A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites","summary":"  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n","authors":["Betim Bahtiri","Behrouz Arash","Sven Scheffler","Maximilian Jux","Raimund Rolfes"],"pdf_url":"https://arxiv.org/pdf/2403.18310v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08102"},{"id":"http://arxiv.org/abs/2310.17072v3","updated":"2024-03-27T07:04:58Z","published":"2023-10-26T00:28:37Z","title":"MMP++: Motion Manifold Primitives with Parametric Curve Models","summary":"  Motion Manifold Primitives (MMP), a manifold-based approach for encoding\nbasic motion skills, can produce diverse trajectories, enabling the system to\nadapt to unseen constraints. Nonetheless, we argue that current MMP models lack\ncrucial functionalities of movement primitives, such as temporal and via-points\nmodulation, found in traditional approaches. This shortfall primarily stems\nfrom MMP's reliance on discrete-time trajectories. To overcome these\nlimitations, we introduce Motion Manifold Primitives++ (MMP++), a new model\nthat integrates the strengths of both MMP and traditional methods by\nincorporating parametric curve representations into the MMP framework.\nFurthermore, we identify a significant challenge with MMP++: performance\ndegradation due to geometric distortions in the latent space, meaning that\nsimilar motions are not closely positioned. To address this, Isometric Motion\nManifold Primitives++ (IMMP++) is proposed to ensure the latent space\naccurately preserves the manifold's geometry. Our experimental results across\nvarious applications, including 2-DoF planar motions, 7-DoF robot arm motions,\nand SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing\nmethods in trajectory generation tasks, achieving substantial improvements in\nsome cases. Moreover, they enable the modulation of latent coordinates and\nvia-points, thereby allowing efficient online adaptation to dynamic\nenvironments.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17072v3.pdf","comment":"12 pages. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2403.18302v1","updated":"2024-03-27T06:58:01Z","published":"2024-03-27T06:58:01Z","title":"Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using\n  SDO/HMI Data and an Attention-Aided Convolutional Neural Network","summary":"  Image super-resolution has been an important subject in image processing and\nrecognition. Here, we present an attention-aided convolutional neural network\n(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to\nenhance the quality of line-of-sight (LOS) magnetograms of solar active regions\n(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and\nHeliospheric Observatory (SOHO). The ground-truth labels used for training\nSolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist\nof strong magnetic fields in which magnetic energy can suddenly be released to\nproduce extreme space weather events, such as solar flares, coronal mass\nejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which\nis stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI\nmagnetograms allow for better understanding and forecasting of violent events\nof space weather. Experimental results show that SolarCNN improves the quality\nof SOHO/MDI magnetograms in terms of the structural similarity index measure\n(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise\nratio (PSNR).\n","authors":["Chunhui Xu","Jason T. L. Wang","Haimin Wang","Haodi Jiang","Qin Li","Yasser Abduallah","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18302v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.06912v4","updated":"2024-03-27T06:57:30Z","published":"2023-02-14T08:56:50Z","title":"Regret-Based Defense in Adversarial Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable\nto small adversarial noise in observations. Such adversarial noise can have\ndisastrous consequences in safety-critical environments. For instance, a\nself-driving car receiving adversarially perturbed sensory observations about\nnearby signs (e.g., a stop sign physically altered to be perceived as a speed\nlimit sign) or objects (e.g., cars altered to be recognized as trees) can be\nfatal. Existing approaches for making RL algorithms robust to an\nobservation-perturbing adversary have focused on reactive approaches that\niteratively improve against adversarial examples generated at each iteration.\nWhile such approaches have been shown to provide improvements over regular RL\nmethods, they are reactive and can fare significantly worse if certain\ncategories of adversarial examples are not generated during training. To that\nend, we pursue a more proactive approach that relies on directly optimizing a\nwell-studied robustness measure, regret instead of expected value. We provide a\nprincipled approach that minimizes maximum regret over a \"neighborhood\" of\nobservations to the received \"observation\". Our regret criterion can be used to\nmodify existing value- and policy-based Deep RL methods. We demonstrate that\nour approaches provide a significant improvement in performance across a wide\nvariety of benchmarks against leading approaches for robust Deep RL.\n","authors":["Roman Belaire","Pradeep Varakantham","Thanh Nguyen","David Lo"],"pdf_url":"https://arxiv.org/pdf/2302.06912v4.pdf","comment":"Accepted at AAMAS 2024"},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.18296v1","updated":"2024-03-27T06:46:59Z","published":"2024-03-27T06:46:59Z","title":"GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic\n  Communication Paradigm","summary":"  Traditional approaches to semantic communication tasks rely on the knowledge\nof the signal-to-noise ratio (SNR) to mitigate channel noise. However, these\nmethods necessitate training under specific SNR conditions, entailing\nconsiderable time and computational resources. In this paper, we propose GeNet,\na Graph Neural Network (GNN)-based paradigm for semantic communication aimed at\ncombating noise, thereby facilitating Task-Oriented Communication (TOC). We\npropose a novel approach where we first transform the input data image into\ngraph structures. Then we leverage a GNN-based encoder to extract semantic\ninformation from the source data. This extracted semantic information is then\ntransmitted through the channel. At the receiver's end, a GNN-based decoder is\nutilized to reconstruct the relevant semantic information from the source data\nfor TOC. Through experimental evaluation, we show GeNet's effectiveness in\nanti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's\nperformance by varying the number of nodes, revealing its versatility as a new\nparadigm for semantic communication. Additionally, we show GeNet's robustness\nto geometric transformations by testing it with different rotation angles,\nwithout resorting to data augmentation.\n","authors":["Chunhang Zheng","Kechao Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.18269v1","updated":"2024-03-27T05:50:23Z","published":"2024-03-27T05:50:23Z","title":"Clustering Change Sign Detection by Fusing Mixture Complexity","summary":"  This paper proposes an early detection method for cluster structural changes.\nCluster structure refers to discrete structural characteristics, such as the\nnumber of clusters, when data are represented using finite mixture models, such\nas Gaussian mixture models. We focused on scenarios in which the cluster\nstructure gradually changed over time. For finite mixture models, the concept\nof mixture complexity (MC) measures the continuous cluster size by considering\nthe cluster proportion bias and overlap between clusters. In this paper, we\npropose MC fusion as an extension of MC to handle situations in which multiple\nmixture numbers are possible in a finite mixture model. By incorporating the\nfusion of multiple models, our approach accurately captured the cluster\nstructure during transitional periods of gradual change. Moreover, we introduce\na method for detecting changes in the cluster structure by examining the\ntransition of MC fusion. We demonstrate the effectiveness of our method through\nempirical analysis using both artificial and real-world datasets.\n","authors":["Kento Urano","Ryo Yuki","Kenji Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2403.18269v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2312.12558v2","updated":"2024-03-27T05:48:21Z","published":"2023-12-19T19:53:58Z","title":"Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge","summary":"  The problem of sample complexity of online reinforcement learning is often\nstudied in the literature without taking into account any partial knowledge\nabout the system dynamics that could potentially accelerate the learning\nprocess. In this paper, we study the sample complexity of online Q-learning\nmethods when some prior knowledge about the dynamics is available or can be\nlearned efficiently. We focus on systems that evolve according to an additive\ndisturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$\nrepresents the underlying system dynamics, and $W_h$ are unknown disturbances\nindependent of states and actions. In the setting of finite episodic Markov\ndecision processes with $S$ states, $A$ actions, and episode length $H$, we\npresent an optimistic Q-learning algorithm that achieves\n$\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$ regret under perfect knowledge of\n$f$, where $T$ is the total number of interactions with the system. This is in\ncontrast to the typical $\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{SAT})$ regret\nfor existing Q-learning methods. Further, if only a noisy estimate $\\hat{f}$ of\n$f$ is available, our method can learn an approximately optimal policy in a\nnumber of samples that is independent of the cardinalities of state and action\nspaces. The sub-optimality gap depends on the approximation error $\\hat{f}-f$,\nas well as the Lipschitz constant of the corresponding optimal value function.\nOur approach does not require modeling of the transition probabilities and\nenjoys the same memory complexity as model-free methods.\n","authors":["Meshal Alharbi","Mardavij Roozbehani","Munther Dahleh"],"pdf_url":"https://arxiv.org/pdf/2312.12558v2.pdf","comment":"Published in the 38th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2305.14258v2","updated":"2024-03-27T05:45:37Z","published":"2023-05-23T17:11:33Z","title":"Weakly Supervised AUC Optimization: A Unified Partial AUC Approach","summary":"  Since acquiring perfect supervision is usually difficult, real-world machine\nlearning tasks often confront inaccurate, incomplete, or inexact supervision,\ncollectively referred to as weak supervision. In this work, we present WSAUC, a\nunified framework for weakly supervised AUC optimization problems, which covers\nnoisy label learning, positive-unlabeled learning, multi-instance learning, and\nsemi-supervised learning scenarios. Within the WSAUC framework, we first frame\nthe AUC optimization problems in various weakly supervised scenarios as a\ncommon formulation of minimizing the AUC risk on contaminated sets, and\ndemonstrate that the empirical risk minimization problems are consistent with\nthe true AUC. Then, we introduce a new type of partial AUC, specifically, the\nreversed partial AUC (rpAUC), which serves as a robust training objective for\nAUC maximization in the presence of contaminated labels. WSAUC offers a\nuniversal solution for AUC optimization in various weakly supervised scenarios\nby maximizing the empirical rpAUC. Theoretical and experimental results under\nmultiple settings support the effectiveness of WSAUC on a range of weakly\nsupervised AUC optimization tasks.\n","authors":["Zheng Xie","Yu Liu","Hao-Yuan He","Ming Li","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.14258v2.pdf","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2403.18267v1","updated":"2024-03-27T05:41:50Z","published":"2024-03-27T05:41:50Z","title":"DSF-GAN: DownStream Feedback Generative Adversarial Network","summary":"  Utility and privacy are two crucial measurements of the quality of synthetic\ntabular data. While significant advancements have been made in privacy\nmeasures, generating synthetic samples with high utility remains challenging.\nTo enhance the utility of synthetic samples, we propose a novel architecture\ncalled the DownStream Feedback Generative Adversarial Network (DSF-GAN). This\napproach incorporates feedback from a downstream prediction model during\ntraining to augment the generator's loss function with valuable information.\nThus, DSF-GAN utilizes a downstream prediction task to enhance the utility of\nsynthetic samples. To evaluate our method, we tested it using two popular\ndatasets. Our experiments demonstrate improved model performance when training\non synthetic samples generated by DSF-GAN, compared to those generated by the\nsame GAN architecture without feedback. The evaluation was conducted on the\nsame validation set comprising real samples. All code and datasets used in this\nresearch will be made openly available for ease of reproduction.\n","authors":["Oriel Perets","Nadav Rappoport"],"pdf_url":"https://arxiv.org/pdf/2403.18267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18266v1","updated":"2024-03-27T05:38:48Z","published":"2024-03-27T05:38:48Z","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has emerged as an effective paradigm for\nderiving general representations from vast amounts of unlabeled data. However,\nas real-world applications continually integrate new content, the high\ncomputational and resource demands of SSL necessitate continual learning rather\nthan complete retraining. This poses a challenge in striking a balance between\nstability and plasticity when adapting to new information. In this paper, we\nemploy Centered Kernel Alignment for quantitatively analyzing model stability\nand plasticity, revealing the critical roles of batch normalization layers for\nstability and convolutional layers for plasticity. Motivated by this, we\npropose Branch-tuning, an efficient and straightforward method that achieves a\nbalance between stability and plasticity in continual SSL. Branch-tuning\nconsists of branch expansion and compression, and can be easily applied to\nvarious SSL methods without the need of modifying the original methods,\nretaining old data or models. We validate our method through incremental\nexperiments on various benchmark datasets, demonstrating its effectiveness and\npractical value in real-world scenarios. We hope our work offers new insights\nfor future continual self-supervised learning research. The code will be made\npublicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02561v2","updated":"2024-03-27T05:23:40Z","published":"2024-02-04T16:27:37Z","title":"Foundation Model Makes Clustering A Better Initialization For Cold-Start\n  Active Learning","summary":"  Active learning selects the most informative samples from the unlabelled\ndataset to annotate in the context of a limited annotation budget. While\nnumerous methods have been proposed for subsequent sample selection based on an\ninitialized model, scant attention has been paid to the indispensable phase of\nactive learning: selecting samples for model cold-start initialization. Most of\nthe previous studies resort to random sampling or naive clustering. However,\nrandom sampling is prone to fluctuation, and naive clustering suffers from\nconvergence speed, particularly when dealing with high-dimensional data such as\nimaging data. In this work, we propose to integrate foundation models with\nclustering methods to select samples for cold-start active learning\ninitialization. Foundation models refer to those trained on massive datasets by\nthe self-supervised paradigm and capable of generating informative and\ncompacted embeddings for various downstream tasks. Leveraging these embeddings\nto replace raw features such as pixel values, clustering quickly converges and\nidentifies better initial samples. For a comprehensive comparison, we included\na classic ImageNet-supervised model to acquire embeddings. Experiments on two\nclinical tasks of image classification and segmentation demonstrated that\nfoundation model-based clustering efficiently pinpointed informative initial\nsamples, leading to models showcasing enhanced performance than the baseline\nmethods. We envisage that this study provides an effective paradigm for future\ncold-start active learning.\n","authors":["Han Yuan","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2402.02561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17458v2","updated":"2024-03-27T04:54:59Z","published":"2024-03-26T07:46:27Z","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in\n  Practice","summary":"  Our paper provides empirical comparisons between recent IDSs to provide an\nobjective comparison between them to help users choose the most appropriate\nsolution based on their requirements. Our results show that no one solution is\nthe best, but is dependent on external variables such as the types of attacks,\ncomplexity, and network environment in the dataset. For example, BoT_IoT and\nStratosphere IoT datasets both capture IoT-related attacks, but the deep neural\nnetwork performed the best when tested using the BoT_IoT dataset while HELAD\nperformed the best when tested using the Stratosphere IoT dataset. So although\nwe found that a deep neural network solution had the highest average F1 scores\non tested datasets, it is not always the best-performing one. We further\ndiscuss difficulties in using IDS from literature and project repositories,\nwhich complicated drawing definitive conclusions regarding IDS selection.\n","authors":["Jake Hesford","Daniel Cheng","Alan Wan","Larry Huynh","Seungho Kim","Hyoungshick Kim","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2403.17458v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2401.16025v2","updated":"2024-03-27T04:36:17Z","published":"2024-01-29T10:17:54Z","title":"Simple Policy Optimization","summary":"  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent\nperformance in many fields, and it is considered as a simple version of TRPO\n(Trust Region Policy Optimization) algorithm. However, the ratio clipping\noperation in PPO may not always effectively enforce the trust region\nconstraints, this can be a potential factor affecting the stability of the\nalgorithm. In this paper, we propose Simple Policy Optimization (SPO)\nalgorithm, which introduces a novel clipping method for KL divergence between\nthe old and current policies. Extensive experimental results in Atari 2600\nenvironments indicate that, compared to the mainstream variants of PPO, SPO\nachieves better sample efficiency, extremely low KL divergence, and higher\npolicy entropy, and is robust to the increase in network depth or complexity.\nMore importantly, SPO maintains the simplicity of an unconstrained first-order\nalgorithm. Code is available at\nhttps://github.com/MyRepositories-hub/Simple-Policy-Optimization.\n","authors":["Zhengpeng Xie"],"pdf_url":"https://arxiv.org/pdf/2401.16025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2301.11104v4","updated":"2024-03-27T03:47:20Z","published":"2023-01-26T13:58:46Z","title":"Discovering and Mitigating Visual Biases through Keyword Explanation","summary":"  Addressing biases in computer vision models is crucial for real-world AI\ndeployments. However, mitigating visual biases is challenging due to their\nunexplainable nature, often identified indirectly through visualization or\nsample statistics, which necessitates additional human supervision for\ninterpretation. To tackle this issue, we propose the Bias-to-Text (B2T)\nframework, which interprets visual biases as keywords. Specifically, we extract\ncommon keywords from the captions of mispredicted images to identify potential\nbiases in the model. We then validate these keywords by measuring their\nsimilarity to the mispredicted images using a vision-language scoring model.\nThe keyword explanation form of visual bias offers several advantages, such as\na clear group naming for bias discovery and a natural extension for debiasing\nusing these group names. Our experiments demonstrate that B2T can identify\nknown biases, such as gender bias in CelebA, background bias in Waterbirds, and\ndistribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in\nlarger datasets, such as Dollar Street and ImageNet. For example, we discovered\na contextual bias between \"bee\" and \"flower\" in ImageNet. We also highlight\nvarious applications of B2T keywords, including debiased training, CLIP\nprompting, and model comparison.\n","authors":["Younghyun Kim","Sangwoo Mo","Minkyu Kim","Kyungmin Lee","Jaeho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2301.11104v4.pdf","comment":"CVPR 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18233v1","updated":"2024-03-27T03:39:57Z","published":"2024-03-27T03:39:57Z","title":"Benchmarking Image Transformers for Prostate Cancer Detection from\n  Ultrasound Data","summary":"  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in\nultrasound images typically employ convolutional networks (CNNs) to detect\ncancer in small regions of interest (ROI) along a needle trace region. However,\nthis approach suffers from weak labelling, since the ground-truth\nhistopathology labels do not describe the properties of individual ROIs.\nRecently, multi-scale approaches have sought to mitigate this issue by\ncombining the context awareness of transformers with a CNN feature extractor to\ndetect cancer from multiple ROIs using multiple-instance learning (MIL). In\nthis work, we present a detailed study of several image transformer\narchitectures for both ROI-scale and multi-scale classification, and a\ncomparison of the performance of CNNs and transformers for ultrasound-based\nprostate cancer classification. We also design a novel multi-objective learning\nstrategy that combines both ROI and core predictions to further mitigate label\nnoise. METHODS: We evaluate 3 image transformers on ROI-scale cancer\nclassification, then use the strongest model to tune a multi-scale classifier\nwith MIL. We train our MIL models using our novel multi-objective learning\nstrategy and compare our results to existing baselines. RESULTS: We find that\nfor both ROI-scale and multi-scale PCa detection, image transformer backbones\nlag behind their CNN counterparts. This deficit in performance is even more\nnoticeable for larger models. When using multi-objective learning, we can\nimprove performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a\nspecificity of 66.3%. CONCLUSION: Convolutional networks are better suited for\nmodelling sparse datasets of prostate ultrasounds, producing more robust\nfeatures than transformers in PCa detection. Multi-scale methods remain the\nbest architecture for this task, with multi-objective learning presenting an\neffective way to improve performance.\n","authors":["Mohamed Harmanani","Paul F. R. Wilson","Fahimeh Fooladgar","Amoon Jamzad","Mahdi Gilany","Minh Nguyen Nhat To","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2403.18233v1.pdf","comment":"early draft, 7 pages; Accepted to SPIE Medical Imaging 2024"},{"id":"http://arxiv.org/abs/2403.18228v1","updated":"2024-03-27T03:31:16Z","published":"2024-03-27T03:31:16Z","title":"Fourier or Wavelet bases as counterpart self-attention in spikformer for\n  efficient visual classification","summary":"  Energy-efficient spikformer has been proposed by integrating the biologically\nplausible spiking neural network (SNN) and artificial Transformer, whereby the\nSpiking Self-Attention (SSA) is used to achieve both higher accuracy and lower\ncomputational cost. However, it seems that self-attention is not always\nnecessary, especially in sparse spike-form calculation manners. In this paper,\nwe innovatively replace vanilla SSA (using dynamic bases calculating from Query\nand Key) with spike-form Fourier Transform, Wavelet Transform, and their\ncombinations (using fixed triangular or wavelets bases), based on a key\nhypothesis that both of them use a set of basis functions for information\ntransformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is\nproposed and verified in visual classification tasks, including both static\nimage and event-based video datasets. The FWformer can achieve comparable or\neven higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$\nfor training and $19\\%$-$70\\%$ for inference), reduced theoretical energy\nconsumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$),\ncompared to the standard spikformer. Our result indicates the continuous\nrefinement of new Transformers, that are inspired either by biological\ndiscovery (spike-form), or information theory (Fourier or Wavelet Transform),\nis promising.\n","authors":["Qingyu Wang","Duzhen Zhang","Tilelin Zhang","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18228v1.pdf","comment":"18 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2308.02557"},{"id":"http://arxiv.org/abs/2403.18223v1","updated":"2024-03-27T03:25:45Z","published":"2024-03-27T03:25:45Z","title":"A Transformer-Based Framework for Payload Malware Detection and\n  Classification","summary":"  As malicious cyber threats become more sophisticated in breaching computer\nnetworks, the need for effective intrusion detection systems (IDSs) becomes\ncrucial. Techniques such as Deep Packet Inspection (DPI) have been introduced\nto allow IDSs analyze the content of network packets, providing more context\nfor identifying potential threats. IDSs traditionally rely on using\nanomaly-based and signature-based detection techniques to detect unrecognized\nand suspicious activity. Deep learning techniques have shown great potential in\nDPI for IDSs due to their efficiency in learning intricate patterns from the\npacket content being transmitted through the network. In this paper, we propose\na revolutionary DPI algorithm based on transformers adapted for the purpose of\ndetecting malicious traffic with a classifier head. Transformers learn the\ncomplex content of sequence data and generalize them well to similar scenarios\nthanks to their self-attention mechanism. Our proposed method uses the raw\npayload bytes that represent the packet contents and is deployed as\nman-in-the-middle. The payload bytes are used to detect malicious packets and\nclassify their types. Experimental results on the UNSW-NB15 and CIC-IOT23\ndatasets demonstrate that our transformer-based model is effective in\ndistinguishing malicious from benign traffic in the test dataset, attaining an\naverage accuracy of 79\\% using binary classification and 72\\% on the\nmulti-classification experiment, both using solely payload bytes.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2403.18223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18222v1","updated":"2024-03-27T03:19:36Z","published":"2024-03-27T03:19:36Z","title":"Uncertainty-Aware Deployment of Pre-trained Language-Conditioned\n  Imitation Learning Policies","summary":"  Large-scale robotic policies trained on data from diverse tasks and robotic\nplatforms hold great promise for enabling general-purpose robots; however,\nreliable generalization to new environment conditions remains a major\nchallenge. Toward addressing this challenge, we propose a novel approach for\nuncertainty-aware deployment of pre-trained language-conditioned imitation\nlearning agents. Specifically, we use temperature scaling to calibrate these\nmodels and exploit the calibrated model to make uncertainty-aware decisions by\naggregating the local information of candidate actions. We implement our\napproach in simulation using three such pre-trained models, and showcase its\npotential to significantly enhance task completion rates. The accompanying code\nis accessible at the link:\nhttps://github.com/BobWu1998/uncertainty_quant_all.git\n","authors":["Bo Wu","Bruce D. Lee","Kostas Daniilidis","Bernadette Bucher","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2403.18222v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.03256v2","updated":"2024-03-27T03:14:14Z","published":"2023-12-06T03:09:19Z","title":"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale\n  Recommendation Models","summary":"  Recently, the growing memory demands of embedding tables in Deep Learning\nRecommendation Models (DLRMs) pose great challenges for model training and\ndeployment. Existing embedding compression solutions cannot simultaneously meet\nthree key design requirements: memory efficiency, low latency, and adaptability\nto dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,\nand Fast Embedding compression framework that addresses the above requirements.\nThe design philosophy of CAFE is to dynamically allocate more memory resources\nto important features (called hot features), and allocate less memory to\nunimportant ones. In CAFE, we propose a fast and lightweight sketch data\nstructure, named HotSketch, to capture feature importance and report hot\nfeatures in real time. For each reported hot feature, we assign it a unique\nembedding. For the non-hot features, we allow multiple features to share one\nembedding by using hash embedding technique. Guided by our design philosophy,\nwe further propose a multi-level hash embedding framework to optimize the\nembedding tables of non-hot features. We theoretically analyze the accuracy of\nHotSketch, and analyze the model convergence against deviation. Extensive\nexperiments show that CAFE significantly outperforms existing embedding\ncompression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo\nKaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The\nsource codes of CAFE are available at GitHub.\n","authors":["Hailin Zhang","Zirui Liu","Boxuan Chen","Yikai Zhao","Tong Zhao","Tong Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2312.03256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18219v1","updated":"2024-03-27T03:07:18Z","published":"2024-03-27T03:07:18Z","title":"From Two-Dimensional to Three-Dimensional Environment with Q-Learning:\n  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries","summary":"  Reinforcement learning (RL) algorithms have become indispensable tools in\nartificial intelligence, empowering agents to acquire optimal decision-making\npolicies through interactions with their environment and feedback mechanisms.\nThis study explores the performance of RL agents in both two-dimensional (2D)\nand three-dimensional (3D) environments, aiming to research the dynamics of\nlearning across different spatial dimensions. A key aspect of this\ninvestigation is the absence of pre-made libraries for learning, with the\nalgorithm developed exclusively through computational mathematics. The\nmethodological framework centers on RL principles, employing a Q-learning agent\nclass and distinct environment classes tailored to each spatial dimension. The\nresearch aims to address the question: How do reinforcement learning agents\nadapt and perform in environments of varying spatial dimensions, particularly\nin 2D and 3D settings? Through empirical analysis, the study evaluates agents'\nlearning trajectories and adaptation processes, revealing insights into the\nefficacy of RL algorithms in navigating complex, multi-dimensional spaces.\nReflections on the findings prompt considerations for future research,\nparticularly in understanding the dynamics of learning in higher-dimensional\nenvironments.\n","authors":["Ergon Cugler de Moraes Silva"],"pdf_url":"https://arxiv.org/pdf/2403.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18216v1","updated":"2024-03-27T02:59:04Z","published":"2024-03-27T02:59:04Z","title":"Minimax Optimal Fair Classification with Bounded Demographic Disparity","summary":"  Mitigating the disparate impact of statistical machine learning methods is\ncrucial for ensuring fairness. While extensive research aims to reduce\ndisparity, the effect of using a \\emph{finite dataset} -- as opposed to the\nentire population -- remains unclear. This paper explores the statistical\nfoundations of fair binary classification with two protected groups, focusing\non controlling demographic disparity, defined as the difference in acceptance\nrates between the groups. Although fairness may come at the cost of accuracy\neven with infinite data, we show that using a finite sample incurs additional\ncosts due to the need to estimate group-specific acceptance thresholds. We\nstudy the minimax optimal classification error while constraining demographic\ndisparity to a user-specified threshold. To quantify the impact of fairness\nconstraints, we introduce a novel measure called \\emph{fairness-aware excess\nrisk} and derive a minimax lower bound on this measure that all classifiers\nmust satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding\nmethod with an offset that we show attains the minimax lower bound. Our lower\nbound proofs involve several innovations. Experiments support that\nFairBayes-DDP+ controls disparity at the user-specified level, while being\nfaster and having a more favorable fairness-accuracy tradeoff than several\nbaselines.\n","authors":["Xianli Zeng","Guang Cheng","Edgar Dobriban"],"pdf_url":"https://arxiv.org/pdf/2403.18216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06427v2","updated":"2024-03-27T02:58:26Z","published":"2023-04-13T11:46:32Z","title":"In-Distribution and Out-of-Distribution Self-supervised ECG\n  Representation Learning for Arrhythmia Detection","summary":"  This paper presents a systematic investigation into the effectiveness of\nSelf-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia\ndetection. We begin by conducting a novel analysis of the data distributions on\nthree popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To\nthe best of our knowledge, our study is the first to quantitatively explore and\ncharacterize these distributions in the area. We then perform a comprehensive\nset of experiments using different augmentations and parameters to evaluate the\neffectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG\nrepresentation learning, where we observe the best performance achieved by\nSwAV. Furthermore, our analysis shows that SSL methods achieve highly\ncompetitive results to those achieved by supervised state-of-the-art methods.\nTo further assess the performance of these methods on both In-Distribution (ID)\nand Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and\ntesting experiments. Our comprehensive experiments show almost identical\nresults when comparing ID and OOD schemes, indicating that SSL techniques can\nlearn highly effective representations that generalize well across different\nOOD datasets. This finding can have major implications for ECG-based arrhythmia\ndetection. Lastly, to further analyze our results, we perform detailed\nper-disease studies on the performance of the SSL methods on the three\ndatasets.\n","authors":["Sahar Soltanieh","Javad Hashemi","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2304.06427v2.pdf","comment":"This paper has been published in the IEEE Journal of Biomedical and\n  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.\n  Hashemi and A. Etemad, \"In-Distribution and Out-of-Distribution\n  Self-Supervised ECG Representation Learning for Arrhythmia Detection,\" in\n  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.\n  789-800, Feb. 2024"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18211v1","updated":"2024-03-27T02:42:52Z","published":"2024-03-27T02:42:52Z","title":"NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual\n  Pretraining and Multi-level Modulation","summary":"  Recent fMRI-to-image approaches mainly focused on associating fMRI signals\nwith specific conditions of pre-trained diffusion models. These approaches,\nwhile producing high-quality images, capture only a limited aspect of the\ncomplex information in fMRI signals and offer little detailed control over\nimage creation. In contrast, this paper proposes to directly modulate the\ngeneration process of diffusion models using fMRI signals. Our approach,\nNeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI\ncalibrated-encoding, to tackle multi-individual pre-training for a shared\nlatent space to minimize individual difference and enable the subsequent\ncross-subject training; ii) fMRI-to-image cross-subject pre-training,\nperceptually learning to guide diffusion model with high- and low-level\nconditions across different individuals; iii) fMRI-to-image single-subject\nrefining, similar with step ii but focus on adapting to particular individual.\nNeuroPictor extracts high-level semantic features from fMRI signals that\ncharacterizing the visual stimulus and incrementally fine-tunes the diffusion\nmodel with a low-level manipulation network to provide precise structural\ninstructions. By training with over 60,000 fMRI-image pairs from various\nindividuals, our model enjoys superior fMRI-to-image decoding capacity,\nparticularly in the within-subject setting, as evidenced in benchmark datasets.\nProject page: https://jingyanghuo.github.io/neuropictor/.\n","authors":["Jingyang Huo","Yikai Wang","Xuelin Qian","Yun Wang","Chong Li","Jianfeng Feng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18209v1","updated":"2024-03-27T02:41:52Z","published":"2024-03-27T02:41:52Z","title":"Long and Short-Term Constraints Driven Safe Reinforcement Learning for\n  Autonomous Driving","summary":"  Reinforcement learning (RL) has been widely used in decision-making tasks,\nbut it cannot guarantee the agent's safety in the training process due to the\nrequirements of interaction with the environment, which seriously limits its\nindustrial applications such as autonomous driving. Safe RL methods are\ndeveloped to handle this issue by constraining the expected safety violation\ncosts as a training objective, but they still permit unsafe state occurrence,\nwhich is unacceptable in autonomous driving tasks. Moreover, these methods are\ndifficult to achieve a balance between the cost and return expectations, which\nleads to learning performance degradation for the algorithms. In this paper, we\npropose a novel algorithm based on the long and short-term constraints (LSTC)\nfor safe RL. The short-term constraint aims to guarantee the short-term state\nsafety that the vehicle explores, while the long-term constraint ensures the\noverall safety of the vehicle throughout the decision-making process. In\naddition, we develop a safe RL method with dual-constraint optimization based\non the Lagrange multiplier to optimize the training process for end-to-end\nautonomous driving. Comprehensive experiments were conducted on the MetaDrive\nsimulator. Experimental results demonstrate that the proposed method achieves\nhigher safety in continuous state and action tasks, and exhibits higher\nexploration performance in long-distance decision-making tasks compared with\nstate-of-the-art methods.\n","authors":["Xuemin Hu","Pan Chen","Yijun Wen","Bo Tang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17456v2","updated":"2024-03-27T02:39:26Z","published":"2024-03-26T07:41:54Z","title":"Imitating Cost-Constrained Behaviors in Reinforcement Learning","summary":"  Complex planning and scheduling problems have long been solved using various\noptimization or heuristic approaches. In recent years, imitation learning that\naims to learn from expert demonstrations has been proposed as a viable\nalternative to solving these problems. Generally speaking, imitation learning\nis designed to learn either the reward (or preference) model or directly the\nbehavioral policy by observing the behavior of an expert. Existing work in\nimitation learning and inverse reinforcement learning has focused on imitation\nprimarily in unconstrained settings (e.g., no limit on fuel consumed by the\nvehicle). However, in many real-world domains, the behavior of an expert is\ngoverned not only by reward (or preference) but also by constraints. For\ninstance, decisions on self-driving delivery vehicles are dependent not only on\nthe route preferences/rewards (depending on past demand data) but also on the\nfuel in the vehicle and the time available. In such problems, imitation\nlearning is challenging as decisions are not only dictated by the reward model\nbut are also dependent on a cost-constrained model. In this paper, we provide\nmultiple methods that match expert distributions in the presence of trajectory\ncost constraints through (a) Lagrangian-based method; (b) Meta-gradients to\nfind a good trade-off between expected return and minimizing constraint\nviolation; and (c) Cost-violation-based alternating gradient. We empirically\nshow that leading imitation learning approaches imitate cost-constrained\nbehaviors poorly and our meta-gradient-based approach achieves the best\nperformance.\n","authors":["Qian Shao","Pradeep Varakantham","Shih-Fen Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.17456v2.pdf","comment":"Accepted to the 34th International Conference on Automated Planning\n  and Scheduling (ICAPS-24)"},{"id":"http://arxiv.org/abs/2403.18196v1","updated":"2024-03-27T02:13:20Z","published":"2024-03-27T02:13:20Z","title":"Looking Beyond What You See: An Empirical Analysis on Subgroup\n  Intersectional Fairness for Multi-label Chest X-ray Classification Using\n  Social Determinants of Racial Health Inequities","summary":"  There has been significant progress in implementing deep learning models in\ndisease diagnosis using chest X- rays. Despite these advancements, inherent\nbiases in these models can lead to disparities in prediction accuracy across\nprotected groups. In this study, we propose a framework to achieve accurate\ndiagnostic outcomes and ensure fairness across intersectional groups in\nhigh-dimensional chest X- ray multi-label classification. Transcending\ntraditional protected attributes, we consider complex interactions within\nsocial determinants, enabling a more granular benchmark and evaluation of\nfairness. We present a simple and robust method that involves retraining the\nlast classification layer of pre-trained models using a balanced dataset across\ngroups. Additionally, we account for fairness constraints and integrate\nclass-balanced fine-tuning for multi-label settings. The evaluation of our\nmethod on the MIMIC-CXR dataset demonstrates that our framework achieves an\noptimal tradeoff between accuracy and fairness compared to baseline methods.\n","authors":["Dana Moukheiber","Saurabh Mahindre","Lama Moukheiber","Mira Moukheiber","Mingchen Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18196v1.pdf","comment":"ICCV CVAMD 2023"},{"id":"http://arxiv.org/abs/2403.18192v1","updated":"2024-03-27T02:00:18Z","published":"2024-03-27T02:00:18Z","title":"Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced\n  Samples","summary":"  Deep neural network models have demonstrated their effectiveness in\nclassifying multi-label data from various domains. Typically, they employ a\ntraining mode that combines mini-batches with optimizers, where each sample is\nrandomly selected with equal probability when constructing mini-batches.\nHowever, the intrinsic class imbalance in multi-label data may bias the model\ntowards majority labels, since samples relevant to minority labels may be\nunderrepresented in each mini-batch. Meanwhile, during the training process, we\nobserve that instances associated with minority labels tend to induce greater\nlosses. Existing heuristic batch selection methods, such as priority selection\nof samples with high contribution to the objective function, i.e., samples with\nhigh loss, have been proven to accelerate convergence while reducing the loss\nand test error in single-label data. However, batch selection methods have not\nyet been applied and validated in multi-label data. In this study, we introduce\na simple yet effective adaptive batch selection algorithm tailored to\nmulti-label deep learning models. It adaptively selects each batch by\nprioritizing hard samples related to minority labels. A variant of our method\nalso takes informative label correlations into consideration. Comprehensive\nexperiments combining five multi-label deep learning models on thirteen\nbenchmark datasets show that our method converges faster and performs better\nthan random batch selection.\n","authors":["Ao Zhou","Bin Liu","Jin Wang","Grigorios Tsoumakas"],"pdf_url":"https://arxiv.org/pdf/2403.18192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18181v1","updated":"2024-03-27T01:18:00Z","published":"2024-03-27T01:18:00Z","title":"Compression of the Koopman matrix for nonlinear physical models via\n  hierarchical clustering","summary":"  Machine learning methods allow the prediction of nonlinear dynamical systems\nfrom data alone. The Koopman operator is one of them, which enables us to\nemploy linear analysis for nonlinear dynamical systems. The linear\ncharacteristics of the Koopman operator are hopeful to understand the nonlinear\ndynamics and perform rapid predictions. The extended dynamic mode decomposition\n(EDMD) is one of the methods to approximate the Koopman operator as a\nfinite-dimensional matrix. In this work, we propose a method to compress the\nKoopman matrix using hierarchical clustering. Numerical demonstrations for the\ncart-pole model and comparisons with the conventional singular value\ndecomposition (SVD) are shown; the results indicate that the hierarchical\nclustering performs better than the naive SVD compressions.\n","authors":["Tomoya Nishikata","Jun Ohkubo"],"pdf_url":"https://arxiv.org/pdf/2403.18181v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18176v1","updated":"2024-03-27T01:05:45Z","published":"2024-03-27T01:05:45Z","title":"Mistake, Manipulation and Margin Guarantees in Online Strategic\n  Classification","summary":"  We consider an online strategic classification problem where each arriving\nagent can manipulate their true feature vector to obtain a positive predicted\nlabel, while incurring a cost that depends on the amount of manipulation. The\nlearner seeks to predict the agent's true label given access to only the\nmanipulated features. After the learner releases their prediction, the agent's\ntrue label is revealed. Previous algorithms such as the strategic perceptron\nguarantee finitely many mistakes under a margin assumption on agents' true\nfeature vectors. However, these are not guaranteed to encourage agents to be\ntruthful. Promoting truthfulness is intimately linked to obtaining adequate\nmargin on the predictions, thus we provide two new algorithms aimed at\nrecovering the maximum margin classifier in the presence of strategic agent\nbehavior. We prove convergence, finite mistake and finite manipulation\nguarantees for a variety of agent cost structures. We also provide generalized\nversions of the strategic perceptron with mistake guarantees for different\ncosts. Our numerical study on real and synthetic data demonstrates that the new\nalgorithms outperform previous ones in terms of margin, number of manipulation\nand number of mistakes.\n","authors":["Lingqing Shen","Nam Ho-Nguyen","Khanh-Hung Giang-Tran","Fatma Kılınç-Karzan"],"pdf_url":"https://arxiv.org/pdf/2403.18176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2401.17098v2","updated":"2024-03-27T00:46:26Z","published":"2024-01-30T15:29:32Z","title":"Deep Learning-Driven Approach for Handwritten Chinese Character\n  Classification","summary":"  Handwritten character recognition (HCR) is a challenging problem for machine\nlearning researchers. Unlike printed text data, handwritten character datasets\nhave more variation due to human-introduced bias. With numerous unique\ncharacter classes present, some data, such as Logographic Scripts or\nSino-Korean character sequences, bring new complications to the HCR problem.\nThe classification task on such datasets requires the model to learn\nhigh-complexity details of the images that share similar features. With recent\nadvances in computational resource availability and further computer vision\ntheory development, some research teams have effectively addressed the arising\nchallenges. Although known for achieving high accuracy while keeping the number\nof parameters small, many common approaches are still not generalizable and use\ndataset-specific solutions to achieve better results. Due to complex structure,\nexisting methods frequently prevent the solutions from gaining popularity. This\npaper proposes a highly scalable approach for detailed character image\nclassification by introducing the model architecture, data preprocessing steps,\nand testing design instructions. We also perform experiments to compare the\nperformance of our method with that of existing ones to show the improvements\nachieved.\n","authors":["Boris Kriuk","Fedor Kriuk"],"pdf_url":"https://arxiv.org/pdf/2401.17098v2.pdf","comment":"30 pages, 9 figures, 2 tables, preprint v2"},{"id":"http://arxiv.org/abs/2312.10812v2","updated":"2024-03-27T00:15:16Z","published":"2023-12-17T20:39:54Z","title":"Learning to Act without Actions","summary":"  Pre-training large models on vast amounts of web data has proven to be an\neffective approach for obtaining powerful, general models in domains such as\nlanguage and vision. However, this paradigm has not yet taken hold in\nreinforcement learning. This is because videos, the most abundant form of\nembodied behavioral data on the web, lack the action labels required by\nexisting methods for imitating behavior from demonstrations. We introduce\nLatent Action Policies (LAPO), a method for recovering latent action\ninformation, and thereby latent-action policies, world models, and inverse\ndynamics models, purely from videos. LAPO is the first method able to recover\nthe structure of the true action space just from observed dynamics, even in\nchallenging procedurally-generated environments. LAPO enables training\nlatent-action policies that can be rapidly fine-tuned into expert-level\npolicies, either offline using a small action-labeled dataset, or online with\nrewards. LAPO takes a first step towards pre-training powerful, generalist\npolicies and world models on the vast amounts of videos readily available on\nthe web.\n","authors":["Dominik Schmidt","Minqi Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10812v2.pdf","comment":"Accepted at ICLR 2024 (spotlight). The code can be found at\n  http://github.com/schmidtdominik/LAPO"}]},"2024-03-26T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.16335v2","updated":"2024-03-26T23:29:49Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v2.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"},{"id":"http://arxiv.org/abs/2308.02396v2","updated":"2024-03-26T23:17:24Z","published":"2023-07-24T17:09:40Z","title":"HOOD: Real-Time Human Presence and Out-of-Distribution Detection Using\n  FMCW Radar","summary":"  Detecting human presence indoors with millimeter-wave frequency-modulated\ncontinuous-wave (FMCW) radar faces challenges from both moving and stationary\nclutter. This work proposes a robust and real-time capable human presence and\nout-of-distribution (OOD) detection method using 60 GHz short-range FMCW radar.\nHOOD solves the human presence and OOD detection problems simultaneously in a\nsingle pipeline. Our solution relies on a reconstruction-based architecture and\nworks with radar macro and micro range-Doppler images (RDIs). HOOD aims to\naccurately detect the presence of humans in the presence or absence of moving\nand stationary disturbers. Since HOOD is also an OOD detector, it aims to\ndetect moving or stationary clutters as OOD in humans' absence and predicts the\ncurrent scene's output as \"no presence.\" HOOD performs well in diverse\nscenarios, demonstrating its effectiveness across different human activities\nand situations. On our dataset collected with a 60 GHz short-range FMCW radar,\nwe achieve an average AUROC of 94.36%. Additionally, our extensive evaluations\nand experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD\ndetection methods in terms of common OOD detection metrics. Importantly, HOOD\nalso perfectly fits on Raspberry Pi 3B+ with an ARM Cortex-A53 CPU, which\nshowcases its versatility across different hardware environments. Videos of our\nhuman presence detection experiments are available at:\nhttps://muskahya.github.io/HOOD\n","authors":["Sabri Mustafa Kahya","Muhammet Sami Yavuz","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2308.02396v2.pdf","comment":"10 pages, 2 figures, project page: https://muskahya.github.io/HOOD"},{"id":"http://arxiv.org/abs/2403.18147v1","updated":"2024-03-26T23:14:15Z","published":"2024-03-26T23:14:15Z","title":"Divide, Conquer, Combine Bayesian Decision Tree Sampling","summary":"  Decision trees are commonly used predictive models due to their flexibility\nand interpretability. This paper is directed at quantifying the uncertainty of\ndecision tree predictions by employing a Bayesian inference approach. This is\nchallenging because these approaches need to explore both the tree structure\nspace and the space of decision parameters associated with each tree structure.\nThis has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a\nMarkov Chain is constructed to provide samples from the desired Bayesian\nestimate. Importantly, the structure and the decision parameters are tightly\ncoupled; small changes in the tree structure can demand vastly different\ndecision parameters to provide accurate predictions. A challenge for existing\nMCMC approaches is proposing joint changes in both the tree structure and the\ndecision parameters that result in efficient sampling. This paper takes a\ndifferent approach, where each distinct tree structure is associated with a\nunique set of decision parameters. The proposed approach, entitled DCC-Tree, is\ninspired by the work in Zhou et al. [23] for probabilistic programs and\nCochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for\ndecision trees. Results show that DCC-Tree performs comparably to other\nHMC-based methods and better than existing Bayesian tree methods while\nimproving on consistency and reducing the per-proposal complexity.\n","authors":["Jodie A. Cochrane","Adrian Wills","Sarah J. Johnson"],"pdf_url":"https://arxiv.org/pdf/2403.18147v1.pdf","comment":"38 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.18142v1","updated":"2024-03-26T23:03:06Z","published":"2024-03-26T23:03:06Z","title":"HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded\n  Graph Neural Networks","summary":"  As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced\ninterpretability and flexibility over traditional designs. Nevertheless, they\nstill suffer from scalability challenges when it comes to the training cost.\nAlthough many methods have been proposed to address the scalability issues,\nthey mostly focus on per-iteration efficiency, without worst-case convergence\nguarantees. Moreover, those methods typically add components to or modify the\noriginal model, thus possibly breaking the interpretability of Unfolded GNNs.\nIn this paper, we propose HERTA: a High-Efficiency and Rigorous Training\nAlgorithm for Unfolded GNNs that accelerates the whole training process,\nachieving a nearly-linear time worst-case training guarantee. Crucially, HERTA\nconverges to the optimum of the original model, thus preserving the\ninterpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we\npropose a new spectral sparsification method applicable to normalized and\nregularized graph Laplacians that ensures tighter bounds for our algorithm than\nexisting spectral sparsifiers do. Experiments on real-world datasets verify the\nsuperiority of HERTA as well as its adaptability to various loss functions and\noptimizers.\n","authors":["Yongyi Yang","Jiaming Yang","Wei Hu","Michał Dereziński"],"pdf_url":"https://arxiv.org/pdf/2403.18142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2312.05677v2","updated":"2024-03-26T22:53:56Z","published":"2023-12-09T20:51:48Z","title":"Batched Low-Rank Adaptation of Foundation Models","summary":"  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.\n","authors":["Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2312.05677v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2304.01973v3","updated":"2024-03-26T22:46:10Z","published":"2023-04-04T17:31:15Z","title":"ERM++: An Improved Baseline for Domain Generalization","summary":"  Domain Generalization (DG) measures a classifier's ability to generalize to\nnew distributions of data it was not trained on. Recent work has shown that a\nhyperparameter-tuned Empirical Risk Minimization (ERM) training procedure, that\nis simply minimizing the empirical risk on the source domains, can outperform\nmost existing DG methods. ERM has achieved such strong results while only\ntuning hyper-parameters such as learning rate, weight decay, batch size, and\ndropout. However there are additional hyperparameters which further limit\noverfitting and catastrophic forgetting. We therefore focus on tuning\npreviously untuned hyper-parameters, including training amount, initialization,\nand additional regularizers. We call the resulting stronger baseline ERM++.\nERM++ improves the performance of DG by over 5% compared to prior ERM baselines\non a standard benchmark of 5 datasets with a ResNet-50 and over 15% with a\nViT-B/16, and outperforms all SOTA methods on DomainBed with both\narchitectures. We also explore the relationship between DG performance and\nsimilarity to pre-training data, and find that similarity to pre-training data\ndistributions is an important driver of performance, but that ERM++ with\nstronger initializations can deliver strong performance even on dissimilar\ndatasets.Code is released at https://github.com/piotr-teterwak/erm_plusplus.\n","authors":["Piotr Teterwak","Kuniaki Saito","Theodoros Tsiligkaridis","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2304.01973v3.pdf","comment":"An improved baseline for Domain Generalization"},{"id":"http://arxiv.org/abs/2403.18136v1","updated":"2024-03-26T22:41:41Z","published":"2024-03-26T22:41:41Z","title":"Securing GNNs: Explanation-Based Identification of Backdoored Training\n  Graphs","summary":"  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Following an initial investigation,\nwe observed that while graph-level explanations can offer limited insights,\ntheir effectiveness in detecting backdoor triggers is inconsistent and\nincomplete. To bridge this gap, we extract and transform secondary outputs of\nGNN explanation mechanisms, designing seven novel metrics that more effectively\ndetect backdoor attacks. Additionally, we develop an adaptive attack to\nrigorously evaluate our approach. We test our method on multiple benchmark\ndatasets and examine its efficacy against various attack models. Our results\nshow that our method can achieve high detection performance, marking a\nsignificant advancement in safeguarding GNNs against backdoor attacks.\n","authors":["Jane Downer","Ren Wang","Binghui Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18133v1","updated":"2024-03-26T22:28:43Z","published":"2024-03-26T22:28:43Z","title":"AE SemRL: Learning Semantic Association Rules with Autoencoders","summary":"  Association Rule Mining (ARM) is the task of learning associations among data\nfeatures in the form of logical rules. Mining association rules from\nhigh-dimensional numerical data, for example, time series data from a large\nnumber of sensors in a smart environment, is a computationally intensive task.\nIn this study, we propose an Autoencoder-based approach to learn and extract\nassociation rules from time series data (AE SemRL). Moreover, we argue that in\nthe presence of semantic information related to time series data sources,\nsemantics can facilitate learning generalizable and explainable association\nrules. Despite enriching time series data with additional semantic features, AE\nSemRL makes learning association rules from high-dimensional data feasible. Our\nexperiments show that semantic association rules can be extracted from a latent\nrepresentation created by an Autoencoder and this method has in the order of\nhundreds of times faster execution time than state-of-the-art ARM approaches in\nmany scenarios. We believe that this study advances a new way of extracting\nassociations from representations and has the potential to inspire more\nresearch in this field.\n","authors":["Erkan Karabulut","Victoria Degeler","Paul Groth"],"pdf_url":"https://arxiv.org/pdf/2403.18133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18132v1","updated":"2024-03-26T22:26:39Z","published":"2024-03-26T22:26:39Z","title":"Recommendation of data-free class-incremental learning algorithms by\n  simulating future data","summary":"  Class-incremental learning deals with sequential data streams composed of\nbatches of classes. Various algorithms have been proposed to address the\nchallenging case where samples from past classes cannot be stored. However,\nselecting an appropriate algorithm for a user-defined setting is an open\nproblem, as the relative performance of these algorithms depends on the\nincremental settings. To solve this problem, we introduce an algorithm\nrecommendation method that simulates the future data stream. Given an initial\nset of classes, it leverages generative models to simulate future classes from\nthe same visual domain. We evaluate recent algorithms on the simulated stream\nand recommend the one which performs best in the user-defined incremental\nsetting. We illustrate the effectiveness of our method on three large datasets\nusing six algorithms and six incremental settings. Our method outperforms\ncompetitive baselines, and performance is close to that of an oracle choosing\nthe best algorithm in each setting. This work contributes to facilitate the\npractical deployment of incremental learning.\n","authors":["Eva Feillet","Adrian Popescu","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2403.18132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18128v1","updated":"2024-03-26T22:17:01Z","published":"2024-03-26T22:17:01Z","title":"HealthGAT: Node Classifications in Electronic Health Records using Graph\n  Attention Networks","summary":"  While electronic health records (EHRs) are widely used across various\napplications in healthcare, most applications use the EHRs in their raw\n(tabular) format. Relying on raw or simple data pre-processing can greatly\nlimit the performance or even applicability of downstream tasks using EHRs. To\naddress this challenge, we present HealthGAT, a novel graph attention network\nframework that utilizes a hierarchical approach to generate embeddings from\nEHR, surpassing traditional graph-based methods. Our model iteratively refines\nthe embeddings for medical codes, resulting in improved EHR data analysis. We\nalso introduce customized EHR-centric auxiliary pre-training tasks to leverage\nthe rich medical knowledge embedded within the data. This approach provides a\ncomprehensive analysis of complex medical relationships and offers significant\nadvancement over standard data representation techniques. HealthGAT has\ndemonstrated its effectiveness in various healthcare scenarios through\ncomprehensive evaluations against established methodologies. Specifically, our\nmodel shows outstanding performance in node classification and downstream tasks\nsuch as predicting readmissions and diagnosis classifications.\n  Our code is available at https://github.com/healthylaife/HealthGAT\n","authors":["Fahmida Liza Piya","Mehak Gupta","Rahmatollah Beheshti"],"pdf_url":"https://arxiv.org/pdf/2403.18128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18127v1","updated":"2024-03-26T22:15:47Z","published":"2024-03-26T22:15:47Z","title":"A Correction of Pseudo Log-Likelihood Method","summary":"  Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method\nused in various fields including contextual bandits, influence maximization of\nsocial networks, and causal bandits. However, in previous literature\n\\citep{li2017provably, zhang2022online, xiong2022combinatorial,\nfeng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function\nmay not be bounded, which may result in the algorithm they proposed not\nwell-defined. In this paper, we give a counterexample that the maximum pseudo\nlog-likelihood estimation fails and then provide a solution to correct the\nalgorithms in \\citep{li2017provably, zhang2022online, xiong2022combinatorial,\nfeng2023combinatorial1, feng2023combinatorial2}.\n","authors":["Shi Feng","Nuoya Xiong","Zhijie Zhang","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18127v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18120v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"  Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.\n","authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18120v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16967v2","updated":"2024-03-26T22:00:27Z","published":"2024-03-25T17:26:08Z","title":"Visual Whole-Body Control for Legged Loco-Manipulation","summary":"  We study the problem of mobile manipulation using legged robots equipped with\nan arm, namely legged loco-manipulation. The robot legs, while usually utilized\nfor mobility, offer an opportunity to amplify the manipulation capabilities by\nconducting whole-body control. That is, the robot can control the legs and the\narm at the same time to extend its workspace. We propose a framework that can\nconduct the whole-body control autonomously with visual observations. Our\napproach, namely Visual Whole-Body Control(VBC), is composed of a low-level\npolicy using all degrees of freedom to track the end-effector manipulator\nposition and a high-level policy proposing the end-effector position based on\nvisual inputs. We train both levels of policies in simulation and perform\nSim2Real transfer for real robot deployment. We perform extensive experiments\nand show significant improvements over baselines in picking up diverse objects\nin different configurations (heights, locations, orientations) and\nenvironments. Project page: https://wholebody-b1.github.io\n","authors":["Minghuan Liu","Zixuan Chen","Xuxin Cheng","Yandong Ji","Ruihan Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16967v2.pdf","comment":"The first two authors contribute equally. Project page:\n  https://wholebody-b1.github.io"},{"id":"http://arxiv.org/abs/2310.03906v8","updated":"2024-03-26T21:48:44Z","published":"2023-10-05T21:24:54Z","title":"PyDCM: Custom Data Center Models with Reinforcement Learning for\n  Sustainability","summary":"  The increasing global emphasis on sustainability and reducing carbon\nemissions is pushing governments and corporations to rethink their approach to\ndata center design and operation. Given their high energy consumption and\nexponentially large computational workloads, data centers are prime candidates\nfor optimizing power consumption, especially in areas such as cooling and IT\nenergy usage. A significant challenge in this pursuit is the lack of a\nconfigurable and scalable thermal data center model that offers an end-to-end\npipeline. Data centers consist of multiple IT components whose geometric\nconfiguration and heat dissipation make thermal modeling difficult. This paper\npresents PyDCM, a customizable Data Center Model implemented in Python, that\nallows users to create unique configurations of IT equipment with custom server\nspecifications and geometric arrangements of IT cabinets. The use of vectorized\nthermal calculations makes PyDCM orders of magnitude faster (30 times) than\ncurrent Energy Plus modeling implementations and scales sublinearly with the\nnumber of CPUs. Also, PyDCM enables the use of Deep Reinforcement Learning via\nthe Gymnasium wrapper to optimize data center cooling and offers a\nuser-friendly platform for testing various data center design prototypes.\n","authors":["Avisek Naug","Antonio Guillen","Ricardo Luna Gutiérrez","Vineet Gundecha","Dejan Markovikj","Lekhapriya Dheeraj Kashyap","Lorenz Krause","Sahand Ghorbanpour","Sajad Mousavi","Ashwin Ramesh Babu","Soumyendu Sarkar"],"pdf_url":"https://arxiv.org/pdf/2310.03906v8.pdf","comment":"The 10th ACM International Conference on Systems for Energy-Efficient\n  Buildings, Cities, and Transportation (BuildSys '23), November 15-16, 2023,\n  Istanbul, Turkey"},{"id":"http://arxiv.org/abs/2403.18104v1","updated":"2024-03-26T21:04:18Z","published":"2024-03-26T21:04:18Z","title":"Mathematical Foundation and Corrections for Full Range Head Pose\n  Estimation","summary":"  Numerous works concerning head pose estimation (HPE) offer algorithms or\nproposed neural network-based approaches for extracting Euler angles from\neither facial key points or directly from images of the head region. However,\nmany works failed to provide clear definitions of the coordinate systems and\nEuler or Tait-Bryan angles orders in use. It is a well-known fact that rotation\nmatrices depend on coordinate systems, and yaw, roll, and pitch angles are\nsensitive to their application order. Without precise definitions, it becomes\nchallenging to validate the correctness of the output head pose and drawing\nroutines employed in prior works. In this paper, we thoroughly examined the\nEuler angles defined in the 300W-LP dataset, head pose estimation such as\n3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of\nthe Euler angles. When necessary, we infer their coordinate system and sequence\nof yaw, roll, pitch from provided code. This paper presents (1) code and\nalgorithms for inferring coordinate system from provided source code, code for\nEuler angle application order and extracting precise rotation matrices and the\nEuler angles, (2) code and algorithms for converting poses from one rotation\nsystem to another, (3) novel formulae for 2D augmentations of the rotation\nmatrices, and (4) derivations and code for the correct drawing routines for\nrotation matrices and poses. This paper also addresses the feasibility of\ndefining rotations with right-handed coordinate system in Wikipedia and SciPy,\nwhich makes the Euler angle extraction much easier for full-range head pose\nresearch.\n","authors":["Huei-Chung Hu","Xuyang Wu","Yuan Wang","Yi Fang","Hsin-Tai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18103v1","updated":"2024-03-26T21:01:41Z","published":"2024-03-26T21:01:41Z","title":"Tutorial on Diffusion Models for Imaging and Vision","summary":"  The astonishing growth of generative tools in recent years has empowered many\nexciting applications in text-to-image generation and text-to-video generation.\nThe underlying principle behind these generative tools is the concept of\ndiffusion, a particular sampling mechanism that has overcome some shortcomings\nthat were deemed difficult in the previous approaches. The goal of this\ntutorial is to discuss the essential ideas underlying the diffusion models. The\ntarget audience of this tutorial includes undergraduate and graduate students\nwho are interested in doing research on diffusion models or applying these\nmodels to solve other problems.\n","authors":["Stanley H. Chan"],"pdf_url":"https://arxiv.org/pdf/2403.18103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18101v1","updated":"2024-03-26T21:00:06Z","published":"2024-03-26T21:00:06Z","title":"Towards Explainable Clustering: A Constrained Declarative based Approach","summary":"  The domain of explainable AI is of interest in all Machine Learning fields,\nand it is all the more important in clustering, an unsupervised task whose\nresult must be validated by a domain expert. We aim at finding a clustering\nthat has high quality in terms of classic clustering criteria and that is\nexplainable, and we argue that these two dimensions must be considered when\nbuilding the clustering. We consider that a good global explanation of a\nclustering should give the characteristics of each cluster taking into account\ntheir abilities to describe its objects (coverage) while distinguishing it from\nthe other clusters (discrimination). Furthermore, we aim at leveraging expert\nknowledge, at different levels, on the structure of the expected clustering or\non its explanations. In our framework an explanation of a cluster is a set of\npatterns, and we propose a novel interpretable constrained clustering method\ncalled ECS for declarative clustering with Explainabilty-driven Cluster\nSelection that integrates structural or domain expert knowledge expressed by\nmeans of constraints. It is based on the notion of coverage and discrimination\nthat are formalized at different levels (cluster / clustering), each allowing\nfor exceptions through parameterized thresholds. Our method relies on four\nsteps: generation of a set of partitions, computation of frequent patterns for\neach cluster, pruning clusters that violates some constraints, and selection of\nclusters and associated patterns to build an interpretable clustering. This\nlast step is combinatorial and we have developed a Constraint-Programming (CP)\nmodel to solve it. The method can integrate prior knowledge in the form of user\nconstraints, both before or in the CP model.\n","authors":["Mathieu Guilbert","Christel Vrain","Thi-Bich-Hanh Dao"],"pdf_url":"https://arxiv.org/pdf/2403.18101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18100v1","updated":"2024-03-26T20:59:48Z","published":"2024-03-26T20:59:48Z","title":"Driving Intelligent IoT Monitoring and Control through Cloud Computing\n  and Machine Learning","summary":"  This article explores how to drive intelligent iot monitoring and control\nthrough cloud computing and machine learning. As iot and the cloud continue to\ngenerate large and diverse amounts of data as sensor devices in the network,\nthe collected data is sent to the cloud for statistical analysis, prediction,\nand data analysis to achieve business objectives. However, because the cloud\ncomputing model is limited by distance, it can be problematic in environments\nwhere the quality of the Internet connection is not ideal for critical\noperations. Therefore, edge computing, as a distributed computing architecture,\nmoves the location of processing applications, data and services from the\ncentral node of the network to the logical edge node of the network to reduce\nthe dependence on cloud processing and analysis of data, and achieve near-end\ndata processing and analysis. The combination of iot and edge computing can\nreduce latency, improve efficiency, and enhance security, thereby driving the\ndevelopment of intelligent systems. The paper also introduces the development\nof iot monitoring and control technology, the application of edge computing in\niot monitoring and control, and the role of machine learning in data analysis\nand fault detection. Finally, the application and effect of intelligent\nInternet of Things monitoring and control system in industry, agriculture,\nmedical and other fields are demonstrated through practical cases and\nexperimental studies.\n","authors":["Hanzhe Li","Xiangxiang Wang","Yuan Feng","Yaqian Qi","Jingxiao Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07572v3","updated":"2024-03-26T20:50:44Z","published":"2023-07-14T18:29:24Z","title":"High-Rate Phase Association with Travel Time Neural Fields","summary":"  Our understanding of regional seismicity from multi-station seismograms\nrelies on the ability to associate arrival phases with their originating\nearthquakes. Deep-learning-based phase detection now detects small, high-rate\narrivals from seismicity clouds, even at negative magnitudes. This new data\ncould give important insight into earthquake dynamics, but it is presents a\nchallenging association task. Existing techniques relying on coarsely\napproximated, fixed wave speed models fail in this unexplored dense regime\nwhere the complexity of unknown wave speed cannot be ignored. We introduce\nHarpa, a high-rate association framework built on deep generative modeling and\nneural fields. Harpa incorporates wave physics by using optimal transport to\ncompare arrival sequences. It is thus robust to unknown wave speeds and\nestimates the wave speed model as a by-product of association. Experiments with\nrealistic, complex synthetic models show that Harpa is the first seismic phase\nassociation framework which is accurate in the high-rate regime, paving the way\nfor new avenues in exploratory Earth science and improved understanding of\nseismicity.\n","authors":["Cheng Shi","Maarten V. de Hoop","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2307.07572v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18094v1","updated":"2024-03-26T20:30:55Z","published":"2024-03-26T20:30:55Z","title":"A Personalized Video-Based Hand Taxonomy: Application for Individuals\n  with Spinal Cord Injury","summary":"  Hand function is critical for our interactions and quality of life. Spinal\ncord injuries (SCI) can impair hand function, reducing independence. A\ncomprehensive evaluation of function in home and community settings requires a\nhand grasp taxonomy for individuals with impaired hand function. Developing\nsuch a taxonomy is challenging due to unrepresented grasp types in standard\ntaxonomies, uneven data distribution across injury levels, and limited data.\nThis study aims to automatically identify the dominant distinct hand grasps in\negocentric video using semantic clustering. Egocentric video recordings\ncollected in the homes of 19 individual with cervical SCI were used to cluster\ngrasping actions with semantic significance. A deep learning model integrating\nposture and appearance data was employed to create a personalized hand\ntaxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with\nwith 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful\nclusters in video content. This methodology provides a flexible and effective\nstrategy to analyze hand function in the wild. It offers researchers and\nclinicians an efficient tool for evaluating hand function, aiding sensitive\nassessments and tailored intervention plans.\n","authors":["Mehdy Dousty","David J. Fleet","José Zariffa"],"pdf_url":"https://arxiv.org/pdf/2403.18094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18079v1","updated":"2024-03-26T19:58:39Z","published":"2024-03-26T19:58:39Z","title":"Paths to Equilibrium in Normal-Form Games","summary":"  In multi-agent reinforcement learning (MARL), agents repeatedly interact\nacross time and revise their strategies as new data arrives, producing a\nsequence of strategy profiles. This paper studies sequences of strategies\nsatisfying a pairwise constraint inspired by policy updating in reinforcement\nlearning, where an agent who is best responding in period $t$ does not switch\nits strategy in the next period $t+1$. This constraint merely requires that\noptimizing agents do not switch strategies, but does not constrain the other\nnon-optimizing agents in any way, and thus allows for exploration. Sequences\nwith this property are called satisficing paths, and arise naturally in many\nMARL algorithms. A fundamental question about strategic dynamics is such: for a\ngiven game and initial strategy profile, is it always possible to construct a\nsatisficing path that terminates at an equilibrium strategy? The resolution of\nthis question has implications about the capabilities or limitations of a class\nof MARL algorithms. We answer this question in the affirmative for mixed\nextensions of finite normal-form games.%\n","authors":["Bora Yongacoglu","Gürdal Arslan","Lacra Pavel","Serdar Yüksel"],"pdf_url":"https://arxiv.org/pdf/2403.18079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18072v1","updated":"2024-03-26T19:49:58Z","published":"2024-03-26T19:49:58Z","title":"Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models\n  using Markov Chain Monte Carlo","summary":"  Optimal experimental design (OED) provides a systematic approach to quantify\nand maximize the value of experimental data. Under a Bayesian approach,\nconventional OED maximizes the expected information gain (EIG) on model\nparameters. However, we are often interested in not the parameters themselves,\nbut predictive quantities of interest (QoIs) that depend on the parameters in a\nnonlinear manner. We present a computational framework of predictive\ngoal-oriented OED (GO-OED) suitable for nonlinear observation and prediction\nmodels, which seeks the experimental design providing the greatest EIG on the\nQoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG,\nfeaturing Markov chain Monte Carlo for posterior sampling and kernel density\nestimation for evaluating the posterior-predictive density and its\nKullback-Leibler divergence from the prior-predictive. The GO-OED design is\nthen found by maximizing the EIG over the design space using Bayesian\noptimization. We demonstrate the effectiveness of the overall nonlinear GO-OED\nmethod, and illustrate its differences versus conventional non-GO-OED, through\nvarious test problems and an application of sensor placement for source\ninversion in a convection-diffusion field.\n","authors":["Shijie Zhong","Wanggang Shen","Tommie Catanach","Xun Huan"],"pdf_url":"https://arxiv.org/pdf/2403.18072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10842v2","updated":"2024-03-26T19:45:15Z","published":"2023-12-17T23:20:51Z","title":"Compositional Inductive Invariant Based Verification of Neural Network\n  Controlled Systems","summary":"  The integration of neural networks into safety-critical systems has shown\ngreat potential in recent years. However, the challenge of effectively\nverifying the safety of Neural Network Controlled Systems (NNCS) persists. This\npaper introduces a novel approach to NNCS safety verification, leveraging the\ninductive invariant method. Verifying the inductiveness of a candidate\ninductive invariant in the context of NNCS is hard because of the scale and\nnonlinearity of neural networks. Our compositional method makes this\nverification process manageable by decomposing the inductiveness proof\nobligation into smaller, more tractable subproblems. Alongside the high-level\nmethod, we present an algorithm capable of automatically verifying the\ninductiveness of given candidates by automatically inferring the necessary\ndecomposition predicates. The algorithm significantly outperforms the baseline\nmethod and shows remarkable reductions in execution time in our case studies,\nshortening the verification time from hours (or timeout) to seconds.\n","authors":["Yuhao Zhou","Stavros Tripakis"],"pdf_url":"https://arxiv.org/pdf/2312.10842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06822v2","updated":"2024-03-26T19:39:23Z","published":"2023-08-13T17:40:56Z","title":"Approximate and Weighted Data Reconstruction Attack in Federated\n  Learning","summary":"  Federated Learning (FL) is a distributed learning paradigm that enables\nmultiple clients to collaborate on building a machine learning model without\nsharing their private data. Although FL is considered privacy-preserved by\ndesign, recent data reconstruction attacks demonstrate that an attacker can\nrecover clients' training data based on the parameters shared in FL. However,\nmost existing methods fail to attack the most widely used horizontal Federated\nAveraging (FedAvg) scenario, where clients share model parameters after\nmultiple local training steps. To tackle this issue, we propose an\ninterpolation-based approximation method, which makes attacking FedAvg\nscenarios feasible by generating the intermediate model updates of the clients'\nlocal training processes. Then, we design a layer-wise weighted loss function\nto improve the data quality of reconstruction. We assign different weights to\nmodel updates in different layers concerning the neural network structure, with\nthe weights tuned by Bayesian optimization. Finally, experimental results\nvalidate the superiority of our proposed approximate and weighted attack (AWA)\nmethod over the other state-of-the-art methods, as demonstrated by the\nsubstantial improvement in different evaluation metrics for image data\nreconstructions.\n","authors":["Yongcun Song","Ziqi Wang","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2308.06822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18067v1","updated":"2024-03-26T19:36:50Z","published":"2024-03-26T19:36:50Z","title":"State of the art applications of deep learning within tracking and\n  detecting marine debris: A survey","summary":"  Deep learning techniques have been explored within the marine litter problem\nfor approximately 20 years but the majority of the research has developed\nrapidly in the last five years. We provide an in-depth, up to date, summary and\nanalysis of 28 of the most recent and significant contributions of deep\nlearning in marine debris. From cross referencing the research paper results,\nthe YOLO family significantly outperforms all other methods of object detection\nbut there are many respected contributions to this field that have\ncategorically agreed that a comprehensive database of underwater debris is not\ncurrently available for machine learning. Using a small dataset curated and\nlabelled by us, we tested YOLOv5 on a binary classification task and found the\naccuracy was low and the rate of false positives was high; highlighting the\nimportance of a comprehensive database. We conclude this survey with over 40\nfuture research recommendations and open challenges.\n","authors":["Zoe Moorton","Dr. Zeyneb Kurt","Dr. Wai Lok Woo"],"pdf_url":"https://arxiv.org/pdf/2403.18067v1.pdf","comment":"Review paper, 60 pages including references, 1 figure, 3 tables, 1\n  supplementary data"},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12492v2","updated":"2024-03-26T19:28:15Z","published":"2024-01-23T05:20:35Z","title":"Comparing Pre-trained Human Language Models: Is it Better with Human\n  Context as Groups, Individual Traits, or Both?","summary":"  Incorporating human context into language models is the next frontier for\nhuman-centered natural language processing. Currently, two pre-training methods\nexist: group-wise attributes (e.g., over-45-year-olds) or individual traits.\nGroup attributes are coarse -- not all 45-year-olds write the same way -- while\nmodeling individual traits allows for a more personalized representation, but\nrequires more complex modeling and data. So far, it is unclear which\npre-training approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on 5 user- and document-level tasks. We find that pre-training with\nboth group and individual features significantly improves the two user-level\nregression tasks like age estimation and personality assessment. Pre-training\non individual users significantly improves the three document-level\nclassification tasks like stance and topic detection. It even does well for\ndownstream tasks without historical user data. Our results suggest both\napproaches have specific use cases, opening new avenues for human-centered\nlanguage modeling.\n","authors":["Nikita Soni","Niranjan Balasubramanian","H. Andrew Schwartz","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2401.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18052v1","updated":"2024-03-26T19:10:08Z","published":"2024-03-26T19:10:08Z","title":"R2D2 image reconstruction with model uncertainty quantification in radio\n  astronomy","summary":"  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)\napproach was recently introduced for Radio-Interferometric (RI) imaging in\nastronomy. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. In\nthis work, we investigate the robustness of the R2D2 image estimation process,\nby studying the uncertainty associated with its series of learned models.\nAdopting an ensemble averaging approach, multiple series can be trained,\narising from different random DNN initializations of the training process at\neach iteration. The resulting multiple R2D2 instances can also be leveraged to\ngenerate ``R2D2 samples'', from which empirical mean and standard deviation\nendow the algorithm with a joint estimation and uncertainty quantification\nfunctionality. Focusing on RI imaging, and adopting a telescope-specific\napproach, multiple R2D2 instances were trained to encompass the most general\nobservation setting of the Very Large Array (VLA). Simulations and real-data\nexperiments confirm that: (i) R2D2's image estimation capability is superior to\nthat of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction\ncapability (arising from series with only few DNNs) makes the computation of\nmultiple reconstruction samples and of uncertainty maps practical even at large\nimage dimension; (iii) it is characterized by a very low model uncertainty.\n","authors":["Amir Aghabiglou","Chung San Chu","Arwa Dabbech","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.18052v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18044v1","updated":"2024-03-26T18:57:56Z","published":"2024-03-26T18:57:56Z","title":"Deep polytopic autoencoders for low-dimensional linear parameter-varying\n  approximations and nonlinear feedback design","summary":"  Polytopic autoencoders provide low-dimensional parametrizations of states in\na polytope. For nonlinear PDEs, this is readily applied to low-dimensional\nlinear parameter-varying (LPV) approximations as they have been exploited for\nefficient nonlinear controller design via series expansions of the solution to\nthe state-dependent Riccati equation. In this work, we develop a polytopic\nautoencoder for control applications and show how it outperforms standard\nlinear approaches in view of LPV approximations of nonlinear systems and how\nthe particular architecture enables higher order series expansions at little\nextra computational effort. We illustrate the properties and potentials of this\napproach to computational nonlinear controller design for large-scale systems\nwith a thorough numerical study.\n","authors":["Jan Heiland","Yongho Kim","Steffen W. R. Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18044v1.pdf","comment":"9 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.05465v2","updated":"2024-03-26T18:43:35Z","published":"2024-03-08T17:28:49Z","title":"Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit\n  Encodings for Efficient DNN Inference","summary":"  Traditional Deep Neural Network (DNN) quantization methods using integer,\nfixed-point, or floating-point data types struggle to capture diverse DNN\nparameter distributions at low precision, and often require large silicon\noverhead and intensive quantization-aware training. In this study, we introduce\nLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by\nposits that dynamically adapts to DNN weight/activation distributions by\nparameterizing LP bit fields. We also develop a novel genetic-algorithm based\nframework, LP Quantization (LPQ), to find optimal layer-wise LP parameters\nwhile reducing representational divergence between quantized and full-precision\nmodels through a novel global-local contrastive objective. Additionally, we\ndesign a unified mixed-precision LP accelerator (LPA) architecture comprising\nof processing elements (PEs) incorporating LP in the computational datapath.\nOur algorithm-hardware co-design demonstrates on average <1% drop in top-1\naccuracy across various CNN and ViT models. It also achieves ~ 2x improvements\nin performance per unit area and 2.2x gains in energy efficiency compared to\nstate-of-the-art quantization accelerators using different data types.\n","authors":["Akshat Ramachandran","Zishen Wan","Geonhwa Jeong","John Gustafson","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.05465v2.pdf","comment":"2024 61st IEEE/ACM Design Automation Conference (DAC)"},{"id":"http://arxiv.org/abs/2403.18035v1","updated":"2024-03-26T18:40:36Z","published":"2024-03-26T18:40:36Z","title":"Bidirectional Consistency Models","summary":"  Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE\nsolver complicates the inversion process. To resolve this, we introduce the\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. Notably, our\nproposed method enables one-step generation and inversion while also allowing\nthe use of additional steps to enhance generation quality or reduce\nreconstruction error. Furthermore, by leveraging our model's bidirectional\nconsistency, we introduce a sampling strategy that can enhance FID while\npreserving the generated image content. We further showcase our model's\ncapabilities in several downstream tasks, such as interpolation and inpainting,\nand present demonstrations of potential applications, including blind\nrestoration of compressed images and defending black-box adversarial attacks.\n","authors":["Liangchen Li","Jiajun He"],"pdf_url":"https://arxiv.org/pdf/2403.18035v1.pdf","comment":"40 pages, 25 figures"},{"id":"http://arxiv.org/abs/2211.10438v6","updated":"2024-03-26T18:36:31Z","published":"2022-11-18T18:59:33Z","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models","summary":"  Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.\n","authors":["Guangxuan Xiao","Ji Lin","Mickael Seznec","Hao Wu","Julien Demouth","Song Han"],"pdf_url":"https://arxiv.org/pdf/2211.10438v6.pdf","comment":"ICML 2023. First two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.18028v1","updated":"2024-03-26T18:29:39Z","published":"2024-03-26T18:29:39Z","title":"Predicting species occurrence patterns from partial observations","summary":"  To address the interlinked biodiversity and climate crises, we need an\nunderstanding of where species occur and how these patterns are changing.\nHowever, observational data on most species remains very limited, and the\namount of data available varies greatly between taxonomic groups. We introduce\nthe problem of predicting species occurrence patterns given (a) satellite\nimagery, and (b) known information on the occurrence of other species. To\nevaluate algorithms on this task, we introduce SatButterfly, a dataset of\nsatellite images, environmental data and observational data for butterflies,\nwhich is designed to pair with the existing SatBird dataset of bird\nobservational data. To address this task, we propose a general model, R-Tran,\nfor predicting species occurrence patterns that enables the use of partial\nobservational data wherever found. We find that R-Tran outperforms other\nmethods in predicting species encounter rates with partial information both\nwithin a taxon (birds) and across taxa (birds and butterflies). Our approach\nopens new perspectives to leveraging insights from species with abundant data\nto other species with scarce data, by modelling the ecosystems in which they\nco-occur.\n","authors":["Hager Radi Abdelwahed","Mélisande Teng","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2403.18028v1.pdf","comment":"Tackling Climate Change with Machine Learning workshop at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18026v1","updated":"2024-03-26T18:23:31Z","published":"2024-03-26T18:23:31Z","title":"Cross-system biological image quality enhancement based on the\n  generative adversarial network as a foundation for establishing a\n  multi-institute microscopy cooperative network","summary":"  High-quality fluorescence imaging of biological systems is limited by\nprocesses like photobleaching and phototoxicity, and also in many cases, by\nlimited access to the latest generations of microscopes. Moreover, low temporal\nresolution can lead to a motion blur effect in living systems. Our work\npresents a deep learning (DL) generative-adversarial approach to the problem of\nobtaining high-quality (HQ) images based on their low-quality (LQ) equivalents.\nWe propose a generative-adversarial network (GAN) for contrast transfer between\ntwo different separate microscopy systems: a confocal microscope (producing HQ\nimages) and a wide-field fluorescence microscope (producing LQ images). Our\nmodel proves that such transfer is possible, allowing us to receive\nHQ-generated images characterized by low mean squared error (MSE) values, high\nstructural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR)\nvalues. For our best model in the case of comparing HQ-generated images and\nHQ-ground truth images, the median values of the metrics are 6x10-4, 0.9413,\nand 31.87, for MSE, SSIM, and PSNR, respectively. In contrast, in the case of\ncomparison between LQ and HQ ground truth median values of the metrics are\nequal to 0.0071, 0.8304, and 21.48 for MSE, SSIM, and PSNR respectively.\nTherefore, we observe a significant increase ranging from 14% to 49% for SSIM\nand PSNR respectively. These results, together with other single-system\ncross-modality studies, provide proof of concept for further implementation of\na cross-system biological image quality enhancement.\n","authors":["Dominik Panek","Carina Rząca","Maksymilian Szczypior","Joanna Sorysz","Krzysztof Misztal","Zbigniew Baster","Zenon Rajfur"],"pdf_url":"https://arxiv.org/pdf/2403.18026v1.pdf","comment":"15 Pages, 5 Figures, 1 Table, 3 pages Supplementary Materials"},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2306.15328v3","updated":"2024-03-26T18:19:54Z","published":"2023-06-27T09:34:32Z","title":"Simulating counterfactuals","summary":"  Counterfactual inference considers a hypothetical intervention in a parallel\nworld that shares some evidence with the factual world. If the evidence\nspecifies a conditional distribution on a manifold, counterfactuals may be\nanalytically intractable. We present an algorithm for simulating values from a\ncounterfactual distribution where conditions can be set on both discrete and\ncontinuous variables. We show that the proposed algorithm can be presented as a\nparticle filter leading to asymptotically valid inference. The algorithm is\napplied to fairness analysis in credit-scoring.\n","authors":["Juha Karvanen","Santtu Tikka","Matti Vihola"],"pdf_url":"https://arxiv.org/pdf/2306.15328v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14814v2","updated":"2024-03-26T18:10:10Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v2.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.18018v1","updated":"2024-03-26T18:07:10Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2306.16772v5","updated":"2024-03-26T18:04:33Z","published":"2023-06-29T08:13:57Z","title":"Learning from Synthetic Human Group Activities","summary":"  The study of complex human interactions and group activities has become a\nfocal point in human-centric computer vision. However, progress in related\ntasks is often hindered by the challenges of obtaining large-scale labeled\ndatasets from real-world scenarios. To address the limitation, we introduce\nM3Act, a synthetic data generator for multi-view multi-group multi-person human\natomic actions and group activities. Powered by Unity Engine, M3Act features\nmultiple semantic groups, highly diverse and photorealistic images, and a\ncomprehensive set of annotations, which facilitates the learning of\nhuman-centered tasks across single-person, multi-person, and multi-group\nconditions. We demonstrate the advantages of M3Act across three core\nexperiments. The results suggest our synthetic dataset can significantly\nimprove the performance of several downstream methods and replace real-world\ndatasets to reduce cost. Notably, M3Act improves the state-of-the-art MOTRv2 on\nDanceTrack dataset, leading to a hop on the leaderboard from 10th to 2nd place.\nMoreover, M3Act opens new research for controllable 3D group activity\ngeneration. We define multiple metrics and propose a competitive baseline for\nthe novel task. Our code and data are available at our project page:\nhttp://cjerry1243.github.io/M3Act.\n","authors":["Che-Jui Chang","Danrui Li","Deep Patel","Parth Goel","Honglu Zhou","Seonghyeon Moon","Samuel S. Sohn","Sejong Yoon","Vladimir Pavlovic","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2306.16772v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17921v1","updated":"2024-03-26T17:55:58Z","published":"2024-03-26T17:55:58Z","title":"The Need for Speed: Pruning Transformers with One Recipe","summary":"  We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.\n","authors":["Samir Khaki","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2403.17921v1.pdf","comment":"Accepted in the International Conference on Learning Representations\n  (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01364v3","updated":"2024-03-26T17:45:01Z","published":"2022-11-02T17:59:09Z","title":"An optimal control perspective on diffusion-based generative modeling","summary":"  We establish a connection between stochastic optimal control and generative\nmodels based on stochastic differential equations (SDEs), such as recently\ndeveloped diffusion probabilistic models. In particular, we derive a\nHamilton-Jacobi-Bellman equation that governs the evolution of the\nlog-densities of the underlying SDE marginals. This perspective allows to\ntransfer methods from optimal control theory to generative modeling. First, we\nshow that the evidence lower bound is a direct consequence of the well-known\nverification theorem from control theory. Further, we can formulate\ndiffusion-based generative modeling as a minimization of the Kullback-Leibler\ndivergence between suitable measures in path space. Finally, we develop a novel\ndiffusion-based method for sampling from unnormalized densities -- a problem\nfrequently occurring in statistics and computational sciences. We demonstrate\nthat our time-reversed diffusion sampler (DIS) can outperform other\ndiffusion-based sampling approaches on multiple numerical examples.\n","authors":["Julius Berner","Lorenz Richter","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2211.01364v3.pdf","comment":"Accepted for oral presentation at NeurIPS 2022 Workshop on\n  Score-Based Methods"},{"id":"http://arxiv.org/abs/2401.15059v2","updated":"2024-03-26T17:44:45Z","published":"2024-01-26T18:42:01Z","title":"Fully Independent Communication in Multi-Agent Reinforcement Learning","summary":"  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n","authors":["Rafael Pina","Varuna De Silva","Corentin Artaud","Xiaolan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.15059v2.pdf","comment":"Extended version of the paper appearing on AAMAS 2024 with the same\n  title. 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2310.18841v2","updated":"2024-03-26T17:39:30Z","published":"2023-10-28T22:57:56Z","title":"A randomized algorithm for nonconvex minimization with inexact\n  evaluations and complexity guarantees","summary":"  We consider minimization of a smooth nonconvex function with inexact oracle\naccess to gradient and Hessian (without assuming access to the function value)\nto achieve approximate second-order optimality. A novel feature of our method\nis that if an approximate direction of negative curvature is chosen as the\nstep, we choose its sense to be positive or negative with equal probability. We\nallow gradients to be inexact in a relative sense and relax the coupling\nbetween inexactness thresholds for the first- and second-order optimality\nconditions. Our convergence analysis includes both an expectation bound based\non martingale analysis and a high-probability bound based on concentration\ninequalities. We apply our algorithm to empirical risk minimization problems\nand obtain improved gradient sample complexity over existing works.\n","authors":["Shuyao Li","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2310.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09428v2","updated":"2024-03-26T17:38:38Z","published":"2024-03-14T14:19:48Z","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity","summary":"  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n","authors":["Zhuo Zhi","Ziquan Liu","Moe Elbadawi","Adam Daneshmend","Mine Orlu","Abdul Basit","Andreas Demosthenous","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2403.09428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02156v4","updated":"2024-03-26T17:36:54Z","published":"2023-10-03T15:43:59Z","title":"Probabilistically Rewired Message-Passing Neural Networks","summary":"  Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.\n","authors":["Chendi Qian","Andrei Manolache","Kareem Ahmed","Zhe Zeng","Guy Van den Broeck","Mathias Niepert","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2310.02156v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07809v2","updated":"2024-03-26T17:29:07Z","published":"2024-01-15T16:30:12Z","title":"Optimal Data Splitting in Distributed Optimization for Machine Learning","summary":"  The distributed optimization problem has become increasingly relevant\nrecently. It has a lot of advantages such as processing a large amount of data\nin less time compared to non-distributed methods. However, most distributed\napproaches suffer from a significant bottleneck - the cost of communications.\nTherefore, a large amount of research has recently been directed at solving\nthis problem. One such approach uses local data similarity. In particular,\nthere exists an algorithm provably optimally exploiting the similarity\nproperty. But this result, as well as results from other works solve the\ncommunication bottleneck by focusing only on the fact that communication is\nsignificantly more expensive than local computing and does not take into\naccount the various capacities of network devices and the different\nrelationship between communication time and local computing expenses. We\nconsider this setup and the objective of this study is to achieve an optimal\nratio of distributed data between the server and local machines for any costs\nof communications and local computations. The running times of the network are\ncompared between uniform and optimal distributions. The superior theoretical\nperformance of our solutions is experimentally validated.\n","authors":["Daniil Medyakov","Gleb Molodtsov","Aleksandr Beznosikov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07809v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.17891v1","updated":"2024-03-26T17:22:29Z","published":"2024-03-26T17:22:29Z","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels","summary":"  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n","authors":["Nurettin Sergin","Jiayu Huang","Tzyy-Shuh Chang","Hao Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17891v1.pdf","comment":"Accepted in IISE Transaction"},{"id":"http://arxiv.org/abs/2403.17889v1","updated":"2024-03-26T17:21:54Z","published":"2024-03-26T17:21:54Z","title":"Large scale paired antibody language models","summary":"  Antibodies are proteins produced by the immune system that can identify and\nneutralise a wide variety of antigens with high specificity and affinity, and\nconstitute the most successful class of biotherapeutics. With the advent of\nnext-generation sequencing, billions of antibody sequences have been collected\nin recent years, though their application in the design of better therapeutics\nhas been constrained by the sheer volume and complexity of the data. To address\nthis challenge, we present IgBert and IgT5, the best performing\nantibody-specific language models developed to date which can consistently\nhandle both paired and unpaired variable region sequences as input. These\nmodels are trained comprehensively using the more than two billion unpaired\nsequences and two million paired sequences of light and heavy chains present in\nthe Observed Antibody Space dataset. We show that our models outperform\nexisting antibody and protein language models on a diverse range of design and\nregression tasks relevant to antibody engineering. This advancement marks a\nsignificant leap forward in leveraging machine learning, large scale data sets\nand high-performance computing for enhancing antibody design for therapeutic\ndevelopment.\n","authors":["Henry Kenlay","Frédéric A. Dreyer","Aleksandr Kovaltsuk","Dom Miketa","Douglas Pires","Charlotte M. Deane"],"pdf_url":"https://arxiv.org/pdf/2403.17889v1.pdf","comment":"14 pages, 2 figures, 6 tables, model weights available at\n  https://zenodo.org/doi/10.5281/zenodo.10876908"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17886v1","updated":"2024-03-26T17:19:23Z","published":"2024-03-26T17:19:23Z","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training\n  and inference in Earth Observation","summary":"  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n","authors":["Carlos Gomes","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2403.17886v1.pdf","comment":"Published at IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.04202v3","updated":"2024-03-26T17:18:33Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several important real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex-valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17868v1","updated":"2024-03-26T16:57:01Z","published":"2024-03-26T16:57:01Z","title":"Sample complexity of quantum hypothesis testing","summary":"  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n","authors":["Hao-Chung Cheng","Nilanjana Datta","Nana Liu","Theshani Nuradha","Robert Salzmann","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2403.17868v1.pdf","comment":"38 pages, 1 figure, preliminary version; see independent and\n  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"},{"id":"http://arxiv.org/abs/2401.07788v2","updated":"2024-03-26T16:49:44Z","published":"2024-01-15T15:54:54Z","title":"Activations and Gradients Compression for Model-Parallel Training","summary":"  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n","authors":["Mikhail Rudakov","Aleksandr Beznosikov","Yaroslav Kholodov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07788v2.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2210.06459v2","updated":"2024-03-26T16:49:11Z","published":"2022-10-12T17:56:04Z","title":"Differentially private multivariate medians","summary":"  Statistical tools which satisfy rigorous privacy guarantees are necessary for\nmodern data analysis. It is well-known that robustness against contamination is\nlinked to differential privacy. Despite this fact, using multivariate medians\nfor differentially private and robust multivariate location estimation has not\nbeen systematically studied. We develop novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians, which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to d = 100,\nand compare them to a state-of-the-art private mean estimation algorithm. As a\nby-product of our investigation, we prove concentration inequalities for the\noutput of the exponential mechanism about the maximizer of the population\nobjective function. This bound applies to objective functions that satisfy a\nmild regularity condition.\n","authors":["Kelly Ramsay","Aukosh Jagannath","Shoja'eddin Chenouri"],"pdf_url":"https://arxiv.org/pdf/2210.06459v2.pdf","comment":"42 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17852v1","updated":"2024-03-26T16:40:08Z","published":"2024-03-26T16:40:08Z","title":"Counterfactual Fairness through Transforming Data Orthogonal to Bias","summary":"  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.\n","authors":["Shuyi Chen","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17847v1","updated":"2024-03-26T16:36:50Z","published":"2024-03-26T16:36:50Z","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections","summary":"  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n","authors":["Chia-Hao Chiang","Zheng-Han Huang","Liwen Liu","Hsin-Chien Liang","Yi-Chi Wang","Wan-Ling Tseng","Chao Wang","Che-Ta Chen","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2312.17336v2","updated":"2024-03-26T16:35:15Z","published":"2023-12-28T19:28:23Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model","summary":"  Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17845v1","updated":"2024-03-26T16:34:05Z","published":"2024-03-26T16:34:05Z","title":"TractOracle: towards an anatomically-informed reward function for\n  RL-based tractography","summary":"  Reinforcement learning (RL)-based tractography is a competitive alternative\nto machine learning and classical tractography algorithms due to its high\nanatomical accuracy obtained without the need for any annotated data. However,\nthe reward functions so far used to train RL agents do not encapsulate\nanatomical knowledge which causes agents to generate spurious false positives\ntracts. In this paper, we propose a new RL tractography system, TractOracle,\nwhich relies on a reward network trained for streamline classification. This\nnetwork is used both as a reward function during training as well as a mean for\nstopping the tracking process early and thus reduce the number of false\npositive streamlines. This makes our system a unique method that evaluates and\nreconstructs WM streamlines at the same time. We report an improvement of true\npositive ratios by almost 20\\% and a reduction of 3x of false positive ratios\non one dataset and an increase between 2x and 7x in the number true positive\nstreamlines on another dataset.\n","authors":["Antoine Théberge","Maxime Descoteaux","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2403.17845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17844v1","updated":"2024-03-26T16:33:12Z","published":"2024-03-26T16:33:12Z","title":"Mechanistic Design and Scaling of Hybrid Architectures","summary":"  The development of deep learning architectures is a resource-demanding\nprocess, due to a vast design space, long prototyping times, and high compute\ncosts associated with at-scale model training and evaluation. We set out to\nsimplify this process by grounding it in an end-to-end mechanistic architecture\ndesign (MAD) pipeline, encompassing small-scale capability unit tests\npredictive of scaling laws. Through a suite of synthetic token manipulation\ntasks such as compression and recall, designed to probe capabilities, we\nidentify and test new hybrid architectures constructed from a variety of\ncomputational primitives. We experimentally validate the resulting\narchitectures via an extensive compute-optimal and a new state-optimal scaling\nlaw analysis, training over 500 language models between 70M to 7B parameters.\nSurprisingly, we find MAD synthetics to correlate with compute-optimal\nperplexity, enabling accurate evaluation of new architectures via isolated\nproxy tasks. The new architectures found via MAD, based on simple ideas such as\nhybridization and sparsity, outperform state-of-the-art Transformer,\nconvolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in\nscaling, both at compute-optimal budgets and in overtrained regimes. Overall,\nthese results provide evidence that performance on curated synthetic tasks can\nbe predictive of scaling laws, and that an optimal architecture should leverage\nspecialized layers via a hybrid topology.\n","authors":["Michael Poli","Armin W Thomas","Eric Nguyen","Pragaash Ponnusamy","Björn Deiseroth","Kristian Kersting","Taiji Suzuki","Brian Hie","Stefano Ermon","Christopher Ré","Ce Zhang","Stefano Massaroli"],"pdf_url":"https://arxiv.org/pdf/2403.17844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time-consuming. Therefore, the challenging task of reconstructing\nvisually accurate HDR images from their Low Dynamic Range (LDR) counterparts is\ngaining attention in the vision research community. A major challenge in this\nresearch problem is the lack of datasets, which capture diverse scene\nconditions (e.g., lighting, shadows, weather, locations, landscapes, objects,\nhumans, buildings) and various image features (e.g., color, contrast,\nsaturation, hue, luminance, brightness, radiance). To address this gap, in this\npaper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic\nHDR images sampled from the GTA-V video game. We perform thorough evaluation of\nthe proposed dataset, which demonstrates significant qualitative and\nquantitative improvements of the state-of-the-art HDR image reconstruction\nmethods. Furthermore, we demonstrate the effectiveness of the proposed dataset\nand its impact on additional computer vision tasks including 3D human pose\nestimation, human body part segmentation, and holistic scene segmentation. The\ndataset, data collection pipeline, and evaluation code are available at:\nhttps://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2312.17329v2","updated":"2024-03-26T16:22:36Z","published":"2023-12-28T19:09:56Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part I:\n  Implementation and multi-fidelity hierarchies for the single-particle model","summary":"  To plan and optimize energy storage demands that account for Li-ion battery\naging dynamics, techniques need to be developed to diagnose battery internal\nstates accurately and rapidly. This study seeks to reduce the computational\nresources needed to determine a battery's internal states by replacing\nphysics-based Li-ion battery models -- such as the single-particle model (SPM)\nand the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)\nsurrogate. The surrogate model makes high-throughput techniques, such as\nBayesian calibration, tractable to determine battery internal parameters from\nvoltage responses. This manuscript is the first of a two-part series that\nintroduces PINN surrogates of Li-ion battery models for parameter inference\n(i.e., state-of-health diagnostics). In this first part, a method is presented\nfor constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical\ntraining, where several neural nets are trained with multiple physics-loss\nfidelities is shown to significantly improve the surrogate accuracy when only\ntraining on the governing equation residuals. The implementation is made\navailable in a companion repository (https://github.com/NREL/pinnstripes). The\ntechniques used to develop a PINN surrogate of the SPM are extended in Part II\nfor the PINN surrogate for the P2D battery model, and explore the Bayesian\ncalibration capabilities of both surrogates.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14427v2","updated":"2024-03-26T16:15:40Z","published":"2023-11-24T12:00:50Z","title":"Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal","summary":"  The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of Hodge spectral clustering and (b) to classify\nedges and higher-order simplices based on their relationship to the smallest\nharmonic, curl, and gradient eigenvectors.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2311.14427v2.pdf","comment":"5 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.17833v1","updated":"2024-03-26T16:14:43Z","published":"2024-03-26T16:14:43Z","title":"GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning","summary":"  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n","authors":["Shijie Na","Yuzhi Liang","Siu-Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2403.17833v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17831v1","updated":"2024-03-26T16:13:55Z","published":"2024-03-26T16:13:55Z","title":"Learning the Optimal Power Flow: Environment Design Matters","summary":"  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n","authors":["Thomas Wolgast","Astrid Nieße"],"pdf_url":"https://arxiv.org/pdf/2403.17831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2302.03788v4","updated":"2024-03-26T15:41:28Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v4.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2306.15909v4","updated":"2024-03-26T15:13:20Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12243v4","updated":"2024-03-26T15:12:19Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Multi-Task Learning","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN\n","authors":["S. S. Hotegni","M. Berkemeier","S. Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12243v4.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17775v1","updated":"2024-03-26T15:07:58Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Östman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric M. John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.17995v1","updated":"2024-03-26T14:47:05Z","published":"2024-03-26T14:47:05Z","title":"Semi-Supervised Image Captioning Considering Wasserstein Graph Matching","summary":"  Image captioning can automatically generate captions for the given images,\nand the key challenge is to learn a mapping function from visual features to\nnatural language features. Existing approaches are mostly supervised ones,\ni.e., each image has a corresponding sentence in the training set. However,\nconsidering that describing images always requires a huge of manpower, we\nusually have limited amount of described images (i.e., image-text pairs) and a\nlarge number of undescribed images in real-world applications. Thereby, a\ndilemma is the \"Semi-Supervised Image Captioning\". To solve this problem, we\npropose a novel Semi-Supervised Image Captioning method considering Wasserstein\nGraph Matching (SSIC-WGM), which turns to adopt the raw image inputs to\nsupervise the generated sentences. Different from traditional single modal\nsemi-supervised methods, the difficulty of semi-supervised cross-modal learning\nlies in constructing intermediately comparable information among heterogeneous\nmodalities. In this paper, SSIC-WGM adopts the successful scene graphs as\nintermediate information, and constrains the generated sentences from two\naspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of\nthe raw image and generated sentence respectively, then employs the wasserstein\ndistance to better measure the similarity between region embeddings of\ndifferent graphs. 2) intra-modal consistency. SSIC-WGM takes the data\naugmentation techniques for the raw images, then constrains the consistency\namong augmented images and generated sentences. Consequently, SSIC-WGM combines\nthe cross-modal pseudo supervision and structure invariant measure for\nefficiently using the undescribed images, and learns more reasonable mapping\nfunction.\n","authors":["Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17753v1","updated":"2024-03-26T14:43:57Z","published":"2024-03-26T14:43:57Z","title":"CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream\n  Enhanced Rectified Transformer Model","summary":"  Accurate, and effective traffic forecasting is vital for smart traffic\nsystems, crucial in urban traffic planning and management. Current\nSpatio-Temporal Transformer models, despite their prediction capabilities,\nstruggle with balancing computational efficiency and accuracy, favoring global\nover local information, and handling spatial and temporal data separately,\nlimiting insight into complex interactions. We introduce the Criss-Crossed\nDual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes\nthree innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),\nEnhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified\nTemporal Self-attention (ReTSA). These modules aim to lower computational needs\nvia sparse attention, focus on local information for better traffic dynamics\nunderstanding, and merge spatial and temporal insights through a unique\nlearning method. Extensive tests on six real-world datasets highlight\nCCDSReFormer's superior performance. An ablation study also confirms the\nsignificant impact of each component on the model's predictive accuracy,\nshowcasing our model's ability to forecast traffic flow effectively.\n","authors":["Zhiqi Shao","Michael G. H. Bell","Ze Wang","D. Glenn Geers","Xusheng Yao","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17753v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17745v1","updated":"2024-03-26T14:36:22Z","published":"2024-03-26T14:36:22Z","title":"Leave No Patient Behind: Enhancing Medication Recommendation for Rare\n  Disease Patients","summary":"  Medication recommendation systems have gained significant attention in\nhealthcare as a means of providing tailored and effective drug combinations\nbased on patients' clinical information. However, existing approaches often\nsuffer from fairness issues, as recommendations tend to be more accurate for\npatients with common diseases compared to those with rare conditions. In this\npaper, we propose a novel model called Robust and Accurate REcommendations for\nMedication (RAREMed), which leverages the pretrain-finetune learning paradigm\nto enhance accuracy for rare diseases. RAREMed employs a transformer encoder\nwith a unified input sequence approach to capture complex relationships among\ndisease and procedure codes. Additionally, it introduces two self-supervised\npre-training tasks, namely Sequence Matching Prediction (SMP) and Self\nReconstruction (SR), to learn specialized medication needs and interrelations\namong clinical codes. Experimental results on two real-world datasets\ndemonstrate that RAREMed provides accurate drug sets for both rare and common\ndisease patients, thereby mitigating unfairness in medication recommendation\nsystems.\n","authors":["Zihao Zhao","Yi Jing","Fuli Feng","Jiancan Wu","Chongming Gao","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2403.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17728v1","updated":"2024-03-26T14:17:01Z","published":"2024-03-26T14:17:01Z","title":"Masked Autoencoders are PDE Learners","summary":"  Neural solvers for partial differential equations (PDEs) have great\npotential, yet their practicality is currently limited by their\ngeneralizability. PDEs evolve over broad scales and exhibit diverse behaviors;\npredicting these phenomena will require learning representations across a wide\nvariety of inputs, which may encompass different coefficients, geometries, or\nequations. As a step towards generalizable PDE modeling, we adapt masked\npretraining for PDEs. Through self-supervised learning across PDEs, masked\nautoencoders can learn useful latent representations for downstream tasks. In\nparticular, masked pretraining can improve coefficient regression and\ntimestepping performance of neural solvers on unseen equations. We hope that\nmasked pretraining can emerge as a unifying method across large, unlabeled, and\nheterogeneous datasets to learn latent physics at scale.\n","authors":["Anthony Zhou","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.17728v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.05337v2","updated":"2024-03-26T14:09:56Z","published":"2023-12-08T19:52:48Z","title":"Artificial Neural Nets and the Representation of Human Concepts","summary":"  What do artificial neural networks (ANNs) learn? The machine learning (ML)\ncommunity shares the narrative that ANNs must develop abstract human concepts\nto perform complex tasks. Some go even further and believe that these concepts\nare stored in individual units of the network. Based on current research, I\nsystematically investigate the assumptions underlying this narrative. I\nconclude that ANNs are indeed capable of performing complex prediction tasks,\nand that they may learn human and non-human concepts to do so. However,\nevidence indicates that ANNs do not represent these concepts in individual\nunits.\n","authors":["Timo Freiesleben"],"pdf_url":"https://arxiv.org/pdf/2312.05337v2.pdf","comment":"For: Philosophy of Science for Machine Learning: Core Issues and New\n  Perspectives, edited by Juan Duran and Giorgia Pozzi"},{"id":"http://arxiv.org/abs/2310.02969v2","updated":"2024-03-26T14:00:59Z","published":"2023-10-04T17:06:30Z","title":"Dual Conic Proxies for AC Optimal Power Flow","summary":"  In recent years, there has been significant interest in the development of\nmachine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).\nAlthough significant progress has been achieved in predicting high-quality\nprimal solutions, no existing learning-based approach can provide valid dual\nbounds for AC-OPF. This paper addresses this gap by training optimization\nproxies for a convex relaxation of AC-OPF. Namely, the paper considers a\nsecond-order cone (SOC) relaxation of AC-OPF, and proposes \\revision{a novel\narchitecture} that embeds a fast, differentiable (dual) feasibility recovery,\nthus providing valid dual bounds. The paper combines this new architecture with\na self-supervised learning scheme, which alleviates the need for costly\ntraining data generation. Extensive numerical experiments on medium- and\nlarge-scale power grids demonstrate the efficiency and scalability of the\nproposed methodology.\n","authors":["Guancheng Qiu","Mathieu Tanneau","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2310.02969v2.pdf","comment":"accepted to PSCC 2024"},{"id":"http://arxiv.org/abs/2403.17994v1","updated":"2024-03-26T13:50:39Z","published":"2024-03-26T13:50:39Z","title":"Solution for Point Tracking Task of ICCV 1st Perception Test Challenge\n  2023","summary":"  This report proposes an improved method for the Tracking Any Point (TAP)\ntask, which tracks any physical surface through a video. Several existing\napproaches have explored the TAP by considering the temporal relationships to\nobtain smooth point motion trajectories, however, they still suffer from the\ncumulative error caused by temporal prediction. To address this issue, we\npropose a simple yet effective approach called TAP with confident static points\n(TAPIR+), which focuses on rectifying the tracking of the static point in the\nvideos shot by a static camera. To clarify, our approach contains two key\ncomponents: (1) Multi-granularity Camera Motion Detection, which could identify\nthe video sequence by the static camera shot. (2) CMR-based point trajectory\nprediction with one moving object segmentation approach to isolate the static\npoint from the moving object. Our approach ranked first in the final test with\na score of 0.46.\n","authors":["Hongpeng Pan","Yang Yang","Zhongtian Fu","Yuxuan Zhang","Shian Du","Yi Xu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.17994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02869v2","updated":"2024-03-26T13:43:55Z","published":"2023-10-04T15:03:56Z","title":"Harmonic Control Lyapunov Barrier Functions for Constrained Optimal\n  Control with Reach-Avoid Specifications","summary":"  This paper introduces harmonic control Lyapunov barrier functions (harmonic\nCLBF) that aid in constrained control problems such as reach-avoid problems.\nHarmonic CLBFs exploit the maximum principle that harmonic functions satisfy to\nencode the properties of control Lyapunov barrier functions (CLBFs). As a\nresult, they can be initiated at the start of an experiment rather than trained\nbased on sample trajectories. The control inputs are selected to maximize the\ninner product of the system dynamics with the steepest descent direction of the\nharmonic CLBF. Numerical results are presented with four different systems\nunder different reach-avoid environments. Harmonic CLBFs show a significantly\nlow risk of entering unsafe regions and a high probability of entering the goal\nregion.\n","authors":["Amartya Mukherjee","Ruikun Zhou","Haocheng Chang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17698v1","updated":"2024-03-26T13:38:06Z","published":"2024-03-26T13:38:06Z","title":"MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding\n  Length Extrapolation","summary":"  When the predicted sequence length exceeds the length seen during training,\nthe transformer's inference accuracy diminishes. Existing relative position\nencoding methods, such as those based on the ALiBi technique, address the\nlength extrapolation challenge exclusively through the implementation of a\nsingle kernel function, which introduces a constant bias to every post-softmax\nattention scores according to their distance. These approaches do not\ninvestigate or employ multiple kernel functions to address the extrapolation\nchallenge. Drawing on the ALiBi approach, this study proposes a novel relative\npositional encoding method, called MEP, which employs a weighted average to\ncombine distinct kernel functions(such as the exponential kernel and the\nGaussian kernel) to generate a bias that is applied to post-softmax attention\nscores. Initially, the framework utilizes various kernel functions to construct\nmultiple kernel functions. Each kernel function adheres to a consistent mean\nweight coefficient, harnessing the synergistic advantages of different kernels\nto formulate an innovative bias function. Subsequently, specific slopes are\ntailored for each kernel function, applying penalties at varying rates, to\nenhance the model's extrapolation capabilities. Finally, this bias is\nseamlessly incorporated as a penalty to the post-softmax scores. We present two\ndistinct versions of our method: a parameter-free variant that requires no new\nlearnable parameters, which enhances length extrapolation capabilities without\ncompromising training efficiency, and a parameterized variant capable of\nintegrating state-of-the-art techniques. Empirical evaluations across diverse\ndatasets have demonstrated that both variants of our method achieve\nstate-of-the-art performance, outperforming traditional parameter-free and\nparameterized approaches.\n","authors":["Weiguo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2301.12778v2","updated":"2024-03-26T13:29:16Z","published":"2023-01-30T10:48:10Z","title":"Investigating Feature and Model Importance in Android Malware Detection:\n  An Implemented Survey and Experimental Comparison of ML-Based Methods","summary":"  The popularity of Android means it is a common target for malware. Over the\nyears, various studies have found that machine learning models can effectively\ndiscriminate malware from benign applications. However, as the operating system\nevolves, so does malware, bringing into question the findings of these previous\nstudies, many of which report very high accuracies using small, outdated, and\noften imbalanced datasets. In this paper, we reimplement 18 representative past\nworks and reevaluate them using a balanced, relevant, and up-to-date dataset\ncomprising 124,000 applications. We also carry out new experiments designed to\nfill holes in existing knowledge, and use our findings to identify the most\neffective features and models to use for Android malware detection within a\ncontemporary environment. We show that high detection accuracies (up to 96.8%)\ncan be achieved using features extracted through static analysis alone,\nyielding a modest benefit (1%) from using far more expensive dynamic analysis.\nAPI calls and opcodes are the most productive static and TCP network traffic\nprovide the most predictive dynamic features. Random forests are generally the\nmost effective model, outperforming more complex deep learning approaches.\nWhilst directly combining static and dynamic features is generally ineffective,\nensembling models separately leads to performances comparable to the best\nmodels but using less brittle features.\n","authors":["Ali Muzaffar","Hani Ragab Hassen","Hind Zantout","Michael A Lones"],"pdf_url":"https://arxiv.org/pdf/2301.12778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17673v1","updated":"2024-03-26T13:02:43Z","published":"2024-03-26T13:02:43Z","title":"How Private is DP-SGD?","summary":"  We demonstrate a substantial gap between the privacy guarantees of the\nAdaptive Batch Linear Queries (ABLQ) mechanism under different types of batch\nsampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of\nDifferentially Private Stochastic Gradient Descent (DP-SGD) follows by\ninterpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is\nmore commonly used in practical implementations, it is neither analytically nor\nnumerically amenable to easy privacy analysis. On the other hand, Poisson\nsubsampling based DP-SGD is challenging to scalably implement, but has a\nwell-understood privacy analysis, with multiple open-source numerically tight\nprivacy accountants available. This has led to a common practice of using\nshuffling based DP-SGD in practice, but using the privacy analysis for the\ncorresponding Poisson subsampling version. Our result shows that there can be a\nsubstantial gap between the privacy analysis when using the two types of batch\nsampling, and thus advises caution in reporting privacy parameters for DP-SGD.\n","authors":["Lynn Chua","Badih Ghazi","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Amer Sinha","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01885v2","updated":"2024-03-26T12:59:44Z","published":"2023-11-03T12:54:05Z","title":"Domain Randomization via Entropy Maximization","summary":"  Varying dynamics parameters in simulation is a popular Domain Randomization\n(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\nNevertheless, DR heavily hinges on the choice of the sampling distribution of\nthe dynamics parameters, since high variability is crucial to regularize the\nagent's behavior but notoriously leads to overly conservative policies when\nrandomizing excessively. In this paper, we propose a novel approach to address\nsim-to-real transfer, which automatically shapes dynamics distributions during\ntraining in simulation without requiring real-world data. We introduce DOmain\nRAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\nproblem that directly maximizes the entropy of the training distribution while\nretaining generalization capabilities. In achieving this, DORAEMON gradually\nincreases the diversity of sampled dynamics parameters as long as the\nprobability of success of the current policy is sufficiently high. We\nempirically validate the consistent benefits of DORAEMON in obtaining highly\nadaptive and generalizable policies, i.e. solving the task at hand across the\nwidest range of dynamics parameters, as opposed to representative baselines\nfrom the DR literature. Notably, we also demonstrate the Sim2Real applicability\nof DORAEMON through its successful zero-shot transfer in a robotic manipulation\nsetup under unknown real-world parameters.\n","authors":["Gabriele Tiboni","Pascal Klink","Jan Peters","Tatiana Tommasi","Carlo D'Eramo","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2311.01885v2.pdf","comment":"Published as a conference paper at ICLR 2024. Project website at\n  https://gabrieletiboni.github.io/doraemon/"},{"id":"http://arxiv.org/abs/2403.17660v1","updated":"2024-03-26T12:47:04Z","published":"2024-03-26T12:47:04Z","title":"CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1\n  Perturbations","summary":"  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly \"uplift payments\" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n","authors":["Luis Piloto","Sofia Liguori","Sephora Madjiheurem","Miha Zgubic","Sean Lovett","Hamish Tomlinson","Sophie Elster","Chris Apps","Sims Witherspoon"],"pdf_url":"https://arxiv.org/pdf/2403.17660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17993v1","updated":"2024-03-26T12:45:52Z","published":"2024-03-26T12:45:52Z","title":"Mixing Artificial and Natural Intelligence: From Statistical Mechanics\n  to AI and Back to Turbulence","summary":"  The paper reflects on the future role of AI in scientific research, with a\nspecial focus on turbulence studies, and examines the evolution of AI,\nparticularly through Diffusion Models rooted in non-equilibrium statistical\nmechanics. It underscores the significant impact of AI on advancing reduced,\nLagrangian models of turbulence through innovative use of deep neural networks.\nAdditionally, the paper reviews various other AI applications in turbulence\nresearch and outlines potential challenges and opportunities in the concurrent\nadvancement of AI and statistical hydrodynamics. This discussion sets the stage\nfor a future where AI and turbulence research are intricately intertwined,\nleading to more profound insights and advancements in both fields.\n","authors":[" Michael"," Chertkov"],"pdf_url":"https://arxiv.org/pdf/2403.17993v1.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17656v1","updated":"2024-03-26T12:39:02Z","published":"2024-03-26T12:39:02Z","title":"SGHormer: An Energy-Saving Graph Transformer Driven by Spikes","summary":"  Graph Transformers (GTs) with powerful representation learning ability make a\nhuge success in wide range of graph tasks. However, the costs behind\noutstanding performances of GTs are higher energy consumption and computational\noverhead. The complex structure and quadratic complexity during attention\ncalculation in vanilla transformer seriously hinder its scalability on the\nlarge-scale graph data. Though existing methods have made strides in\nsimplifying combinations among blocks or attention-learning paradigm to improve\nGTs' efficiency, a series of energy-saving solutions originated from\nbiologically plausible structures are rarely taken into consideration when\nconstructing GT framework. To this end, we propose a new spiking-based graph\ntransformer (SGHormer). It turns full-precision embeddings into sparse and\nbinarized spikes to reduce memory and computational costs. The spiking graph\nself-attention and spiking rectify blocks in SGHormer explicitly capture global\nstructure information and recover the expressive power of spiking embeddings,\nrespectively. In experiments, SGHormer achieves comparable performances to\nother full-precision GTs with extremely low computational energy consumption.\nThe results show that SGHomer makes a remarkable progress in the field of\nlow-energy GTs.\n","authors":["Huizhe Zhang","Jintang Li","Liang Chen","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17656v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.17646v1","updated":"2024-03-26T12:28:04Z","published":"2024-03-26T12:28:04Z","title":"Uncertainty-aware Distributional Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) presents distinct challenges as it relies\nsolely on observational data. A central concern in this context is ensuring the\nsafety of the learned policy by quantifying uncertainties associated with\nvarious actions and environmental stochasticity. Traditional approaches\nprimarily emphasize mitigating epistemic uncertainty by learning risk-averse\npolicies, often overlooking environmental stochasticity. In this study, we\npropose an uncertainty-aware distributional offline RL method to simultaneously\naddress both epistemic uncertainty and environmental stochasticity. We propose\na model-free offline RL algorithm capable of learning risk-averse policies and\ncharacterizing the entire distribution of discounted cumulative rewards, as\nopposed to merely maximizing the expected value of accumulated discounted\nreturns. Our method is rigorously evaluated through comprehensive experiments\nin both risk-sensitive and risk-neutral benchmarks, demonstrating its superior\nperformance.\n","authors":["Xiaocong Chen","Siyu Wang","Tong Yu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17992v1","updated":"2024-03-26T12:20:10Z","published":"2024-03-26T12:20:10Z","title":"Interpretable cancer cell detection with phonon microscopy using\n  multi-task conditional neural networks for inter-batch calibration","summary":"  Advances in artificial intelligence (AI) show great potential in revealing\nunderlying information from phonon microscopy (high-frequency ultrasound) data\nto identify cancerous cells. However, this technology suffers from the 'batch\neffect' that comes from unavoidable technical variations between each\nexperiment, creating confounding variables that the AI model may inadvertently\nlearn. We therefore present a multi-task conditional neural network framework\nto simultaneously achieve inter-batch calibration, by removing confounding\nvariables, and accurate cell classification of time-resolved phonon-derived\nsignals. We validate our approach by training and validating on different\nexperimental batches, achieving a balanced precision of 89.22% and an average\ncross-validated precision of 89.07% for classifying background, healthy and\ncancerous regions. Classification can be performed in 0.5 seconds with only\nsimple prior batch information required for multiple batch corrections.\nFurther, we extend our model to reconstruct denoised signals, enabling physical\ninterpretation of salient features indicating disease state including sound\nvelocity, sound attenuation and cell-adhesion to substrate.\n","authors":["Yijie Zheng","Rafael Fuentes-Dominguez","Matt Clark","George S. D. Gordon","Fernando Perez-Cota"],"pdf_url":"https://arxiv.org/pdf/2403.17992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17637v1","updated":"2024-03-26T12:12:44Z","published":"2024-03-26T12:12:44Z","title":"PeersimGym: An Environment for Solving the Task Offloading Problem with\n  Reinforcement Learning","summary":"  Task offloading, crucial for balancing computational loads across devices in\nnetworks such as the Internet of Things, poses significant optimization\nchallenges, including minimizing latency and energy usage under strict\ncommunication and storage constraints. While traditional optimization falls\nshort in scalability; and heuristic approaches lack in achieving optimal\noutcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the\nlearning of optimal offloading strategies through iterative interactions.\nHowever, the efficacy of RL hinges on access to rich datasets and\ncustom-tailored, realistic training environments. To address this, we introduce\nPeersimGym, an open-source, customizable simulation environment tailored for\ndeveloping and optimizing task offloading strategies within computational\nnetworks. PeersimGym supports a wide range of network topologies and\ncomputational constraints and integrates a \\textit{PettingZoo}-based interface\nfor RL agent deployment in both solo and multi-agent setups. Furthermore, we\ndemonstrate the utility of the environment through experiments with Deep\nReinforcement Learning agents, showcasing the potential of RL-based approaches\nto significantly enhance offloading strategies in distributed computing\nsettings. PeersimGym thus bridges the gap between theoretical RL models and\ntheir practical applications, paving the way for advancements in efficient task\noffloading methodologies.\n","authors":["Frederico Metelo","Stevo Racković","Pedro Ákos","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2403.17637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17632v1","updated":"2024-03-26T12:08:05Z","published":"2024-03-26T12:08:05Z","title":"Data-driven Energy Consumption Modelling for Electric Micromobility\n  using an Open Dataset","summary":"  The escalating challenges of traffic congestion and environmental degradation\nunderscore the critical importance of embracing E-Mobility solutions in urban\nspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,\nplay a pivotal role in this transition, offering sustainable alternatives for\nurban commuters. However, the energy consumption patterns for these tools are a\ncritical aspect that impacts their effectiveness in real-world scenarios and is\nessential for trip planning and boosting user confidence in using these. To\nthis effect, recent studies have utilised physical models customised for\nspecific mobility tools and conditions, but these models struggle with\ngeneralization and effectiveness in real-world scenarios due to a notable\nabsence of open datasets for thorough model evaluation and verification. To\nfill this gap, our work presents an open dataset, collected in Dublin, Ireland,\nspecifically designed for energy modelling research related to E-Scooters and\nE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption\nmodelling based on the dataset using a set of representative machine learning\nalgorithms and compare their performance against the contemporary mathematical\nmodels as a baseline. Our results demonstrate a notable advantage for\ndata-driven models in comparison to the corresponding mathematical models for\nestimating energy consumption. Specifically, data-driven models outperform\nphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% for\nE-Scooters based on an in-depth analysis of the dataset under certain\nassumptions.\n","authors":["Yue Ding","Sen Yan","Maqsood Hussain Shah","Hongyuan Fang","Ji Li","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17632v1.pdf","comment":"7 pages, 5 figures, 4 tables. This manuscript has been accepted by\n  the IEEE ITEC 2024"},{"id":"http://arxiv.org/abs/2311.07939v2","updated":"2024-03-26T11:54:27Z","published":"2023-11-14T06:33:41Z","title":"Discretized Distributed Optimization over Dynamic Digraphs","summary":"  We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.\n","authors":["Mohammadreza Doostmohammadian","Wei Jiang","Muwahida Liaquat","Alireza Aghasi","Houman Zarrabi"],"pdf_url":"https://arxiv.org/pdf/2311.07939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03197v4","updated":"2024-03-26T11:37:38Z","published":"2023-11-06T15:39:05Z","title":"Stable Linear Subspace Identification: A Machine Learning Approach","summary":"  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n","authors":["Loris Di Natale","Muhammad Zakwan","Bratislav Svetozarevic","Philipp Heer","Giancarlo Ferrari-Trecate","Colin N. Jones"],"pdf_url":"https://arxiv.org/pdf/2311.03197v4.pdf","comment":"Accepted at ECC 2024"},{"id":"http://arxiv.org/abs/2308.16212v2","updated":"2024-03-26T11:32:36Z","published":"2023-08-30T15:09:22Z","title":"RetroBridge: Modeling Retrosynthesis with Markov Bridges","summary":"  Retrosynthesis planning is a fundamental challenge in chemistry which aims at\ndesigning reaction pathways from commercially available starting materials to a\ntarget molecule. Each step in multi-step retrosynthesis planning requires\naccurate prediction of possible precursor molecules given the target molecule\nand confidence estimates to guide heuristic search algorithms. We model\nsingle-step retrosynthesis planning as a distribution learning problem in a\ndiscrete state space. First, we introduce the Markov Bridge Model, a generative\nframework aimed to approximate the dependency between two intractable discrete\ndistributions accessible via a finite sample of coupled data points. Our\nframework is based on the concept of a Markov bridge, a Markov process pinned\nat its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does\nnot need a tractable noise distribution as a sampling proxy and directly\noperates on the input product molecules as samples from the intractable prior\ndistribution. We then address the retrosynthesis planning problem with our\nnovel framework and introduce RetroBridge, a template-free retrosynthesis\nmodeling approach that achieves state-of-the-art results on standard evaluation\nbenchmarks.\n","authors":["Ilia Igashov","Arne Schneuing","Marwin Segler","Michael Bronstein","Bruno Correia"],"pdf_url":"https://arxiv.org/pdf/2308.16212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00093v2","updated":"2024-03-26T11:20:02Z","published":"2024-01-31T12:41:27Z","title":"ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation","summary":"  System Verilog Assertion (SVA) formulation -- a critical yet complex task is\na prerequisite in the Formal Property Verification (FPV) process.\nTraditionally, SVA formulation involves expert-driven interpretation of\nspecifications, which is timeconsuming and prone to human error. However,\nLLM-informed automatic assertion generation is gaining interest. We designeda\nnovel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA\nassertions from natural language specifications. ChIRAAG constitutes the\nsystematic breakdown of design specifications into a standardized format,\nfurther generating assertions from formatted specifications using LLM.\nFurthermore, we developed testbenches to verify/validate the LLM-generated\nassertions. Automatic feedback of log files from the simulation tool to the LLM\nensures that the framework can generate correc SVAs automatically. Only 33% of\nLLM-generated raw assertions had errors. Our results on OpenTitan designs shows\nthat LLMs can streamline and assist engineers in the assertion generation\nprocess, reshaping verification workflows.\n","authors":["Bhabesh Mali","Karthik Maddala","Sweeya Reddy","Vatsal Gupta","Chandan Karfa","Ramesh Karri"],"pdf_url":"https://arxiv.org/pdf/2402.00093v2.pdf","comment":"6 pages, 5 figures and 2 table"},{"id":"http://arxiv.org/abs/2403.17601v1","updated":"2024-03-26T11:13:35Z","published":"2024-03-26T11:13:35Z","title":"LASIL: Learner-Aware Supervised Imitation Learning For Long-term\n  Microscopic Traffic Simulation","summary":"  Microscopic traffic simulation plays a crucial role in transportation\nengineering by providing insights into individual vehicle behavior and overall\ntraffic flow. However, creating a realistic simulator that accurately\nreplicates human driving behaviors in various traffic conditions presents\nsignificant challenges. Traditional simulators relying on heuristic models\noften fail to deliver accurate simulations due to the complexity of real-world\ntraffic environments. Due to the covariate shift issue, existing imitation\nlearning-based simulators often fail to generate stable long-term simulations.\nIn this paper, we propose a novel approach called learner-aware supervised\nimitation learning to address the covariate shift problem in multi-agent\nimitation learning. By leveraging a variational autoencoder simultaneously\nmodeling the expert and learner state distribution, our approach augments\nexpert states such that the augmented state is aware of learner state\ndistribution. Our method, applied to urban traffic simulation, demonstrates\nsignificant improvements over existing state-of-the-art baselines in both\nshort-term microscopic and long-term macroscopic realism when evaluated on the\nreal-world dataset pNEUMA.\n","authors":["Ke Guo","Zhenwei Miao","Wei Jing","Weiwei Liu","Weizi Li","Dayang Hao","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.17601v1.pdf","comment":"accepted by cvpr 2024. arXiv admin note: text overlap with\n  arXiv:2306.06401"},{"id":"http://arxiv.org/abs/2403.15905v2","updated":"2024-03-26T11:11:49Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v2.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2306.00038v3","updated":"2024-03-26T11:07:30Z","published":"2023-05-31T09:51:45Z","title":"FedCSD: A Federated Learning Based Approach for Code-Smell Detection","summary":"  This paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.\n","authors":["Sadi Alawadi","Khalid Alkharabsheh","Fahed Alkhabbas","Victor Kebande","Feras M. Awaysheh","Fabio Palomba","Mohammed Awad"],"pdf_url":"https://arxiv.org/pdf/2306.00038v3.pdf","comment":"17 pages, 7 figures, Journal paper"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.17592v1","updated":"2024-03-26T11:01:53Z","published":"2024-03-26T11:01:53Z","title":"On the Benefits of Over-parameterization for Out-of-Distribution\n  Generalization","summary":"  In recent years, machine learning models have achieved success based on the\nindependently and identically distributed assumption. However, this assumption\ncan be easily violated in real-world applications, leading to the\nOut-of-Distribution (OOD) problem. Understanding how modern over-parameterized\nDNNs behave under non-trivial natural distributional shifts is essential, as\ncurrent theoretical understanding is insufficient. Existing theoretical works\noften provide meaningless results for over-parameterized models in OOD\nscenarios or even contradict empirical findings. To this end, we are\ninvestigating the performance of the over-parameterized model in terms of OOD\ngeneralization under the general benign overfitting conditions. Our analysis\nfocuses on a random feature model and examines non-trivial natural\ndistributional shifts, where the benign overfitting estimators demonstrate a\nconstant excess OOD loss, despite achieving zero excess in-distribution (ID)\nloss. We demonstrate that in this scenario, further increasing the model's\nparameterization can significantly reduce the OOD loss. Intuitively, the\nvariance term of ID loss remains low due to orthogonality of long-tail\nfeatures, meaning overfitting noise during training generally doesn't raise\ntesting loss. However, in OOD cases, distributional shift increases the\nvariance term. Thankfully, the inherent shift is unrelated to individual x,\nmaintaining the orthogonality of long-tail features. Expanding the hidden\ndimension can additionally improve this orthogonality by mapping the features\ninto higher-dimensional spaces, thereby reducing the variance term. We further\nshow that model ensembles also improve OOD loss, akin to increasing model\ncapacity. These insights explain the empirical phenomenon of enhanced OOD\ngeneralization through model ensembles, supported by consistent simulations\nwith theoretical results.\n","authors":["Yifan Hao","Yong Lin","Difan Zou","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17588v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest\n  models","summary":"  Random Forest (RF) is well-known as an efficient ensemble learning method in\nterms of predictive performance. It is also considered a Black Box because of\nits hundreds of deep decision trees. This lack of interpretability can be a\nreal drawback for acceptance of RF models in several real-world applications,\nespecially those affecting one's lives, such as in healthcare, security, and\nlaw. In this work, we present Forest-ORE, a method that makes RF interpretable\nvia an optimized rule ensemble (ORE) for local and global interpretation.\nUnlike other rule-based approaches aiming at interpreting the RF model, this\nmethod simultaneously considers several parameters that influence the choice of\nan interpretable rule ensemble. Existing methods often prioritize predictive\nperformance over interpretability coverage and do not provide information about\nexisting overlaps or interactions between rules. Forest-ORE uses a\nmixed-integer optimization program to build an ORE that considers the trade-off\nbetween predictive performance, interpretability coverage, and model size (size\nof the rule ensemble, rule lengths, and rule overlaps). In addition to\nproviding an ORE competitive in predictive performance with RF, this method\nenriches the ORE through other rules that afford complementary information. It\nalso enables monitoring of the rule selection process and delivers various\nmetrics that can be used to generate a graphical representation of the final\nmodel. This framework is illustrated through an example, and its robustness is\nassessed through 36 benchmark datasets. A comparative analysis of well-known\nmethods shows that Forest-ORE provides an excellent trade-off between\npredictive performance, interpretability coverage, and model size.\n","authors":["Haddouchi Maissae","Berrado Abdelaziz"],"pdf_url":"https://arxiv.org/pdf/2403.17588v1.pdf","comment":"48 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07773v2","updated":"2024-03-26T10:34:14Z","published":"2022-04-16T10:27:23Z","title":"FedCau: A Proactive Stop Policy for Communication and Computation\n  Efficient Federated Learning","summary":"  This paper investigates efficient distributed training of a Federated\nLearning~(FL) model over a wireless network of wireless devices. The\ncommunication iterations of the distributed training algorithm may be\nsubstantially deteriorated or even blocked by the effects of the devices'\nbackground traffic, packet losses, congestion, or latency. We abstract the\ncommunication-computation impacts as an `iteration cost' and propose a\ncost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an\niteration-termination method that trade-offs the training performance and\nnetworking costs. We apply our approach when clients use the slotted-ALOHA, the\ncarrier-sense multiple access with collision avoidance~(CSMA/CA), and the\northogonal frequency-division multiple access~(OFDMA) protocols. We show that,\ngiven a total cost budget, the training performance degrades as either the\nbackground communication traffic or the dimension of the training problem\nincreases. Our results demonstrate the importance of proactively designing\noptimal cost-efficient stopping criteria to avoid unnecessary\ncommunication-computation costs to achieve only a marginal FL training\nimprovement. We validate our method by training and testing FL over the MNIST\ndataset. Finally, we apply our approach to existing communication efficient FL\nmethods from the literature, achieving further efficiency. We conclude that\ncost-efficient stopping criteria are essential for the success of practical FL\nover wireless networks.\n","authors":["Afsaneh Mahmoudi","Hossein S. Ghadikolaei","José Mairton Barros Da Silva Júnior","Carlo Fischione"],"pdf_url":"https://arxiv.org/pdf/2204.07773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17572v1","updated":"2024-03-26T10:25:21Z","published":"2024-03-26T10:25:21Z","title":"Enhancing Privacy in Federated Learning through Local Training","summary":"  In this paper we propose the federated private local training algorithm\n(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive\ncommunications and (ii) privacy preservation. We address (i) by allowing for\nboth partial participation and local training, which significantly reduce the\nnumber of communication rounds between the central coordinator and computing\nagents. The algorithm matches the state of the art in the sense that the use of\nlocal training demonstrably does not impact accuracy. Additionally, agents have\nthe flexibility to choose from various local training solvers, such as\n(stochastic) gradient descent and accelerated gradient descent. Further, we\ninvestigate how employing local training can enhance privacy, addressing point\n(ii). In particular, we derive differential privacy bounds and highlight their\ndependence on the number of local training epochs. We assess the effectiveness\nof the proposed algorithm by comparing it to alternative techniques,\nconsidering both theoretical analysis and numerical results from a\nclassification task.\n","authors":["Nicola Bastianello","Changxin Liu","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03611v2","updated":"2024-03-26T10:13:11Z","published":"2023-12-06T16:55:53Z","title":"DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions","summary":"  Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.\n","authors":["Yunhan Yang","Yukun Huang","Xiaoyang Wu","Yuan-Chen Guo","Song-Hai Zhang","Hengshuang Zhao","Tong He","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03611v2.pdf","comment":"Project Page: https://yhyang-myron.github.io/DreamComposer/"},{"id":"http://arxiv.org/abs/2403.17561v1","updated":"2024-03-26T10:10:53Z","published":"2024-03-26T10:10:53Z","title":"A Survey on Deep Learning and State-of-the-arts Applications","summary":"  Deep learning, a branch of artificial intelligence, is a computational model\nthat uses multiple layers of interconnected units (neurons) to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is a challenging task due to the algorithm`s\ncomplexity and the dynamic nature of real-world problems. Several studies have\nreviewed deep learning concepts and applications. However, the studies mostly\nfocused on the types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art of deep\nlearning models and their applications in solving complex problems across\ndifferent domains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing. We highlight the key features of the models and their effectiveness\nin solving the problems within each domain. Furthermore, this study presents\nthe fundamentals of deep learning, various deep learning model types and\nprominent convolutional neural network architectures. Finally, challenges and\nfuture directions in deep learning research are discussed to offer a broader\nperspective for future researchers.\n","authors":["Mohd Halim Mohd Noor","Ayokunle Olalekan Ige"],"pdf_url":"https://arxiv.org/pdf/2403.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.01050v6","updated":"2024-03-26T09:48:49Z","published":"2023-07-03T14:28:36Z","title":"Transport meets Variational Inference: Controlled Monte Carlo Diffusions","summary":"  Connecting optimal transport and variational inference, we present a\nprincipled and systematic framework for sampling and generative modelling\ncentred around divergences on path space. Our work culminates in the\ndevelopment of the \\emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for\nBayesian computation, a score-based annealing technique that crucially adapts\nboth forward and backward dynamics in a diffusion model. On the way, we clarify\nthe relationship between the EM-algorithm and iterative proportional fitting\n(IPF) for Schr{\\\"o}dinger bridges, deriving as well a regularised objective\nthat bypasses the iterative bottleneck of standard IPF-updates. Finally, we\nshow that CMCD has a strong foundation in the Jarzinsky and Crooks identities\nfrom statistical physics, and that it convincingly outperforms competing\napproaches across a wide array of experiments.\n","authors":["Francisco Vargas","Shreyas Padhy","Denis Blessing","Nikolas Nüsken"],"pdf_url":"https://arxiv.org/pdf/2307.01050v6.pdf","comment":"Workshop on New Frontiers in Learning, Control, and Dynamical Systems\n  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,\n  USA, 2023"},{"id":"http://arxiv.org/abs/2403.17542v1","updated":"2024-03-26T09:44:57Z","published":"2024-03-26T09:44:57Z","title":"VDSC: Enhancing Exploration Timing with Value Discrepancy and State\n  Counts","summary":"  Despite the considerable attention given to the questions of \\textit{how\nmuch} and \\textit{how to} explore in deep reinforcement learning, the\ninvestigation into \\textit{when} to explore remains relatively less researched.\nWhile more sophisticated exploration strategies can excel in specific, often\nsparse reward environments, existing simpler approaches, such as\n$\\epsilon$-greedy, persist in outperforming them across a broader spectrum of\ndomains. The appeal of these simpler strategies lies in their ease of\nimplementation and generality across a wide range of domains. The downside is\nthat these methods are essentially a blind switching mechanism, which\ncompletely disregards the agent's internal state. In this paper, we propose to\nleverage the agent's internal state to decide \\textit{when} to explore,\naddressing the shortcomings of blind switching mechanisms. We present Value\nDiscrepancy and State Counts through homeostasis (VDSC), a novel approach for\nefficient exploration timing. Experimental results on the Atari suite\ndemonstrate the superiority of our strategy over traditional methods such as\n$\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like\nNoisy Nets.\n","authors":["Marius Captari","Remo Sasso","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2403.17542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17533v1","updated":"2024-03-26T09:39:21Z","published":"2024-03-26T09:39:21Z","title":"BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range\n  Air Combat","summary":"  Creating new air combat tactics and discovering novel maneuvers can require\nnumerous hours of expert pilots' time. Additionally, for each different combat\nscenario, the same strategies may not work since small changes in equipment\nperformance may drastically change the air combat outcome. For this reason, we\ncreated a reinforcement learning environment to help investigate potential air\ncombat tactics in the field of beyond-visual-range (BVR) air combat: the BVR\nGym. This type of air combat is important since long-range missiles are often\nthe first weapon to be used in aerial combat. Some existing environments\nprovide high-fidelity simulations but are either not open source or are not\nadapted to the BVR air combat domain. Other environments are open source but\nuse less accurate simulation models. Our work provides a high-fidelity\nenvironment based on the open-source flight dynamics simulator JSBSim and is\nadapted to the BVR air combat domain. This article describes the building\nblocks of the environment and some use cases.\n","authors":["Edvards Scukins","Markus Klein","Lars Kroon","Petter Ögren"],"pdf_url":"https://arxiv.org/pdf/2403.17533v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2207.12730v2","updated":"2024-03-26T09:35:03Z","published":"2022-07-26T08:34:17Z","title":"P2ANet: A Dataset and Benchmark for Dense Action Detection from Table\n  Tennis Match Broadcasting Videos","summary":"  While deep learning has been widely used for video analytics, such as video\nclassification and action detection, dense action detection with fast-moving\nsubjects from sports videos is still challenging. In this work, we release yet\nanother sports video benchmark \\TheName{} for \\emph{\\underline{P}}ing\n\\emph{\\underline{P}}ong-\\emph{\\underline{A}}ction detection, which consists of\n2,721 video clips collected from the broadcasting videos of professional table\ntennis matches in World Table Tennis Championships and Olympiads. We work with\na crew of table tennis professionals and referees on a specially designed\nannotation toolbox to obtain fine-grained action labels (in 14 classes) for\nevery ping-pong action that appeared in the dataset, and formulate two sets of\naction detection problems -- \\emph{action localization} and \\emph{action\nrecognition}. We evaluate a number of commonly-seen action recognition (e.g.,\nTSM, TSN, Video SwinTransformer, and Slowfast) and action localization models\n(e.g., BSN, BSN++, BMN, TCANet), using \\TheName{} for both problems, under\nvarious settings. These models can only achieve 48\\% area under the AR-AN curve\nfor localization and 82\\% top-one accuracy for recognition since the ping-pong\nactions are dense with fast-moving subjects but broadcasting videos are with\nonly 25 FPS. The results confirm that \\TheName{} is still a challenging task\nand can be used as a special benchmark for dense action detection from videos.\n","authors":["Jiang Bian","Xuhong Li","Tao Wang","Qingzhong Wang","Jun Huang","Chen Liu","Jun Zhao","Feixiang Lu","Dejing Dou","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2207.12730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17520v1","updated":"2024-03-26T09:22:37Z","published":"2024-03-26T09:22:37Z","title":"Boosting Adversarial Training via Fisher-Rao Norm-based Regularization","summary":"  Adversarial training is extensively utilized to improve the adversarial\nrobustness of deep neural networks. Yet, mitigating the degradation of standard\ngeneralization performance in adversarial-trained models remains an open\nproblem. This paper attempts to resolve this issue through the lens of model\ncomplexity. First, We leverage the Fisher-Rao norm, a geometrically invariant\nmetric for model complexity, to establish the non-trivial bounds of the\nCross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer\nPerceptron. Then we generalize a complexity-related variable, which is\nsensitive to the changes in model width and the trade-off factors in\nadversarial training. Moreover, intensive empirical evidence validates that\nthis variable highly correlates with the generalization gap of Cross-Entropy\nloss between adversarial-trained and standard-trained models, especially during\nthe initial and final phases of the training process. Building upon this\nobservation, we propose a novel regularization framework, called Logit-Oriented\nAdversarial Training (LOAT), which can mitigate the trade-off between\nrobustness and accuracy while imposing only a negligible increase in\ncomputational overhead. Our extensive experiments demonstrate that the proposed\nregularization strategy can boost the performance of the prevalent adversarial\ntraining algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,\nacross various network architectures. Our code will be available at\nhttps://github.com/TrustAI/LOAT.\n","authors":["Xiangyu Yin","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2403.17520v1.pdf","comment":"This paper has been accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.17515v1","updated":"2024-03-26T09:18:50Z","published":"2024-03-26T09:18:50Z","title":"Prediction-sharing During Training and Inference","summary":"  Two firms are engaged in a competitive prediction task. Each firm has two\nsources of data -- labeled historical data and unlabeled inference-time data --\nand uses the former to derive a prediction model, and the latter to make\npredictions on new instances. We study data-sharing contracts between the\nfirms. The novelty of our study is to introduce and highlight the differences\nbetween contracts that share prediction models only, contracts to share\ninference-time predictions only, and contracts to share both. Our analysis\nproceeds on three levels. First, we develop a general Bayesian framework that\nfacilitates our study. Second, we narrow our focus to two natural settings\nwithin this framework: (i) a setting in which the accuracy of each firm's\nprediction model is common knowledge, but the correlation between the\nrespective models is unknown; and (ii) a setting in which two hypotheses exist\nregarding the optimal predictor, and one of the firms has a structural\nadvantage in deducing it. Within these two settings we study optimal contract\nchoice. More specifically, we find the individually rational and Pareto-optimal\ncontracts for some notable cases, and describe specific settings where each of\nthe different sharing contracts emerge as optimal. Finally, in the third level\nof our analysis we demonstrate the applicability of our concepts in a synthetic\nsimulation using real loan data.\n","authors":["Yotam Gafni","Ronen Gradwohl","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2403.17515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17507v1","updated":"2024-03-26T09:09:40Z","published":"2024-03-26T09:09:40Z","title":"EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields","summary":"  Machine learning force fields (MLFFs) have emerged as a promising approach to\nbridge the accuracy of quantum mechanical methods and the efficiency of\nclassical force fields. However, the abundance of MLFF models and the challenge\nof accurately predicting atomic forces pose significant obstacles in their\npractical application. In this paper, we propose a novel ensemble learning\nframework, EL-MLFFs, which leverages the stacking method to integrate\npredictions from diverse MLFFs and enhance force prediction accuracy. By\nconstructing a graph representation of molecular structures and employing a\ngraph neural network (GNN) as the meta-model, EL-MLFFs effectively captures\natomic interactions and refines force predictions. We evaluate our approach on\ntwo distinct datasets: methane molecules and methanol adsorbed on a Cu(100)\nsurface. The results demonstrate that EL-MLFFs significantly improves force\nprediction accuracy compared to individual MLFFs, with the ensemble of all\neight models yielding the best performance. Moreover, our ablation study\nhighlights the crucial roles of the residual network and graph attention layers\nin the model's architecture. The EL-MLFFs framework offers a promising solution\nto the challenges of model selection and force prediction accuracy in MLFFs,\npaving the way for more reliable and efficient molecular simulations.\n","authors":["Bangchen Yin","Yue Yin","Yuda W. Tang","Hai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17507v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.17503v1","updated":"2024-03-26T09:04:18Z","published":"2024-03-26T09:04:18Z","title":"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) under an exemplar-free constraint has\npresented a significant challenge. Existing methods adhering to this constraint\nare prone to catastrophic forgetting, far more so than replay-based techniques\nthat retain access to past samples. In this paper, to solve the exemplar-free\nCIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The\nDS-AL contains a main stream offering an analytical (i.e., closed-form) linear\nsolution, and a compensation stream improving the inherent under-fitting\nlimitation due to adopting linear mapping. The main stream redefines the CIL\nproblem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an\nequivalence between the CIL and its joint-learning counterpart. The\ncompensation stream is governed by a Dual-Activation Compensation (DAC) module.\nThis module re-activates the embedding with a different activation function\nfrom the main stream one, and seeks fitting compensation by projecting the\nembedding to the null space of the main stream's linear mapping. Empirical\nresults demonstrate that the DS-AL, despite being an exemplar-free technique,\ndelivers performance comparable with or better than that of replay-based\nmethods across various datasets, including CIFAR-100, ImageNet-100 and\nImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to\nexecute CIL in a phase-invariant manner. This is evidenced by a\nnever-before-seen 500-phase CIL ImageNet task, which performs on a level\nidentical to a 5-phase one. Our codes are available at\nhttps://github.com/ZHUANGHP/Analytic-continual-learning.\n","authors":["Huiping Zhuang","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17503v1.pdf","comment":"Accepted in AAAI 2024"},{"id":"http://arxiv.org/abs/2403.17500v1","updated":"2024-03-26T08:59:37Z","published":"2024-03-26T08:59:37Z","title":"Variational Graph Auto-Encoder Based Inductive Learning Method for\n  Semi-Supervised Classification","summary":"  Graph representation learning is a fundamental research issue in various\ndomains of applications, of which the inductive learning problem is\nparticularly challenging as it requires models to generalize to unseen graph\nstructures during inference. In recent years, graph neural networks (GNNs) have\nemerged as powerful graph models for inductive learning tasks such as node\nclassification, whereas they typically heavily rely on the annotated nodes\nunder a fully supervised training setting. Compared with the GNN-based methods,\nvariational graph auto-encoders (VGAEs) are known to be more generalizable to\ncapture the internal structural information of graphs independent of node\nlabels and have achieved prominent performance on multiple unsupervised\nlearning tasks. However, so far there is still a lack of work focusing on\nleveraging the VGAE framework for inductive learning, due to the difficulties\nin training the model in a supervised manner and avoiding over-fitting the\nproximity information of graphs. To solve these problems and improve the model\nperformance of VGAEs for inductive graph representation learning, in this work,\nwe propose the Self-Label Augmented VGAE model. To leverage the label\ninformation for training, our model takes node labels as one-hot encoded inputs\nand then performs label reconstruction in model training. To overcome the\nscarcity problem of node labels for semi-supervised settings, we further\npropose the Self-Label Augmentation Method (SLAM), which uses pseudo labels\ngenerated by our model with a node-wise masking approach to enhance the label\ninformation. Experiments on benchmark inductive learning graph datasets verify\nthat our proposed model archives promising results on node classification with\nparticular superiority under semi-supervised learning settings.\n","authors":["Hanxuan Yang","Zhaoxin Yu","Qingchao Kong","Wei Liu","Wenji Mao"],"pdf_url":"https://arxiv.org/pdf/2403.17500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10891v3","updated":"2024-03-26T08:50:19Z","published":"2023-02-06T10:08:42Z","title":"An Implicit GNN Solver for Poisson-like problems","summary":"  This paper presents $\\Psi$-GNN, a novel Graph Neural Network (GNN) approach\nfor solving the ubiquitous Poisson PDE problems with mixed boundary conditions.\nBy leveraging the Implicit Layer Theory, $\\Psi$-GNN models an \"infinitely\" deep\nnetwork, thus avoiding the empirical tuning of the number of required Message\nPassing layers to attain the solution. Its original architecture explicitly\ntakes into account the boundary conditions, a critical prerequisite for\nphysical applications, and is able to adapt to any initially provided solution.\n$\\Psi$-GNN is trained using a \"physics-informed\" loss, and the training process\nis stable by design, and insensitive to its initialization. Furthermore, the\nconsistency of the approach is theoretically proven, and its flexibility and\ngeneralization efficiency are experimentally demonstrated: the same learned\nmodel can accurately handle unstructured meshes of various sizes, as well as\ndifferent boundary conditions. To the best of our knowledge, $\\Psi$-GNN is the\nfirst physics-informed GNN-based method that can handle various unstructured\ndomains, boundary conditions and initial solutions while also providing\nconvergence guarantees.\n","authors":["Matthieu Nastorg","Michele Alessandro Bucci","Thibault Faney","Jean-Marc Gratien","Guillaume Charpiat","Marc Schoenauer"],"pdf_url":"https://arxiv.org/pdf/2302.10891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13143v2","updated":"2024-03-26T08:36:47Z","published":"2023-02-25T19:11:44Z","title":"Ensemble learning for Physics Informed Neural Networks: a Gradient\n  Boosting approach","summary":"  While the popularity of physics-informed neural networks (PINNs) is steadily\nrising, to this date, PINNs have not been successful in simulating multi-scale\nand singular perturbation problems. In this work, we present a new training\nparadigm referred to as \"gradient boosting\" (GB), which significantly enhances\nthe performance of physics informed neural networks (PINNs). Rather than\nlearning the solution of a given PDE using a single neural network directly,\nour algorithm employs a sequence of neural networks to achieve a superior\noutcome. This approach allows us to solve problems presenting great challenges\nfor traditional PINNs. Our numerical experiments demonstrate the effectiveness\nof our algorithm through various benchmarks, including comparisons with finite\nelement methods and PINNs. Furthermore, this work also unlocks the door to\nemploying ensemble learning techniques in PINNs, providing opportunities for\nfurther improvement in solving PDEs.\n","authors":["Zhiwei Fang","Sifan Wang","Paris Perdikaris"],"pdf_url":"https://arxiv.org/pdf/2302.13143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17480v1","updated":"2024-03-26T08:22:09Z","published":"2024-03-26T08:22:09Z","title":"Capacity Provisioning Motivated Online Non-Convex Optimization Problem\n  with Memory and Switching Cost","summary":"  An online non-convex optimization problem is considered where the goal is to\nminimize the flow time (total delay) of a set of jobs by modulating the number\nof active servers, but with a switching cost associated with changing the\nnumber of active servers over time. Each job can be processed by at most one\nfixed speed server at any time. Compared to the usual online convex\noptimization (OCO) problem with switching cost, the objective function\nconsidered is non-convex and more importantly, at each time, it depends on all\npast decisions and not just the present one. Both worst-case and stochastic\ninputs are considered; for both cases, competitive algorithms are derived.\n","authors":["Rahul Vaze","Jayakrishnan Nair"],"pdf_url":"https://arxiv.org/pdf/2403.17480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.01432v2","updated":"2024-03-26T08:21:24Z","published":"2021-02-02T11:05:34Z","title":"Bayesian data-driven discovery of partial differential equations with\n  variable coefficients","summary":"  The discovery of Partial Differential Equations (PDEs) is an essential task\nfor applied science and engineering. However, data-driven discovery of PDEs is\ngenerally challenging, primarily stemming from the sensitivity of the\ndiscovered equation to noise and the complexities of model selection. In this\nwork, we propose an advanced Bayesian sparse learning algorithm for PDE\ndiscovery with variable coefficients, predominantly when the coefficients are\nspatially or temporally dependent. Specifically, we apply threshold Bayesian\ngroup Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a\nGibbs sampler for Bayesian posterior estimation of PDE coefficients. This\napproach not only enhances the robustness of point estimation with valid\nuncertainty quantification but also relaxes the computational burden from\nBayesian inference through the integration of coefficient thresholds as an\napproximate MCMC method. Moreover, from the quantified uncertainties, we\npropose a Bayesian total error bar criteria for model selection, which\noutperforms classic metrics including the root mean square and the Akaike\ninformation criterion. The capability of this method is illustrated by the\ndiscovery of several classical benchmark PDEs with spatially or temporally\nvarying coefficients from solution data obtained from the reference\nsimulations. In the experiments, we show that the tBGL-SS method is more robust\nthan the baseline methods under noisy environments and provides better model\nselection criteria along the regularization path.\n","authors":["Aoxue Chen","Yifan Du","Liyao Mars Gao","Guang Lin"],"pdf_url":"https://arxiv.org/pdf/2102.01432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17479v1","updated":"2024-03-26T08:19:29Z","published":"2024-03-26T08:19:29Z","title":"Natural Language Requirements Testability Measurement Based on\n  Requirement Smells","summary":"  Requirements form the basis for defining software systems' obligations and\ntasks. Testable requirements help prevent failures, reduce maintenance costs,\nand make it easier to perform acceptance tests. However, despite the importance\nof measuring and quantifying requirements testability, no automatic approach\nfor measuring requirements testability has been proposed based on the\nrequirements smells, which are at odds with the requirements testability. This\npaper presents a mathematical model to evaluate and rank the natural language\nrequirements testability based on an extensive set of nine requirements smells,\ndetected automatically, and acceptance test efforts determined by requirement\nlength and its application domain. Most of the smells stem from uncountable\nadjectives, context-sensitive, and ambiguous words. A comprehensive dictionary\nis required to detect such words. We offer a neural word-embedding technique to\ngenerate such a dictionary automatically. Using the dictionary, we could\nautomatically detect Polysemy smell (domain-specific ambiguity) for the first\ntime in 10 application domains. Our empirical study on nearly 1000 software\nrequirements from six well-known industrial and academic projects demonstrates\nthat the proposed smell detection approach outperforms Smella, a\nstate-of-the-art tool, in detecting requirements smells. The precision and\nrecall of smell detection are improved with an average of 0.03 and 0.33,\nrespectively, compared to the state-of-the-art. The proposed requirement\ntestability model measures the testability of 985 requirements with a mean\nabsolute error of 0.12 and a mean squared error of 0.03, demonstrating the\nmodel's potential for practical use.\n","authors":["Morteza Zakeri-Nasrabadi","Saeed Parsa"],"pdf_url":"https://arxiv.org/pdf/2403.17479v1.pdf","comment":"45 pages, 16 figures, and 13 tables; submitted as a journal paper"},{"id":"http://arxiv.org/abs/2311.08744v2","updated":"2024-03-26T08:14:22Z","published":"2023-11-15T07:25:14Z","title":"Graph Signal Diffusion Model for Collaborative Filtering","summary":"  Collaborative filtering is a critical technique in recommender systems. Among\nvarious methods, an increasingly popular paradigm is to reconstruct user-item\ninteractions based on the historical observations. This can be viewed as a\nconditional generative task, where recently developed diffusion model\ndemonstrates great potential. However, existing studies on diffusion models\nlack effective solutions for modeling implicit feedback data. Particularly, the\nisotropic nature of the standard diffusion process fails to account for the\nheterogeneous dependencies among items, leading to a misalignment with the\ngraphical structure of the interaction space. Meanwhile, random noise\ndestroying personalized information in interaction vectors, causing difficulty\nin reverse reconstruction. In this paper, we make novel adaptions of diffusion\nmodel and propose Graph Signal Diffusion Model for Collaborative Filtering\n(named GiffCF). To better represent the high-dimensional and sparse\ndistribution of implicit feedback, we define a generalized form of denoising\ndiffusion using heat equation on the item-item similarity graph. Our forward\nprocess smooths interaction signals with an advanced family of graph filters.\nHence, instead of losing information, it involves item-item similarities as\nbeneficial prior knowledge for recommendation. To reconstruct high-quality\ninteractions, our reverse process iteratively refines and sharpens preference\nsignals in a deterministic manner, where the update direction is conditioned on\nthe user history and computed from a carefully designed two-stage denoiser.\nFinally, through extensive experiments, we show that GiffCF effectively\nleverages the advantages of both diffusion model and graph signal processing,\nand achieves state-of-the-art performance on three benchmark datasets.\n","authors":["Yunqin Zhu","Chao Wang","Qi Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v2.pdf","comment":"11 pages, 8 figures, Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2311.02766v3","updated":"2024-03-26T08:05:11Z","published":"2023-11-05T20:51:03Z","title":"Riemannian Laplace Approximation with the Fisher Metric","summary":"  Laplace's method approximates a target density with a Gaussian distribution\nat its mode. It is computationally efficient and asymptotically exact for\nBayesian inference due to the Bernstein-von Mises theorem, but for complex\ntargets and finite-data posteriors it is often too crude an approximation. A\nrecent generalization of the Laplace Approximation transforms the Gaussian\napproximation according to a chosen Riemannian geometry providing a richer\napproximation family, while still retaining computational efficiency. However,\nas shown here, its properties depend heavily on the chosen metric, indeed the\nmetric adopted in previous work results in approximations that are overly\nnarrow as well as being biased even at the limit of infinite data. We correct\nthis shortcoming by developing the approximation family further, deriving two\nalternative variants that are exact at the limit of infinite data, extending\nthe theoretical analysis of the method, and demonstrating practical\nimprovements in a range of experiments.\n","authors":["Hanlin Yu","Marcelo Hartmann","Bernardo Williams","Mark Girolami","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2311.02766v3.pdf","comment":"AISTATS 2024, with additional fixes"},{"id":"http://arxiv.org/abs/2403.16861v2","updated":"2024-03-26T07:56:21Z","published":"2024-03-25T15:26:10Z","title":"DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts","summary":"  The DISL dataset features a collection of $514,506$ unique Solidity files\nthat have been deployed to Ethereum mainnet. It caters to the need for a large\nand diverse dataset of real-world smart contracts. DISL serves as a resource\nfor developing machine learning systems and for benchmarking software\nengineering tools designed for smart contracts. By aggregating every verified\nsmart contract from Etherscan up to January 15, 2024, DISL surpasses existing\ndatasets in size and recency.\n","authors":["Gabriele Morello","Mojtaba Eshghie","Sofia Bobadilla","Martin Monperrus"],"pdf_url":"https://arxiv.org/pdf/2403.16861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17467v1","updated":"2024-03-26T07:55:45Z","published":"2024-03-26T07:55:45Z","title":"A Unified Kernel for Neural Network Learning","summary":"  Past decades have witnessed a great interest in the distinction and\nconnection between neural network learning and kernel learning. Recent\nadvancements have made theoretical progress in connecting infinite-wide neural\nnetworks and Gaussian processes. Two predominant approaches have emerged: the\nNeural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The\nformer, rooted in Bayesian inference, represents a zero-order kernel, while the\nlatter, grounded in the tangent space of gradient descents, is a first-order\nkernel. In this paper, we present the Unified Neural Kernel (UNK), which\ncharacterizes the learning dynamics of neural networks with gradient descents\nand parameter initialization. The proposed UNK kernel maintains the limiting\nproperties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite\nlearning step and converging to NNGP as the learning step approaches infinity.\nBesides, we also theoretically characterize the uniform tightness and learning\nconvergence of the UNK kernel, providing comprehensive insights into this\nunified kernel. Experimental results underscore the effectiveness of our\nproposed method.\n","authors":["Shao-Qun Zhang","Zong-Yi Chen","Yong-Ming Tian","Xun Lu"],"pdf_url":"https://arxiv.org/pdf/2403.17467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12086v2","updated":"2024-03-26T07:54:02Z","published":"2023-11-20T13:45:21Z","title":"Masked Autoencoders Are Robust Neural Architecture Search Learners","summary":"  Neural Architecture Search (NAS) currently relies heavily on labeled data,\nwhich is both expensive and time-consuming to acquire. In this paper, we\npropose a novel NAS framework based on Masked Autoencoders (MAE) that\neliminates the need for labeled data during the search process. By replacing\nthe supervised learning objective with an image reconstruction task, our\napproach enables the robust discovery of network architectures without\ncompromising performance and generalization ability. Additionally, we address\nthe problem of performance collapse encountered in the widely-used\nDifferentiable Architecture Search (DARTS) method in the unsupervised paradigm\nby introducing a multi-scale decoder. Through extensive experiments conducted\non various search spaces and datasets, we demonstrate the effectiveness and\nrobustness of the proposed method, providing empirical evidence of its\nsuperiority over baseline approaches.\n","authors":["Yiming Hu","Xiangxiang Chu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07728v5","updated":"2024-03-26T07:43:08Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17447v1","updated":"2024-03-26T07:26:00Z","published":"2024-03-26T07:26:00Z","title":"Chain of Compression: A Systematic Approach to Combinationally Compress\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have achieved significant popularity,\nbut their computational and memory intensity poses challenges for\nresource-constrained computing systems, particularly with the prerequisite of\nreal-time performance. To release this burden, model compression has become an\nimportant research focus. Many approaches like quantization, pruning, early\nexit, and knowledge distillation have demonstrated the effect of reducing\nredundancy in neural networks. Upon closer examination, it becomes apparent\nthat each approach capitalizes on its unique features to compress the neural\nnetwork, and they can also exhibit complementary behavior when combined. To\nexplore the interactions and reap the benefits from the complementary features,\nwe propose the Chain of Compression, which works on the combinational sequence\nto apply these common techniques to compress the neural network. Validated on\nthe image-based regression and classification networks across different data\nsets, our proposed Chain of Compression can significantly compress the\ncomputation cost by 100-1000 times with ignorable accuracy loss compared with\nthe baseline model.\n","authors":["Yingtao Shen","Minqing Sun","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2403.17447v1.pdf","comment":"10 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.17445v1","updated":"2024-03-26T07:23:46Z","published":"2024-03-26T07:23:46Z","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model","summary":"  Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.\n","authors":["Jiqun Chu","Zuoquan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17445v1.pdf","comment":"12 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.17436v1","updated":"2024-03-26T07:05:06Z","published":"2024-03-26T07:05:06Z","title":"Particle identification with machine learning from incomplete data in\n  the ALICE experiment","summary":"  The ALICE experiment at the LHC measures properties of the strongly\ninteracting matter formed in ultrarelativistic heavy-ion collisions. Such\nstudies require accurate particle identification (PID). ALICE provides PID\ninformation via several detectors for particles with momentum from about 100\nMeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular\ncuts. Acmuch better performance can be achieved with machine learning (ML)\nmethods. Our solution uses multiple neural networks (NN) serving as binary\nclassifiers. Moreover, we extended our particle classifier with Feature Set\nEmbedding and attention in order to train on data with incomplete samples. We\nalso present the integration of the ML project with the ALICE analysis\nsoftware, and we discuss domain adaptation, the ML technique needed to transfer\nthe knowledge between simulated and real experimental data.\n","authors":["Maja Karwowska","Łukasz Graczykowski","Kamil Deja","Miłosz Kasak","Małgorzata Janik"],"pdf_url":"https://arxiv.org/pdf/2403.17436v1.pdf","comment":"Proceedings of 3rd Artificial Intelligence for the Electron Ion\n  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to\n  JINST"},{"id":"http://arxiv.org/abs/2403.17431v1","updated":"2024-03-26T06:57:23Z","published":"2024-03-26T06:57:23Z","title":"Robust and Scalable Model Editing for Large Language Models","summary":"  Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.\n","authors":["Yingfa Chen","Zhengyan Zhang","Xu Han","Chaojun Xiao","Zhiyuan Liu","Chen Chen","Kuai Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.17431v1.pdf","comment":"LREC-COLING 2024 paper, 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.01557v2","updated":"2024-03-26T06:50:43Z","published":"2023-08-03T06:36:21Z","title":"Motion Planning Diffusion: Learning and Planning of Robot Motions with\n  Diffusion Models","summary":"  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n","authors":["Joao Carvalho","An T. Le","Mark Baierl","Dorothea Koert","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2308.01557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.04088v4","updated":"2024-03-26T06:47:13Z","published":"2021-06-08T04:05:30Z","title":"A Lightweight and Gradient-Stable Neural Layer","summary":"  To enhance resource efficiency and model deployability of neural networks, we\npropose a neural-layer architecture based on Householder weighting and\nabsolute-value activating, called Householder-absolute neural layer or simply\nHan-layer. Compared to a fully connected layer with $d$-neurons and $d$\noutputs, a Han-layer reduces the number of parameters and the corresponding\ncomputational complexity from $O(d^2)$ to $O(d)$. {The Han-layer structure\nguarantees that the Jacobian of the layer function is always orthogonal, thus\nensuring gradient stability (i.e., free of gradient vanishing or exploding\nissues) for any Han-layer sub-networks.} Extensive numerical experiments show\nthat one can strategically use Han-layers to replace fully connected (FC)\nlayers, reducing the number of model parameters while maintaining or even\nimproving the generalization performance. We will also showcase the\ncapabilities of the Han-layer architecture on a few small stylized models, and\ndiscuss its current limitations.\n","authors":["Yueyao Yu","Yin Zhang"],"pdf_url":"https://arxiv.org/pdf/2106.04088v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17425v1","updated":"2024-03-26T06:42:23Z","published":"2024-03-26T06:42:23Z","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion\n  Rate Prediction with a Single Model","summary":"  In real-world advertising systems, conversions have different types in nature\nand ads can be shown in different display scenarios, both of which highly\nimpact the actual conversion rate (CVR). This results in the multi-type and\nmulti-scenario CVR prediction problem. A desired model for this problem should\nsatisfy the following requirements: 1) Accuracy: the model should achieve\nfine-grained accuracy with respect to any conversion type in any display\nscenario. 2) Scalability: the model parameter size should be affordable. 3)\nConvenience: the model should not require a large amount of effort in data\npartitioning, subset processing and separate storage. Existing approaches\ncannot simultaneously satisfy these requirements. For example, building a\nseparate model for each (conversion type, display scenario) pair is neither\nscalable nor convenient. Building a unified model trained on all the data with\nconversion type and display scenario included as two features is not accurate\nenough. In this paper, we propose the Masked Multi-domain Network (MMN) to\nsolve this problem. To achieve the accuracy requirement, we model\ndomain-specific parameters and propose a dynamically weighted loss to account\nfor the loss scale imbalance issue within each mini-batch. To achieve the\nscalability requirement, we propose a parameter sharing and composition\nstrategy to reduce model parameters from a product space to a sum space. To\nachieve the convenience requirement, we propose an auto-masking strategy which\ncan take mixed data from all the domains as input. It avoids the overhead\ncaused by data partitioning, individual processing and separate storage. Both\noffline and online experimental results validate the superiority of MMN for\nmulti-type and multi-scenario CVR prediction. MMN is now the serving model for\nreal-time CVR prediction in UC Toutiao.\n","authors":["Wentao Ouyang","Xiuwu Zhang","Chaofeng Guo","Shukui Ren","Yupei Sui","Kun Zhang","Jinmei Luo","Yunfeng Chen","Dongbo Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2403.17425v1.pdf","comment":"CIKM 2023 (larger figures)"},{"id":"http://arxiv.org/abs/2403.17410v1","updated":"2024-03-26T06:06:01Z","published":"2024-03-26T06:06:01Z","title":"On permutation-invariant neural networks","summary":"  Conventional machine learning algorithms have traditionally been designed\nunder the assumption that input data follows a vector-based format, with an\nemphasis on vector-centric paradigms. However, as the demand for tasks\ninvolving set-based inputs has grown, there has been a paradigm shift in the\nresearch community towards addressing these challenges. In recent years, the\nemergence of neural network architectures such as Deep Sets and Transformers\nhas presented a significant advancement in the treatment of set-based data.\nThese architectures are specifically engineered to naturally accommodate sets\nas input, enabling more effective representation and processing of set\nstructures. Consequently, there has been a surge of research endeavors\ndedicated to exploring and harnessing the capabilities of these architectures\nfor various tasks involving the approximation of set functions. This\ncomprehensive survey aims to provide an overview of the diverse problem\nsettings and ongoing research efforts pertaining to neural networks that\napproximate set functions. By delving into the intricacies of these approaches\nand elucidating the associated challenges, the survey aims to equip readers\nwith a comprehensive understanding of the field. Through this comprehensive\nperspective, we hope that researchers can gain valuable insights into the\npotential applications, inherent limitations, and future directions of\nset-based neural networks. Indeed, from this survey we gain two insights: i)\nDeep Sets and its variants can be generalized by differences in the aggregation\nfunction, and ii) the behavior of Deep Sets is sensitive to the choice of the\naggregation function. From these observations, we show that Deep Sets, one of\nthe well-known permutation-invariant neural networks, can be generalized in the\nsense of a quasi-arithmetic mean.\n","authors":["Masanari Kimura","Ryotaro Shimizu","Yuki Hirakawa","Ryosuke Goto","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2403.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03520v2","updated":"2024-03-26T06:05:13Z","published":"2023-11-06T20:58:07Z","title":"Brain Networks and Intelligence: A Graph Neural Network Based Approach\n  to Resting State fMRI Data","summary":"  Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful\ntool for investigating the relationship between brain function and cognitive\nprocesses as it allows for the functional organization of the brain to be\ncaptured without relying on a specific task or stimuli. In this paper, we\npresent a novel modeling architecture called BrainRGIN for predicting\nintelligence (fluid, crystallized, and total intelligence) using graph neural\nnetworks on rsfMRI derived static functional network connectivity matrices.\nExtending from the existing graph convolution networks, our approach\nincorporates a clustering-based embedding and graph isomorphism network in the\ngraph convolutional layer to reflect the nature of the brain sub-network\norganization and efficient network expression, in combination with TopK pooling\nand attention-based readout functions. We evaluated our proposed architecture\non a large dataset, specifically the Adolescent Brain Cognitive Development\nDataset, and demonstrated its effectiveness in predicting individual\ndifferences in intelligence. Our model achieved lower mean squared errors and\nhigher correlation scores than existing relevant graph architectures and other\ntraditional machine learning models for all of the intelligence prediction\ntasks. The middle frontal gyrus exhibited a significant contribution to both\nfluid and crystallized intelligence, suggesting their pivotal role in these\ncognitive processes. Total composite scores identified a diverse set of brain\nregions to be relevant which underscores the complex nature of total\nintelligence.\n","authors":["Bishal Thapaliya","Esra Akbas","Jiayu Chen","Raam Sapkota","Bhaskar Ray","Pranav Suresh","Vince Calhoun","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2311.03520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17407v1","updated":"2024-03-26T05:55:21Z","published":"2024-03-26T05:55:21Z","title":"Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens","summary":"  Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.\n","authors":["S M Jishanul Islam","Sadia Ahmmed","Sahid Hossain Mustakim"],"pdf_url":"https://arxiv.org/pdf/2403.17407v1.pdf","comment":"This work became the champion of the Bhashamul challenge"},{"id":"http://arxiv.org/abs/2403.17404v1","updated":"2024-03-26T05:48:02Z","published":"2024-03-26T05:48:02Z","title":"Generalization Error Analysis for Sparse Mixture-of-Experts: A\n  Preliminary Study","summary":"  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates\npredictions from several specialized sub-models (referred to as experts). This\nfusion is accomplished through a router mechanism, dynamically assigning\nweights to each expert's contribution based on the input data. Conventional MoE\nmechanisms select all available experts, incurring substantial computational\ncosts. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages\nonly a limited number, or even just one expert, significantly reducing\ncomputation overhead while empirically preserving, and sometimes even\nenhancing, performance. Despite its wide-ranging applications and these\nadvantageous characteristics, MoE's theoretical underpinnings have remained\nelusive. In this paper, we embark on an exploration of Sparse MoE's\ngeneralization error concerning various critical factors. Specifically, we\ninvestigate the impact of the number of data samples, the total number of\nexperts, the sparsity in expert selection, the complexity of the routing\nmechanism, and the complexity of individual experts. Our analysis sheds light\non \\textit{how \\textbf{sparsity} contributes to the MoE's generalization},\noffering insights from the perspective of classical learning theory.\n","authors":["Jinze Zhao","Peihao Wang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05264v2","updated":"2024-03-26T05:35:38Z","published":"2023-12-05T19:15:51Z","title":"All Rivers Run to the Sea: Private Learning with Asymmetric Flows","summary":"  Data privacy is of great concern in cloud machine-learning service platforms,\nwhen sensitive data are exposed to service providers. While private computing\nenvironments (e.g., secure enclaves), and cryptographic approaches (e.g.,\nhomomorphic encryption) provide strong privacy protection, their computing\nperformance still falls short compared to cloud GPUs. To achieve privacy\nprotection with high computing performance, we propose Delta, a new private\ntraining and inference framework, with comparable model performance as\nnon-private centralized training. Delta features two asymmetric data flows: the\nmain information-sensitive flow and the residual flow. The main part flows into\na small model while the residuals are offloaded to a large model. Specifically,\nDelta embeds the information-sensitive representations into a low-dimensional\nspace while pushing the information-insensitive part into high-dimension\nresiduals. To ensure privacy protection, the low-dimensional\ninformation-sensitive part is secured and fed to a small model in a private\nenvironment. On the other hand, the residual part is sent to fast cloud GPUs,\nand processed by a large model. To further enhance privacy and reduce the\ncommunication cost, Delta applies a random binary quantization technique along\nwith a DP-based technique to the residuals before sharing them with the public\nplatform. We theoretically show that Delta guarantees differential privacy in\nthe public environment and greatly reduces the complexity in the private\nenvironment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet\ndatasets and ResNet-18 and ResNet-34, showing that Delta achieves strong\nprivacy protection, fast training, and inference without significantly\ncompromising the model utility.\n","authors":["Yue Niu","Ramy E. Ali","Saurav Prakash","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2312.05264v2.pdf","comment":"Camera-ready for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14736v2","updated":"2024-03-26T05:25:04Z","published":"2024-03-21T13:27:57Z","title":"NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein\n  Classification in Graph Neural Networks","summary":"  Protein classification tasks are essential in drug discovery. Real-world\nprotein structures are dynamic, which will determine the properties of\nproteins. However, the existing machine learning methods, like ProNet (Wang et\nal., 2022a), only access limited conformational characteristics and protein\nside-chain features, leading to impractical protein structure and inaccuracy of\nprotein classes in their predictions. In this paper, we propose novel semantic\ndata augmentation methods, Novel Augmentation of New Node Attributes (NaNa),\nand Molecular Interactions and Geometric Upgrading (MiGu) to incorporate\nbackbone chemical and side-chain biophysical information into protein\nclassification tasks and a co-embedding residual learning framework.\nSpecifically, we leverage molecular biophysical, secondary structure, chemical\nbonds, and ionic features of proteins to facilitate protein classification\ntasks. Furthermore, our semantic augmentation methods and the co-embedding\nresidual learning framework can improve the performance of GIN (Xu et al.,\n2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%\nand 11.33% respectively. Our code is available at\nhttps://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.\n","authors":["Yi-Shan Lan","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2403.14736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19125v4","updated":"2024-03-26T05:18:13Z","published":"2023-05-30T15:36:37Z","title":"Graph Generation with $K^2$-trees","summary":"  Generating graphs from a target distribution is a significant challenge\nacross many domains, including drug discovery and social network analysis. In\nthis work, we introduce a novel graph generation method leveraging $K^2$-tree\nrepresentation, originally designed for lossless graph compression. The\n$K^2$-tree representation {encompasses inherent hierarchy while enabling\ncompact graph generation}. In addition, we make contributions by (1) presenting\na sequential $K^2$-treerepresentation that incorporates pruning, flattening,\nand tokenization processes and (2) introducing a Transformer-based architecture\ndesigned to generate the sequence by incorporating a specialized tree\npositional encoding scheme. Finally, we extensively evaluate our algorithm on\nfour general and two molecular graph datasets to confirm its superiority for\ngraph generation.\n","authors":["Yunhui Jang","Dongwoo Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2305.19125v4.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2312.02230v2","updated":"2024-03-26T05:10:53Z","published":"2023-12-04T03:43:26Z","title":"A Simple and Scalable Representation for Graph Generation","summary":"  Recently, there has been a surge of interest in employing neural networks for\ngraph generation, a fundamental statistical learning problem with critical\napplications like molecule design and community analysis. However, most\napproaches encounter significant limitations when generating large-scale\ngraphs. This is due to their requirement to output the full adjacency matrices\nwhose size grows quadratically with the number of nodes. In response to this\nchallenge, we introduce a new, simple, and scalable graph representation named\ngap encoded edge list (GEEL) that has a small representation size that aligns\nwith the number of edges. In addition, GEEL significantly reduces the\nvocabulary size by incorporating the gap encoding and bandwidth restriction\nschemes. GEEL can be autoregressively generated with the incorporation of node\npositional encoding, and we further extend GEEL to deal with attributed graphs\nby designing a new grammar. Our findings reveal that the adoption of this\ncompact representation not only enhances scalability but also bolsters\nperformance by simplifying the graph generation process. We conduct a\ncomprehensive evaluation across ten non-attributed and two molecular graph\ngeneration tasks, demonstrating the effectiveness of GEEL.\n","authors":["Yunhui Jang","Seul Lee","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2312.02230v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17381v1","updated":"2024-03-26T04:59:27Z","published":"2024-03-26T04:59:27Z","title":"Application-Driven Innovation in Machine Learning","summary":"  As applications of machine learning proliferate, innovative algorithms\ninspired by specific real-world challenges have become increasingly important.\nSuch work offers the potential for significant impact not merely in domains of\napplication but also in machine learning itself. In this paper, we describe the\nparadigm of application-driven research in machine learning, contrasting it\nwith the more standard paradigm of methods-driven research. We illustrate the\nbenefits of application-driven machine learning and how this approach can\nproductively synergize with methods-driven work. Despite these benefits, we\nfind that reviewing, hiring, and teaching practices in machine learning often\nhold back application-driven innovation. We outline how these processes may be\nimproved.\n","authors":["David Rolnick","Alan Aspuru-Guzik","Sara Beery","Bistra Dilkina","Priya L. Donti","Marzyeh Ghassemi","Hannah Kerner","Claire Monteleoni","Esther Rolf","Milind Tambe","Adam White"],"pdf_url":"https://arxiv.org/pdf/2403.17381v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.00033v4","updated":"2024-03-26T04:58:01Z","published":"2024-02-29T04:01:38Z","title":"Identification of Craving Maps among Marijuana Users via the Analysis of\n  Functional Brain Networks with High-Order Attention Graph Neural Networks","summary":"  The excessive consumption of marijuana can induce substantial psychological\nand social consequences. In this investigation, we propose an elucidative\nframework termed high-order graph attention neural networks (HOGANN) for the\nclassification of Marijuana addiction, coupled with an analysis of localized\nbrain network communities exhibiting abnormal activities among chronic\nmarijuana users. HOGANN integrates dynamic intrinsic functional brain networks,\nestimated from resting-state functional magnetic resonance imaging (rs-fMRI),\nusing long short-term memory (LSTM) to capture temporal network dynamics. We\nemploy a high-order attention module for information fusion and message passing\namong neighboring nodes, enhancing the network community analysis. Our model is\nvalidated across two distinct data cohorts, yielding substantially higher\nclassification accuracy than benchmark algorithms. Furthermore, we discern the\nmost pertinent subnetworks and cognitive regions affected by persistent\nmarijuana consumption, indicating adverse effects on functional brain networks,\nparticularly within the dorsal attention and frontoparietal networks.\nIntriguingly, our model demonstrates superior performance in cohorts exhibiting\nprolonged dependence, implying that prolonged marijuana usage induces more\npronounced alterations in brain networks. The model proficiently identifies\ncraving brain maps, thereby delineating critical brain regions for analysis.\n","authors":["Jun-En Ding","Shihao Yang","Anna Zilverstand","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17377v1","updated":"2024-03-26T04:49:11Z","published":"2024-03-26T04:49:11Z","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","summary":"  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n","authors":["Donghoon Ahn","Hyoungwon Cho","Jaewon Min","Wooseok Jang","Jungwoo Kim","SeonHwa Kim","Hyun Hee Park","Kyong Hwan Jin","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17377v1.pdf","comment":"Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance"},{"id":"http://arxiv.org/abs/2403.17373v1","updated":"2024-03-26T04:27:56Z","published":"2024-03-26T04:27:56Z","title":"AIDE: An Automatic Data Engine for Object Detection in Autonomous\n  Driving","summary":"  Autonomous vehicle (AV) systems rely on robust perception models as a\ncornerstone of safety assurance. However, objects encountered on the road\nexhibit a long-tailed distribution, with rare or unseen categories posing\nchallenges to a deployed perception model. This necessitates an expensive\nprocess of continuously curating and annotating data with significant human\neffort. We propose to leverage recent advances in vision-language and large\nlanguage models to design an Automatic Data Engine (AIDE) that automatically\nidentifies issues, efficiently curates data, improves the model through\nauto-labeling, and verifies the model through generation of diverse scenarios.\nThis process operates iteratively, allowing for continuous self-improvement of\nthe model. We further establish a benchmark for open-world detection on AV\ndatasets to comprehensively evaluate various learning paradigms, demonstrating\nour method's superior performance at a reduced cost.\n","authors":["Mingfu Liang","Jong-Chyi Su","Samuel Schulter","Sparsh Garg","Shiyu Zhao","Ying Wu","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2403.17373v1.pdf","comment":"Accepted by CVPR-2024"},{"id":"http://arxiv.org/abs/2403.17364v1","updated":"2024-03-26T04:02:09Z","published":"2024-03-26T04:02:09Z","title":"A Moreau Envelope Approach for LQR Meta-Policy Estimation","summary":"  We study the problem of policy estimation for the Linear Quadratic Regulator\n(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We\npropose a Moreau Envelope-based surrogate LQR cost, built from a finite set of\nrealizations of the uncertain system, to define a meta-policy efficiently\nadjustable to new realizations. Moreover, we design an algorithm to find an\napproximate first-order stationary point of the meta-LQR cost function.\nNumerical results show that the proposed approach outperforms naive averaging\nof controllers on new realizations of the linear system. We also provide\nempirical evidence that our method has better sample complexity than\nModel-Agnostic Meta-Learning (MAML) approaches.\n","authors":["Ashwin Aravind","Mohammad Taha Toghani","César A. Uribe"],"pdf_url":"https://arxiv.org/pdf/2403.17364v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.17353v1","updated":"2024-03-26T03:32:45Z","published":"2024-03-26T03:32:45Z","title":"Multi-Objective Trajectory Planning with Dual-Encoder","summary":"  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'\nperformance in dynamic tasks. Traditional methods rely on solving complex\nnonlinear programming problems, bringing significant delays in generating\noptimized trajectories. In this paper, we propose a two-stage approach to\naccelerate time-jerk optimal trajectory planning. Firstly, we introduce a\ndual-encoder based transformer model to establish a good preliminary\ntrajectory. This trajectory is subsequently refined through sequential\nquadratic programming to improve its optimality and robustness. Our approach\noutperforms the state-of-the-art by up to 79.72\\% in reducing trajectory\nplanning time. Compared with existing methods, our method shrinks the\noptimality gap with the objective function value decreasing by up to 29.9\\%.\n","authors":["Beibei Zhang","Tian Xiang","Chentao Mao","Yuhua Zheng","Shuai Li","Haoyi Niu","Xiangming Xi","Wenyuan Bai","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17353v1.pdf","comment":"6 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2403.17351v1","updated":"2024-03-26T03:29:42Z","published":"2024-03-26T03:29:42Z","title":"Learn from Heterophily: Heterophilous Information-enhanced Graph Neural\n  Network","summary":"  Under circumstances of heterophily, where nodes with different labels tend to\nbe connected based on semantic meanings, Graph Neural Networks (GNNs) often\nexhibit suboptimal performance. Current studies on graph heterophily mainly\nfocus on aggregation calibration or neighbor extension and address the\nheterophily issue by utilizing node features or structural information to\nimprove GNN representations. In this paper, we propose and demonstrate that the\nvaluable semantic information inherent in heterophily can be utilized\neffectively in graph learning by investigating the distribution of neighbors\nfor each individual node within the graph. The theoretical analysis is carried\nout to demonstrate the efficacy of the idea in enhancing graph learning. Based\non this analysis, we propose HiGNN, an innovative approach that constructs an\nadditional new graph structure, that integrates heterophilous information by\nleveraging node distribution to enhance connectivity between nodes that share\nsimilar semantic characteristics. We conduct empirical assessments on node\nclassification tasks using both homophilous and heterophilous benchmark\ndatasets and compare HiGNN to popular GNN baselines and SoTA methods,\nconfirming the effectiveness in improving graph representations. In addition,\nby incorporating heterophilous information, we demonstrate a notable\nenhancement in existing GNN-based approaches, and the homophily degree across\nreal-world datasets, thus affirming the efficacy of our approach.\n","authors":["Yilun Zheng","Jiahao Xu","Lihui Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15230v2","updated":"2024-03-26T03:07:56Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings. The code will be available at\nhttps://github.com/bighuang624/Troika.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Min Zhang","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17333v1","updated":"2024-03-26T02:33:36Z","published":"2024-03-26T02:33:36Z","title":"The Pursuit of Fairness in Artificial Intelligence Models: A Survey","summary":"  Artificial Intelligence (AI) models are now being utilized in all facets of\nour lives such as healthcare, education and employment. Since they are used in\nnumerous sensitive environments and make decisions that can be life altering,\npotential biased outcomes are a pressing matter. Developers should ensure that\nsuch models don't manifest any unexpected discriminatory practices like\npartiality for certain genders, ethnicities or disabled people. With the\nubiquitous dissemination of AI systems, researchers and practitioners are\nbecoming more aware of unfair models and are bound to mitigate bias in them.\nSignificant research has been conducted in addressing such issues to ensure\nmodels don't intentionally or unintentionally perpetuate bias. This survey\noffers a synopsis of the different ways researchers have promoted fairness in\nAI systems. We explore the different definitions of fairness existing in the\ncurrent literature. We create a comprehensive taxonomy by categorizing\ndifferent types of bias and investigate cases of biased AI in different\napplication domains. A thorough study is conducted of the approaches and\ntechniques employed by researchers to mitigate bias in AI models. Moreover, we\nalso delve into the impact of biased models on user experience and the ethical\nconsiderations to contemplate when developing and deploying such models. We\nhope this survey helps researchers and practitioners understand the intricate\ndetails of fairness and bias in AI systems. By sharing this thorough survey, we\naim to promote additional discourse in the domain of equitable and responsible\nAI.\n","authors":["Tahsin Alamgir Kheya","Mohamed Reda Bouadjenek","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2403.17333v1.pdf","comment":"37 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.00736v3","updated":"2024-03-26T02:33:12Z","published":"2023-09-01T21:16:02Z","title":"Prediction Error Estimation in Random Forests","summary":"  In this paper, error estimates of classification Random Forests are\nquantitatively assessed. Based on the initial theoretical framework built by\nBates et al. (2023), the true error rate and expected error rate are\ntheoretically and empirically investigated in the context of a variety of error\nestimation methods common to Random Forests. We show that in the classification\ncase, Random Forests' estimates of prediction error is closer on average to the\ntrue error rate instead of the average prediction error. This is opposite the\nfindings of Bates et al. (2023) which are given for logistic regression. We\nfurther show that our result holds across different error estimation strategies\nsuch as cross-validation, bagging, and data splitting.\n","authors":["Ian Krupkin","Johanna Hardin"],"pdf_url":"https://arxiv.org/pdf/2309.00736v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2104.00673 by other authors"},{"id":"http://arxiv.org/abs/2403.16950v2","updated":"2024-03-26T02:28:42Z","published":"2024-03-25T17:11:28Z","title":"Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators","summary":"  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n","authors":["Yinhong Liu","Han Zhou","Zhijiang Guo","Ehsan Shareghi","Ivan Vulić","Anna Korhonen","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.16950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17329v1","updated":"2024-03-26T02:24:32Z","published":"2024-03-26T02:24:32Z","title":"Deep Support Vectors","summary":"  While the success of deep learning is commonly attributed to its theoretical\nequivalence with Support Vector Machines (SVM), the practical implications of\nthis relationship have not been thoroughly explored. This paper pioneers an\nexploration in this domain, specifically focusing on the identification of Deep\nSupport Vectors (DSVs) within deep learning models. We introduce the concept of\nDeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)\nconditions tailored for deep learning. Through empirical investigations, we\nillustrate that DSVs exhibit similarities to support vectors in SVM, offering a\ntangible method to interpret the decision-making criteria of models.\nAdditionally, our findings demonstrate that models can be effectively\nreconstructed using DSVs, resembling the process in SVM. The code will be\navailable.\n","authors":["Junhoo Lee","Hyunho Lee","Kyomin Hwang","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2403.17329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13339v4","updated":"2024-03-26T01:53:30Z","published":"2023-09-23T11:21:12Z","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic","summary":"  Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.\n","authors":["Xufeng Zhao","Mengdi Li","Wenhao Lu","Cornelius Weber","Jae Hee Lee","Kun Chu","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2309.13339v4.pdf","comment":"Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT"},{"id":"http://arxiv.org/abs/2312.12467v3","updated":"2024-03-26T01:50:54Z","published":"2023-12-19T05:30:08Z","title":"Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh\n  Transformer","summary":"  Recently, many mesh-based graph neural network (GNN) models have been\nproposed for modeling complex high-dimensional physical systems. Remarkable\nachievements have been made in significantly reducing the solving time compared\nto traditional numerical solvers. These methods are typically designed to i)\nreduce the computational cost in solving physical dynamics and/or ii) propose\ntechniques to enhance the solution accuracy in fluid and rigid body dynamics.\nHowever, it remains under-explored whether they are effective in addressing the\nchallenges of flexible body dynamics, where instantaneous collisions occur\nwithin a very short timeframe. In this paper, we present Hierarchical Contact\nMesh Transformer (HCMT), which uses hierarchical mesh structures and can learn\nlong-range dependencies (occurred by collisions) among spatially distant\npositions of a body -- two close positions in a higher-level mesh correspond to\ntwo distant positions in a lower-level mesh. HCMT enables long-range\ninteractions, and the hierarchical mesh structure quickly propagates collision\neffects to faraway positions. To this end, it consists of a contact mesh\nTransformer and a hierarchical mesh Transformer (CMT and HMT, respectively).\nLastly, we propose a flexible body dynamics dataset, consisting of trajectories\nthat reflect experimental settings frequently used in the display industry for\nproduct designs. We also compare the performance of several baselines using\nwell-known benchmark datasets. Our results show that HCMT provides significant\nperformance improvements over existing methods. Our code is available at\nhttps://github.com/yuyudeep/hcmt.\n","authors":["Youn-Yeol Yu","Jeongwhan Choi","Woojin Cho","Kookjin Lee","Nayong Kim","Kiseok Chang","Chang-Seung Woo","Ilho Kim","Seok-Woo Lee","Joon-Young Yang","Sooyoung Yoon","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.12467v3.pdf","comment":"Accepted at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17312v1","updated":"2024-03-26T01:46:34Z","published":"2024-03-26T01:46:34Z","title":"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV\n  Caching","summary":"  The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.\n","authors":["Youpeng Zhao","Di Wu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17312v1.pdf","comment":"ISCA 2024"},{"id":"http://arxiv.org/abs/2311.01453v2","updated":"2024-03-26T01:44:52Z","published":"2023-11-02T17:59:04Z","title":"PPI++: Efficient Prediction-Powered Inference","summary":"  We present PPI++: a computationally lightweight methodology for estimation\nand inference based on a small labeled dataset and a typically much larger\ndataset of machine-learning predictions. The methods automatically adapt to the\nquality of available predictions, yielding easy-to-compute confidence sets --\nfor parameters of any dimensionality -- that always improve on classical\nintervals using only the labeled data. PPI++ builds on prediction-powered\ninference (PPI), which targets the same problem setting, improving its\ncomputational and statistical efficiency. Real and synthetic experiments\ndemonstrate the benefits of the proposed adaptations.\n","authors":["Anastasios N. Angelopoulos","John C. Duchi","Tijana Zrnic"],"pdf_url":"https://arxiv.org/pdf/2311.01453v2.pdf","comment":"Code available at https://github.com/aangelopoulos/ppi_py"},{"id":"http://arxiv.org/abs/2403.17308v1","updated":"2024-03-26T01:29:46Z","published":"2024-03-26T01:29:46Z","title":"Neural Multimodal Topic Modeling: A Comprehensive Evaluation","summary":"  Neural topic models can successfully find coherent and diverse topics in\ntextual data. However, they are limited in dealing with multimodal datasets\n(e.g., images and text). This paper presents the first systematic and\ncomprehensive evaluation of multimodal topic modeling of documents containing\nboth text and images. In the process, we propose two novel topic modeling\nsolutions and two novel evaluation metrics. Overall, our evaluation on an\nunprecedented rich and diverse collection of datasets indicates that both of\nour models generate coherent and diverse topics. Nevertheless, the extent to\nwhich one method outperforms the other depends on the metrics and dataset\ncombinations, which suggests further exploration of hybrid solutions in the\nfuture. Notably, our succinct human evaluation aligns with the outcomes\ndetermined by our proposed metrics. This alignment not only reinforces the\ncredibility of our metrics but also highlights the potential for their\napplication in guiding future multimodal topic modeling endeavors.\n","authors":["Felipe González-Pizarro","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2403.17308v1.pdf","comment":"Camera-Ready for LREC-COLING 2024 (Long Paper)"},{"id":"http://arxiv.org/abs/2309.10837v2","updated":"2024-03-26T01:23:52Z","published":"2023-09-19T17:01:28Z","title":"Improving Opioid Use Disorder Risk Modelling through Behavioral and\n  Genetic Feature Integration","summary":"  Opioids are an effective analgesic for acute and chronic pain, but also carry\na considerable risk of addiction leading to millions of opioid use disorder\n(OUD) cases and tens of thousands of premature deaths in the United States\nyearly. Estimating OUD risk prior to prescription could improve the efficacy of\ntreatment regimens, monitoring programs, and intervention strategies, but risk\nestimation is typically based on self-reported data or questionnaires. We\ndevelop an experimental design and computational methods that combine genetic\nvariants associated with OUD with behavioral features extracted from GPS and\nWi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility\nand genetic data do not exist for the same cohort, we develop algorithms to (1)\ngenerate mobility features from empirical distributions and (2) synthesize\nmobility and genetic samples assuming an expected level of disease\nco-occurrence. We show that integrating genetic and mobility modalities\nimproves risk modelling using classification accuracy, area under the\nprecision-recall and receiver operator characteristic curves, and $F_1$ score.\nInterpreting the fitted models suggests that mobility features have more\ninfluence on OUD risk, although the genetic contribution was significant,\nparticularly in linear models. While there exist concerns with respect to\nprivacy, security, bias, and generalizability that must be evaluated in\nclinical trials before being implemented in practice, our framework provides\npreliminary evidence that behavioral and genetic features may improve OUD risk\nestimation to assist with personalized clinical decision-making.\n","authors":["Sybille Légitime","Kaustubh Prabhu","Devin McConnell","Bing Wang","Dipak K. Dey","Derek Aguiar"],"pdf_url":"https://arxiv.org/pdf/2309.10837v2.pdf","comment":"32 pages (including References section), 8 figures. Under review by\n  PLOS One"},{"id":"http://arxiv.org/abs/2208.06620v3","updated":"2024-03-26T01:22:44Z","published":"2022-08-13T10:36:04Z","title":"Opinion Market Model: Stemming Far-Right Opinion Spread using Positive\n  Interventions","summary":"  Online extremism has severe societal consequences, including normalizing hate\nspeech, user radicalization, and increased social divisions. Various mitigation\nstrategies have been explored to address these consequences. One such strategy\nuses positive interventions: controlled signals that add attention to the\nopinion ecosystem to boost certain opinions. To evaluate the effectiveness of\npositive interventions, we introduce the Opinion Market Model (OMM), a two-tier\nonline opinion ecosystem model that considers both inter-opinion interactions\nand the role of positive interventions. The size of the opinion attention\nmarket is modeled in the first tier using the multivariate discrete-time Hawkes\nprocess; in the second tier, opinions cooperate and compete for market share,\ngiven limited attention using the market share attraction model. We demonstrate\nthe convergence of our proposed estimation scheme on a synthetic dataset. Next,\nwe test OMM on two learning tasks, applying to two real-world datasets to\npredict attention market shares and uncover latent relationships between online\nitems. The first dataset comprises Facebook and Twitter discussions containing\nmoderate and far-right opinions about bushfires and climate change. The second\ndataset captures popular VEVO artists' YouTube and Twitter attention volumes.\nOMM outperforms the state-of-the-art predictive models on both datasets and\ncaptures latent cooperation-competition relations. We uncover (1) self- and\ncross-reinforcement between far-right and moderate opinions on the bushfires\nand (2) pairwise artist relations that correlate with real-world interactions\nsuch as collaborations and long-lasting feuds. Lastly, we use OMM as a testbed\nfor positive interventions and show how media coverage modulates the spread of\nfar-right opinions.\n","authors":["Pio Calderon","Rohit Ram","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2208.06620v3.pdf","comment":"accepted in the 18th AAAI International Conference on Web and Social\n  Media (ICWSM'24)"},{"id":"http://arxiv.org/abs/2403.10949v2","updated":"2024-03-26T01:15:09Z","published":"2024-03-16T15:30:34Z","title":"SelfIE: Self-Interpretation of Large Language Model Embeddings","summary":"  How do large language models (LLMs) obtain their answers? The ability to\nexplain and control an LLM's reasoning process is key for reliability,\ntransparency, and future model developments. We propose SelfIE\n(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret\ntheir own embeddings in natural language by leveraging their ability to respond\nto inquiries about a given passage. Capable of interpreting open-world concepts\nin the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such\nas making ethical decisions, internalizing prompt injection, and recalling\nharmful knowledge. SelfIE's text descriptions on hidden embeddings also open up\nnew avenues to control LLM reasoning. We propose Supervised Control, which\nallows editing open-ended concepts while only requiring gradient computation of\nindividual layer. We extend RLHF to hidden embeddings and propose Reinforcement\nControl that erases harmful knowledge in LLM without supervision targets.\n","authors":["Haozhe Chen","Carl Vondrick","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.10949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10474v3","updated":"2024-03-26T01:11:52Z","published":"2023-05-17T17:59:16Z","title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models","summary":"  Despite tremendous progress in generating high-quality images using diffusion\nmodels, synthesizing a sequence of animated frames that are both photorealistic\nand temporally coherent is still in its infancy. While off-the-shelf\nbillion-scale datasets for image generation are available, collecting similar\nvideo data of the same scale is still challenging. Also, training a video\ndiffusion model is computationally much more expensive than its image\ncounterpart. In this work, we explore finetuning a pretrained image diffusion\nmodel with video data as a practical solution for the video synthesis task. We\nfind that naively extending the image noise prior to video noise prior in video\ndiffusion leads to sub-optimal performance. Our carefully designed video noise\nprior leads to substantially better performance. Extensive experimental\nvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attains\nSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It\nalso achieves SOTA video generation quality on the small-scale UCF-101\nbenchmark with a $10\\times$ smaller model using significantly less computation\nthan the prior art.\n","authors":["Songwei Ge","Seungjun Nah","Guilin Liu","Tyler Poon","Andrew Tao","Bryan Catanzaro","David Jacobs","Jia-Bin Huang","Ming-Yu Liu","Yogesh Balaji"],"pdf_url":"https://arxiv.org/pdf/2305.10474v3.pdf","comment":"ICCV 2023. Project webpage:\n  https://research.nvidia.com/labs/dir/pyoco"},{"id":"http://arxiv.org/abs/2403.17296v1","updated":"2024-03-26T00:51:12Z","published":"2024-03-26T00:51:12Z","title":"Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure\n  Lookup Table Computation","summary":"  Training machine learning models on data from multiple entities without\ndirect data sharing can unlock applications otherwise hindered by business,\nlegal, or ethical constraints. In this work, we design and implement new\nprivacy-preserving machine learning protocols for logistic regression and\nneural network models. We adopt a two-server model where data owners\nsecret-share their data between two servers that train and evaluate the model\non the joint data. A significant source of inefficiency and inaccuracy in\nexisting methods arises from using Yao's garbled circuits to compute non-linear\nactivation functions. We propose new methods for computing non-linear functions\nbased on secret-shared lookup tables, offering both computational efficiency\nand improved accuracy.\n  Beyond introducing leakage-free techniques, we initiate the exploration of\nrelaxed security measures for privacy-preserving machine learning. Instead of\nclaiming that the servers gain no knowledge during the computation, we contend\nthat while some information is revealed about access patterns to lookup tables,\nit maintains epsilon-dX-privacy. Leveraging this relaxation significantly\nreduces the computational resources needed for training. We present new\ncryptographic protocols tailored to this relaxed security paradigm and define\nand analyze the leakage. Our evaluations show that our logistic regression\nprotocol is up to 9x faster, and the neural network training is up to 688x\nfaster than SecureML. Notably, our neural network achieves an accuracy of 96.6%\non MNIST in 15 epochs, outperforming prior benchmarks that capped at 93.4%\nusing the same architecture.\n","authors":["Hamza Saleem","Amir Ziashahabi","Muhammad Naveed","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2403.17296v1.pdf","comment":"Accepted at Privacy Enhancing Technologies Symposium (PETS) 2024"},{"id":"http://arxiv.org/abs/2403.17287v1","updated":"2024-03-26T00:33:49Z","published":"2024-03-26T00:33:49Z","title":"Not All Federated Learning Algorithms Are Created Equal: A Performance\n  Evaluation Study","summary":"  Federated Learning (FL) emerged as a practical approach to training a model\nfrom decentralized data. The proliferation of FL led to the development of\nnumerous FL algorithms and mechanisms. Many prior efforts have given their\nprimary focus on accuracy of those approaches, but there exists little\nunderstanding of other aspects such as computational overheads, performance and\ntraining stability, etc. To bridge this gap, we conduct extensive performance\nevaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi,\nFedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning\nframework called Flame. Our comprehensive measurement study reveals that no\nsingle algorithm works best across different performance metrics. A few key\nobservations are: (1) While some state-of-the-art algorithms achieve higher\naccuracy than others, they incur either higher computation overheads (FedDyn)\nor communication overheads (SCAFFOLD). (2) Recent algorithms present smaller\nstandard deviation in accuracy across clients than FedAvg, indicating that the\nadvanced algorithms' performances are stable. (3) However, algorithms such as\nFedDyn and SCAFFOLD are more prone to catastrophic failures without the support\nof additional techniques such as gradient clipping. We hope that our empirical\nstudy can help the community to build best practices in evaluating FL\nalgorithms.\n","authors":["Gustav A. Baumgart","Jaemin Shin","Ali Payani","Myungjin Lee","Ramana Rao Kompella"],"pdf_url":"https://arxiv.org/pdf/2403.17287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17285v1","updated":"2024-03-26T00:25:32Z","published":"2024-03-26T00:25:32Z","title":"An Analysis of Switchback Designs in Reinforcement Learning","summary":"  This paper offers a detailed investigation of switchback designs in A/B\ntesting, which alternate between baseline and new policies over time. Our aim\nis to thoroughly evaluate the effects of these designs on the accuracy of their\nresulting average treatment effect (ATE) estimators. We propose a novel \"weak\nsignal analysis\" framework, which substantially simplifies the calculations of\nthe mean squared errors (MSEs) of these ATEs in Markov decision process\nenvironments. Our findings suggest that (i) when the majority of reward errors\nare positively correlated, the switchback design is more efficient than the\nalternating-day design which switches policies in a daily basis. Additionally,\nincreasing the frequency of policy switches tends to reduce the MSE of the ATE\nestimator. (ii) When the errors are uncorrelated, however, all these designs\nbecome asymptotically equivalent. (iii) In cases where the majority of errors\nare negative correlated, the alternating-day design becomes the optimal choice.\nThese insights are crucial, offering guidelines for practitioners on designing\nexperiments in A/B testing. Our analysis accommodates a variety of policy value\nestimators, including model-based estimators, least squares temporal difference\nlearning estimators, and double reinforcement learning estimators, thereby\noffering a comprehensive understanding of optimal design strategies for policy\nevaluation in reinforcement learning.\n","authors":["Qianglin Wen","Chengchun Shi","Ying Yang","Niansheng Tang","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12533v2","updated":"2024-03-26T00:22:59Z","published":"2024-01-23T07:16:32Z","title":"Near-Optimal Algorithms for Constrained k-Center Clustering with\n  Instance-level Background Knowledge","summary":"  Center-based clustering has attracted significant research interest from both\ntheory and practice. In many practical applications, input data often contain\nbackground knowledge that can be used to improve clustering results. In this\nwork, we build on widely adopted $k$-center clustering and model its input\nbackground knowledge as must-link (ML) and cannot-link (CL) constraint sets.\nHowever, most clustering problems including $k$-center are inherently\n$\\mathcal{NP}$-hard, while the more complex constrained variants are known to\nsuffer severer approximation and computation barriers that significantly limit\ntheir applicability. By employing a suite of techniques including reverse\ndominating sets, linear programming (LP) integral polyhedron, and LP duality,\nwe arrive at the first efficient approximation algorithm for constrained\n$k$-center with the best possible ratio of 2. We also construct competitive\nbaseline algorithms and empirically evaluate our approximation algorithm\nagainst them on a variety of real datasets. The results validate our\ntheoretical findings and demonstrate the great advantages of our algorithm in\nterms of clustering cost, clustering quality, and running time.\n","authors":["Longkun Guo","Chaoqi Jia","Kewen Liao","Zhigang Lu","Minhui Xue"],"pdf_url":"https://arxiv.org/pdf/2401.12533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07905v2","updated":"2024-03-26T00:21:41Z","published":"2023-06-13T16:56:13Z","title":"Omega: Optimistic EMA Gradients","summary":"  Stochastic min-max optimization has gained interest in the machine learning\ncommunity with the advancements in GANs and adversarial training. Although game\noptimization is fairly well understood in the deterministic setting, some\nissues persist in the stochastic regime. Recent work has shown that stochastic\ngradient descent-ascent methods such as the optimistic gradient are highly\nsensitive to noise or can fail to converge. Although alternative strategies\nexist, they can be prohibitively expensive. We introduce Omega, a method with\noptimistic-like updates that mitigates the impact of noise by incorporating an\nEMA of historic gradients in its update rule. We also explore a variation of\nthis algorithm that incorporates momentum. Although we do not provide\nconvergence guarantees, our experiments on stochastic games show that Omega\noutperforms the optimistic gradient method when applied to linear players.\n","authors":["Juan Ramirez","Rohan Sukumaran","Quentin Bertrand","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2306.07905v2.pdf","comment":"Oral at the LatinX in AI workshop @ ICML 2023"}]},"2024-03-25T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2403.17266v1","updated":"2024-03-25T23:19:19Z","published":"2024-03-25T23:19:19Z","title":"Exploring CausalWorld: Enhancing robotic manipulation via knowledge\n  transfer and curriculum learning","summary":"  This study explores a learning-based tri-finger robotic arm manipulating\ntask, which requires complex movements and coordination among the fingers. By\nemploying reinforcement learning, we train an agent to acquire the necessary\nskills for proficient manipulation. To enhance the efficiency and effectiveness\nof the learning process, two knowledge transfer strategies, fine-tuning and\ncurriculum learning, were utilized within the soft actor-critic architecture.\nFine-tuning allows the agent to leverage pre-trained knowledge and adapt it to\nnew tasks. Several variations like model transfer, policy transfer, and\nacross-task transfer were implemented and evaluated. To eliminate the need for\npretraining, curriculum learning decomposes the advanced task into simpler,\nprogressive stages, mirroring how humans learn. The number of learning stages,\nthe context of the sub-tasks, and the transition timing were found to be the\ncritical design parameters. The key factors of two learning strategies and\ncorresponding effects were explored in context-aware and context-unaware\nscenarios, enabling us to identify the scenarios where the methods demonstrate\noptimal performance, derive conclusive insights, and contribute to a broader\nrange of learning-based engineering applications.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2403.17266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10971v2","updated":"2024-03-25T23:14:28Z","published":"2023-10-17T03:35:27Z","title":"Context-Aware Meta-Learning","summary":"  Large Language Models like ChatGPT demonstrate a remarkable capacity to learn\nnew concepts during inference without any fine-tuning. However, visual models\ntrained to detect new objects during inference have been unable to replicate\nthis ability, and instead either perform poorly or require meta-training and/or\nfine-tuning on similar objects. In this work, we propose a meta-learning\nalgorithm that emulates Large Language Models by learning new visual concepts\nduring inference without fine-tuning. Our approach leverages a frozen\npre-trained feature extractor, and analogous to in-context learning, recasts\nvisual meta-learning as sequence modeling over datapoints with known labels and\na test datapoint with an unknown label. On 8 out of 11 meta-learning\nbenchmarks, our approach -- without meta-training or fine-tuning -- exceeds or\nmatches the state-of-the-art algorithm, P>M>F, which is meta-trained on these\nbenchmarks. Our code is available at https://github.com/cfifty/CAML.\n","authors":["Christopher Fifty","Dennis Duan","Ronald G. Junkins","Ehsan Amid","Jure Leskovec","Christopher Re","Sebastian Thrun"],"pdf_url":"https://arxiv.org/pdf/2310.10971v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17259v1","updated":"2024-03-25T23:07:31Z","published":"2024-03-25T23:07:31Z","title":"Diffusion-based Negative Sampling on Graphs for Link Prediction","summary":"  Link prediction is a fundamental task for graph analysis with important\napplications on the Web, such as social network analysis and recommendation\nsystems, etc. Modern graph link prediction methods often employ a contrastive\napproach to learn robust node representations, where negative sampling is\npivotal. Typical negative sampling methods aim to retrieve hard examples based\non either predefined heuristics or automatic adversarial approaches, which\nmight be inflexible or difficult to control. Furthermore, in the context of\nlink prediction, most previous methods sample negative nodes from existing\nsubstructures of the graph, missing out on potentially more optimal samples in\nthe latent space. To address these issues, we investigate a novel strategy of\nmulti-level negative sampling that enables negative node generation with\nflexible and controllable ``hardness'' levels from the latent space. Our\nmethod, called Conditional Diffusion-based Multi-level Negative Sampling\n(DMNS), leverages the Markov chain property of diffusion models to generate\nnegative nodes in multiple levels of variable hardness and reconcile them for\neffective graph link prediction. We further demonstrate that DMNS follows the\nsub-linear positivity principle for robust negative sampling. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness of\nDMNS.\n","authors":["Trung-Kien Nguyen","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2403.17259v1.pdf","comment":"Accepted in the TheWebConf 2024"},{"id":"http://arxiv.org/abs/2311.15404v2","updated":"2024-03-25T22:55:43Z","published":"2023-11-26T20:00:53Z","title":"Applying statistical learning theory to deep learning","summary":"  Although statistical learning theory provides a robust framework to\nunderstand supervised learning, many theoretical aspects of deep learning\nremain unclear, in particular how different architectures may lead to inductive\nbias when trained using gradient based methods. The goal of these lectures is\nto provide an overview of some of the main questions that arise when attempting\nto understand deep learning from a learning theory perspective. After a brief\nreminder on statistical learning theory and stochastic optimization, we discuss\nimplicit bias in the context of benign overfitting. We then move to a general\ndescription of the mirror descent algorithm, showing how we may go back and\nforth between a parameter space and the corresponding function space for a\ngiven learning problem, as well as how the geometry of the learning problem may\nbe represented by a metric tensor. Building on this framework, we provide a\ndetailed study of the implicit bias of gradient descent on linear diagonal\nnetworks for various regression tasks, showing how the loss function, scale of\nparameters at initialization and depth of the network may lead to various forms\nof implicit bias, in particular transitioning between kernel or feature\nlearning.\n","authors":["Cédric Gerbelot","Avetik Karagulyan","Stefani Karp","Kavya Ravichandran","Menachem Stern","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/2311.15404v2.pdf","comment":"66 pages, 20 figures"},{"id":"http://arxiv.org/abs/2403.17239v1","updated":"2024-03-25T22:39:47Z","published":"2024-03-25T22:39:47Z","title":"Manufacturing Service Capability Prediction with Graph Neural Networks","summary":"  In the current landscape, the predominant methods for identifying\nmanufacturing capabilities from manufacturers rely heavily on keyword matching\nand semantic matching. However, these methods often fall short by either\noverlooking valuable hidden information or misinterpreting critical data.\nConsequently, such approaches result in an incomplete identification of\nmanufacturers' capabilities. This underscores the pressing need for data-driven\nsolutions to enhance the accuracy and completeness of manufacturing capability\nidentification. To address the need, this study proposes a Graph Neural\nNetwork-based method for manufacturing service capability identification over a\nknowledge graph. To enhance the identification performance, this work\nintroduces a novel approach that involves aggregating information from the\ngraph nodes' neighborhoods as well as oversampling the graph data, which can be\neffectively applied across a wide range of practical scenarios. Evaluations\nconducted on a Manufacturing Service Knowledge Graph and subsequent ablation\nstudies demonstrate the efficacy and robustness of the proposed approach. This\nstudy not only contributes a innovative method for inferring manufacturing\nservice capabilities but also significantly augments the quality of\nManufacturing Service Knowledge Graphs.\n","authors":["Yunqing Li","Xiaorui Liu","Binil Starly"],"pdf_url":"https://arxiv.org/pdf/2403.17239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17238v1","updated":"2024-03-25T22:39:20Z","published":"2024-03-25T22:39:20Z","title":"Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks","summary":"  Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP.\n","authors":["Jonathan Salfity","Selma Wanna","Minkyu Choi","Mitch Pryor"],"pdf_url":"https://arxiv.org/pdf/2403.17238v1.pdf","comment":"8 pages, 3 figures. IROS 2024 Submission"},{"id":"http://arxiv.org/abs/2403.17236v1","updated":"2024-03-25T22:26:09Z","published":"2024-03-25T22:26:09Z","title":"Neural Image Compression with Quantization Rectifier","summary":"  Neural image compression has been shown to outperform traditional image\ncodecs in terms of rate-distortion performance. However, quantization\nintroduces errors in the compression process, which can degrade the quality of\nthe compressed image. Existing approaches address the train-test mismatch\nproblem incurred during quantization, the random impact of quantization on the\nexpressiveness of image features is still unsolved. This paper presents a novel\nquantization rectifier (QR) method for image compression that leverages image\nfeature correlation to mitigate the impact of quantization. Our method designs\na neural network architecture that predicts unquantized features from the\nquantized ones, preserving feature expressiveness for better image\nreconstruction quality. We develop a soft-to-predictive training technique to\nintegrate QR into existing neural image codecs. In evaluation, we integrate QR\ninto state-of-the-art neural image codecs and compare enhanced models and\nbaselines on the widely-used Kodak benchmark. The results show consistent\ncoding efficiency improvement by QR with a negligible increase in the running\ntime.\n","authors":["Wei Luo","Bo Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17236v1.pdf","comment":"Published at International Conference on Machine Learning (ICML)\n  Neural Compression Workshop 2023, Honolulu, Hawaii, USA. PMLR 202, 2023.\n  Copyright 2023 by the authors"},{"id":"http://arxiv.org/abs/2403.17233v1","updated":"2024-03-25T22:20:45Z","published":"2024-03-25T22:20:45Z","title":"Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling\n  Process","summary":"  We present an active learning algorithm for learning dynamics that leverages\nside information by explicitly incorporating prior domain knowledge into the\nsampling process. Our proposed algorithm guides the exploration toward regions\nthat demonstrate high empirical discrepancy between the observed data and an\nimperfect prior model of the dynamics derived from side information. Through\nnumerical experiments, we demonstrate that this strategy explores regions of\nhigh discrepancy and accelerates learning while simultaneously reducing model\nuncertainty. We rigorously prove that our active learning algorithm yields a\nconsistent estimate of the underlying dynamics by providing an explicit rate of\nconvergence for the maximum predictive variance. We demonstrate the efficacy of\nour approach on an under-actuated pendulum system and on the half-cheetah\nMuJoCo environment.\n","authors":["Kevin S. Miller","Adam J. Thorpe","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2403.17233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17231v1","updated":"2024-03-25T22:17:51Z","published":"2024-03-25T22:17:51Z","title":"Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from\n  Learned Hallucination","summary":"  This paper presents a self-supervised learning method to safely learn a\nmotion planner for ground robots to navigate environments with dense and\ndynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict\nobstacles, classical motion planners may not be able to keep up with limited\nonboard computation. For learning-based planners, high-quality demonstrations\nare difficult to acquire for imitation learning while reinforcement learning\nbecomes inefficient due to the high probability of collision during\nexploration. To safely and efficiently provide training data, the Learning from\nHallucination (LfH) approaches synthesize difficult navigation environments\nbased on past successful navigation experiences in relatively easy or\ncompletely open ones, but unfortunately cannot address dynamic obstacles. In\nour new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and\nlearn a novel latent distribution and sample dynamic obstacles from it, so the\ngenerated training data can be used to learn a motion planner to navigate in\ndynamic environments. Dyna-LfLH is evaluated on a ground robot in both\nsimulated and physical environments and achieves up to 25% better success rate\ncompared to baselines.\n","authors":["Saad Abdul Ghani","Zizhao Wang","Peter Stone","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17231v1.pdf","comment":"Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2312.15101v3","updated":"2024-03-25T22:13:44Z","published":"2023-12-22T22:46:48Z","title":"Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model\n  Conversions between Frameworks","summary":"  Converting deep learning models between frameworks is a common step to\nmaximize model compatibility across devices and leverage optimization features\nthat may be exclusively provided in one deep learning framework. However, this\nconversion process may be riddled with bugs, making the converted models either\nundeployable or problematic, considerably degrading their prediction\ncorrectness.\n  In this paper we propose an automated approach for fault localization and\nrepair, Fix-Con, during model conversion between deep learning frameworks.\nFix-Con is capable of detecting and fixing faults introduced in model input,\nparameters, hyperparameters, and the model graph during conversion.\n  Fix-Con uses a set of fault types (mined from surveying conversion issues\nreported \\nick{in code repositories and forums}) to localize potential\nconversion faults in the converted target model and then repair them\nappropriately, e.g., replacing the parameters of the target model with those\nfrom the source model. This is done iteratively for every image in the dataset,\ncomparing output label differences between the source model and the converted\ntarget model until all differences are resolved. We evaluate the effectiveness\nof Fix-Con in fixing model conversion bugs of three widely used image\nrecognition models converted across four different deep learning frameworks.\nOverall, Fix-Con was able to fix $462$ out of $755$ detected conversion faults,\neither completely repairing or significantly improving the performance of $14$\nout of the $15$ erroneous conversion cases.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2312.15101v3.pdf","comment":"12 pages, 4 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2403.17224v1","updated":"2024-03-25T21:56:02Z","published":"2024-03-25T21:56:02Z","title":"Uncertainty Quantification for Gradient-based Explanations in Neural\n  Networks","summary":"  Explanation methods help understand the reasons for a model's prediction.\nThese methods are increasingly involved in model debugging, performance\noptimization, and gaining insights into the workings of a model. With such\ncritical applications of these methods, it is imperative to measure the\nuncertainty associated with the explanations generated by these methods. In\nthis paper, we propose a pipeline to ascertain the explanation uncertainty of\nneural networks by combining uncertainty estimation methods and explanation\nmethods. We use this pipeline to produce explanation distributions for the\nCIFAR-10, FER+, and California Housing datasets. By computing the coefficient\nof variation of these distributions, we evaluate the confidence in the\nexplanation and determine that the explanations generated using Guided\nBackpropagation have low uncertainty associated with them. Additionally, we\ncompute modified pixel insertion/deletion metrics to evaluate the quality of\nthe generated explanations.\n","authors":["Mihir Mulye","Matias Valdenegro-Toro"],"pdf_url":"https://arxiv.org/pdf/2403.17224v1.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.17223v1","updated":"2024-03-25T21:53:36Z","published":"2024-03-25T21:53:36Z","title":"Co-Occurring of Object Detection and Identification towards unlabeled\n  object discovery","summary":"  In this paper, we propose a novel deep learning based approach for\nidentifying co-occurring objects in conjunction with base objects in multilabel\nobject categories. Nowadays, with the advancement in computer vision based\ntechniques we need to know about co-occurring objects with respect to base\nobject for various purposes. The pipeline of the proposed work is composed of\ntwo stages: in the first stage of the proposed model we detect all the bounding\nboxes present in the image and their corresponding labels, then in the second\nstage we perform co-occurrence matrix analysis. In co-occurrence matrix\nanalysis, we set base classes based on the maximum occurrences of the labels\nand build association rules and generate frequent patterns. These frequent\npatterns will show base classes and their corresponding co-occurring classes.\nWe performed our experiments on two publicly available datasets: Pascal VOC and\nMS-COCO. The experimental results on public benchmark dataset is reported in\nSec 4. Further we extend this work by considering all frequently objects as\nunlabeled and what if they are occluded as well.\n","authors":["Binay Kumar Singh","Niels Da Vitoria Lobo"],"pdf_url":"https://arxiv.org/pdf/2403.17223v1.pdf","comment":"6 pages, 2 figures,"},{"id":"http://arxiv.org/abs/2403.17218v1","updated":"2024-03-25T21:47:36Z","published":"2024-03-25T21:47:36Z","title":"A Comprehensive Study of the Capabilities of Large Language Models for\n  Vulnerability Detection","summary":"  Large Language Models (LLMs) have demonstrated great potential for code\ngeneration and other software engineering tasks. Vulnerability detection is of\ncrucial importance to maintaining the security, integrity, and trustworthiness\nof software systems. Precise vulnerability detection requires reasoning about\nthe code, making it a good case study for exploring the limits of LLMs'\nreasoning capabilities. Although recent work has applied LLMs to vulnerability\ndetection using generic prompting techniques, their full capabilities for this\ntask and the types of errors they make when explaining identified\nvulnerabilities remain unclear.\n  In this paper, we surveyed eleven LLMs that are state-of-the-art in code\ngeneration and commonly used as coding assistants, and evaluated their\ncapabilities for vulnerability detection. We systematically searched for the\nbest-performing prompts, incorporating techniques such as in-context learning\nand chain-of-thought, and proposed three of our own prompting methods. Our\nresults show that while our prompting methods improved the models' performance,\nLLMs generally struggled with vulnerability detection. They reported 0.5-0.63\nBalanced Accuracy and failed to distinguish between buggy and fixed versions of\nprograms in 76% of cases on average. By comprehensively analyzing and\ncategorizing 287 instances of model reasoning, we found that 57% of LLM\nresponses contained errors, and the models frequently predicted incorrect\nlocations of buggy code and misidentified bug types. LLMs only correctly\nlocalized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted\ncorrectly by 70-100% of human participants. These findings suggest that despite\ntheir potential for other tasks, LLMs may fail to properly comprehend critical\ncode structures and security-related concepts. Our data and code are available\nat https://figshare.com/s/78fe02e56e09ec49300b.\n","authors":["Benjamin Steenhoek","Md Mahbubur Rahman","Monoshi Kumar Roy","Mirza Sanjida Alam","Earl T. Barr","Wei Le"],"pdf_url":"https://arxiv.org/pdf/2403.17218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17212v1","updated":"2024-03-25T21:39:33Z","published":"2024-03-25T21:39:33Z","title":"Sanity Checks for Explanation Uncertainty","summary":"  Explanations for machine learning models can be hard to interpret or be\nwrong. Combining an explanation method with an uncertainty estimation method\nproduces explanation uncertainty. Evaluating explanation uncertainty is\ndifficult. In this paper we propose sanity checks for uncertainty explanation\nmethods, where a weight and data randomization tests are defined for\nexplanations with uncertainty, allowing for quick tests to combinations of\nuncertainty and explanation methods. We experimentally show the validity and\neffectiveness of these tests on the CIFAR10 and California Housing datasets,\nnoting that Ensembles seem to consistently pass both tests with Guided\nBackpropagation, Integrated Gradients, and LIME explanations.\n","authors":["Matias Valdenegro-Toro","Mihir Mulye"],"pdf_url":"https://arxiv.org/pdf/2403.17212v1.pdf","comment":"15 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.17210v1","updated":"2024-03-25T21:37:31Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v1.pdf","comment":"8 Pages, 4 Figures; In review in IEEE/ACM Transactions on\n  Computational Biology and Bioinformatics"},{"id":"http://arxiv.org/abs/2403.02232v2","updated":"2024-03-25T21:33:18Z","published":"2024-03-04T17:22:43Z","title":"Comprehensive evaluation of Mal-API-2019 dataset by machine learning in\n  malware detection","summary":"  This study conducts a thorough examination of malware detection using machine\nlearning techniques, focusing on the evaluation of various classification\nmodels using the Mal-API-2019 dataset. The aim is to advance cybersecurity\ncapabilities by identifying and mitigating threats more effectively. Both\nensemble and non-ensemble machine learning methods, such as Random Forest,\nXGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special\nemphasis is placed on the importance of data pre-processing techniques,\nparticularly TF-IDF representation and Principal Component Analysis, in\nimproving model performance. Results indicate that ensemble methods,\nparticularly Random Forest and XGBoost, exhibit superior accuracy, precision,\nand recall compared to others, highlighting their effectiveness in malware\ndetection. The paper also discusses limitations and potential future\ndirections, emphasizing the need for continuous adaptation to address the\nevolving nature of malware. This research contributes to ongoing discussions in\ncybersecurity and provides practical insights for developing more robust\nmalware detection systems in the digital era.\n","authors":["Zhenglin Li","Haibei Zhu","Houze Liu","Jintong Song","Qishuo Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.02232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06157v5","updated":"2024-03-25T21:23:11Z","published":"2023-06-10T23:50:02Z","title":"Fault Localization for Buggy Deep Learning Framework Conversions in\n  Image Recognition","summary":"  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 100%.\nTo mitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06157v5.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2106.13329v3","updated":"2024-03-25T21:11:44Z","published":"2021-06-24T21:40:07Z","title":"Covariance-Aware Private Mean Estimation Without Private Covariance\n  Estimation","summary":"  We present two sample-efficient differentially private mean estimators for\n$d$-dimensional (sub)Gaussian distributions with unknown covariance.\nInformally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with\nmean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that\n$\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is\nthe Mahalanobis distance. All previous estimators with the same guarantee\neither require strong a priori bounds on the covariance matrix or require\n$\\Omega(d^{3/2})$ samples.\n  Each of our estimators is based on a simple, general approach to designing\ndifferentially private mechanisms, but with novel technical steps to make the\nestimator private and sample-efficient. Our first estimator samples a point\nwith approximately maximum Tukey depth using the exponential mechanism, but\nrestricted to the set of points of large Tukey depth. Its accuracy guarantees\nhold even for data sets that have a small amount of adversarial corruption.\nProving that this mechanism is private requires a novel analysis. Our second\nestimator perturbs the empirical mean of the data set with noise calibrated to\nthe empirical covariance, without releasing the covariance itself. Its sample\ncomplexity guarantees hold more generally for subgaussian distributions, albeit\nwith a slightly worse dependence on the privacy parameter. For both estimators,\ncareful preprocessing of the data is required to satisfy differential privacy.\n","authors":["Gavin Brown","Marco Gaboardi","Adam Smith","Jonathan Ullman","Lydia Zakynthinou"],"pdf_url":"https://arxiv.org/pdf/2106.13329v3.pdf","comment":"49 pages. Appeared in NeurIPS 2021. Updated version contains improved\n  analysis of Tukey depth mechanism: robustness guarantees, tighter error\n  analysis, and techniques for faster implementation"},{"id":"http://arxiv.org/abs/2306.06208v5","updated":"2024-03-25T21:08:25Z","published":"2023-06-05T23:07:01Z","title":"DeltaNN: Assessing the Impact of Computational Environment Parameters on\n  the Performance of Image Recognition Models","summary":"  Image recognition tasks typically use deep learning and require enormous\nprocessing power, thus relying on hardware accelerators like GPUs and TPUs for\nfast, timely processing. Failure in real-time image recognition tasks can occur\ndue to sub-optimal mapping on hardware accelerators during model deployment,\nwhich may lead to timing uncertainty and erroneous behavior. Mapping on\nhardware accelerators is done using multiple software components like deep\nlearning frameworks, compilers, and device libraries, that we refer to as the\ncomputational environment. Owing to the increased use of image recognition\ntasks in safety-critical applications like autonomous driving and medical\nimaging, it is imperative to assess their robustness to changes in the\ncomputational environment, as the impact of parameters like deep learning\nframeworks, compiler optimizations, and hardware devices on model performance\nand correctness is not yet well understood.\n  In this paper we present a differential testing framework, DeltaNN, that\nallows us to assess the impact of different computational environment\nparameters on the performance of image recognition models during deployment,\npost training. DeltaNN generates different implementations of a given image\nrecognition model for variations in environment parameters, namely, deep\nlearning frameworks, compiler optimizations and hardware devices and analyzes\ndifferences in model performance as a result. Using DeltaNN, we conduct an\nempirical study of robustness analysis of three popular image recognition\nmodels using the ImageNet dataset. We report the impact in terms of\nmisclassifications and inference time differences across different settings. In\ntotal, we observed up to 100% output label differences across deep learning\nframeworks, and up to 81% unexpected performance degradation in terms of\ninference time, when applying compiler optimizations.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06208v5.pdf","comment":"11 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2211.04152v3","updated":"2024-03-25T20:50:12Z","published":"2022-11-08T10:50:29Z","title":"Federated Learning Using Three-Operator ADMM","summary":"  Federated learning (FL) has emerged as an instance of distributed machine\nlearning paradigm that avoids the transmission of data generated on the users'\nside. Although data are not transmitted, edge devices have to deal with limited\ncommunication bandwidths, data heterogeneity, and straggler effects due to the\nlimited computational resources of users' devices. A prominent approach to\novercome such difficulties is FedADMM, which is based on the classical\ntwo-operator consensus alternating direction method of multipliers (ADMM). The\ncommon assumption of FL algorithms, including FedADMM, is that they learn a\nglobal model using data only on the users' side and not on the edge server.\nHowever, in edge learning, the server is expected to be near the base station\nand have direct access to rich datasets. In this paper, we argue that\nleveraging the rich data on the edge server is much more beneficial than\nutilizing only user datasets. Specifically, we show that the mere application\nof FL with an additional virtual user node representing the data on the edge\nserver is inefficient. We propose FedTOP-ADMM, which generalizes FedADMM and is\nbased on a three-operator ADMM-type technique that exploits a smooth cost\nfunction on the edge server to learn a global model parallel to the edge\ndevices. Our numerical experiments indicate that FedTOP-ADMM has substantial\ngain up to 33\\% in communication efficiency to reach a desired test accuracy\nwith respect to FedADMM, including a virtual user on the edge server.\n","authors":["Shashi Kant","José Mairton B. da Silva Jr.","Gabor Fodor","Bo Göransson","Mats Bengtsson","Carlo Fischione"],"pdf_url":"https://arxiv.org/pdf/2211.04152v3.pdf","comment":"accepted to IEEE Journal of Selected Topics in Signal Processing,\n  2022"},{"id":"http://arxiv.org/abs/2403.17181v1","updated":"2024-03-25T20:47:10Z","published":"2024-03-25T20:47:10Z","title":"On the Intersection of Signal Processing and Machine Learning: A Use\n  Case-Driven Analysis Approach","summary":"  Recent advancements in sensing, measurement, and computing technologies have\nsignificantly expanded the potential for signal-based applications, leveraging\nthe synergy between signal processing and Machine Learning (ML) to improve both\nperformance and reliability. This fusion represents a critical point in the\nevolution of signal-based systems, highlighting the need to bridge the existing\nknowledge gap between these two interdisciplinary fields. Despite many attempts\nin the existing literature to bridge this gap, most are limited to specific\napplications and focus mainly on feature extraction, often assuming extensive\nprior knowledge in signal processing. This assumption creates a significant\nobstacle for a wide range of readers. To address these challenges, this paper\ntakes an integrated article approach. It begins with a detailed tutorial on the\nfundamentals of signal processing, providing the reader with the necessary\nbackground knowledge. Following this, it explores the key stages of a standard\nsignal processing-based ML pipeline, offering an in-depth review of feature\nextraction techniques, their inherent challenges, and solutions. Differing from\nexisting literature, this work offers an application-independent review and\nintroduces a novel classification taxonomy for feature extraction techniques.\nFurthermore, it aims at linking theoretical concepts with practical\napplications, and demonstrates this through two specific use cases: a\nspectral-based method for condition monitoring of rolling bearings and a\nwavelet energy analysis for epilepsy detection using EEG signals. In addition\nto theoretical contributions, this work promotes a collaborative research\nculture by providing a public repository of relevant Python and MATLAB signal\nprocessing codes. This effort is intended to support collaborative research\nefforts and ensure the reproducibility of the results presented.\n","authors":["Sulaiman Aburakhia","Abdallah Shami","George K. Karagiannidis"],"pdf_url":"https://arxiv.org/pdf/2403.17181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17177v1","updated":"2024-03-25T20:44:01Z","published":"2024-03-25T20:44:01Z","title":"Brain Stroke Segmentation Using Deep Learning Models: A Comparative\n  Study","summary":"  Stroke segmentation plays a crucial role in the diagnosis and treatment of\nstroke patients by providing spatial information about affected brain regions\nand the extent of damage. Segmenting stroke lesions accurately is a challenging\ntask, given that conventional manual techniques are time consuming and prone to\nerrors. Recently, advanced deep models have been introduced for general medical\nimage segmentation, demonstrating promising results that surpass many state of\nthe art networks when evaluated on specific datasets. With the advent of the\nvision Transformers, several models have been introduced based on them, while\nothers have aimed to design better modules based on traditional convolutional\nlayers to extract long-range dependencies like Transformers. The question of\nwhether such high-level designs are necessary for all segmentation cases to\nachieve the best results remains unanswered. In this study, we selected four\ntypes of deep models that were recently proposed and evaluated their\nperformance for stroke segmentation: a pure Transformer-based architecture\n(DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention\nmechanisms in their design, an advanced hybrid model that incorporates CNNs\nwith Transformers (FCT), and the well-known self-adaptive nnUNet framework with\nits configuration based on given data. We examined their performance on two\npublicly available datasets, and found that the nnUNet achieved the best\nresults with the simplest design among all. Revealing the robustness issue of\nTransformers to such variabilities serves as a potential reason for their\nweaker performance. Furthermore, nnUNet's success underscores the significant\nimpact of preprocessing and postprocessing techniques in enhancing segmentation\nresults, surpassing the focus solely on architectural designs\n","authors":["Ahmed Soliman","Yousif Yousif","Ahmed Ibrahim","Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.17177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17174v1","updated":"2024-03-25T20:43:17Z","published":"2024-03-25T20:43:17Z","title":"Belief Samples Are All You Need For Social Learning","summary":"  In this paper, we consider the problem of social learning, where a group of\nagents embedded in a social network are interested in learning an underlying\nstate of the world. Agents have incomplete, noisy, and heterogeneous sources of\ninformation, providing them with recurring private observations of the\nunderlying state of the world. Agents can share their learning experience with\ntheir peers by taking actions observable to them, with values from a finite\nfeasible set of states. Actions can be interpreted as samples from the beliefs\nwhich agents may form and update on what the true state of the world is.\nSharing samples, in place of full beliefs, is motivated by the limited\ncommunication, cognitive, and information-processing resources available to\nagents especially in large populations. Previous work (Salhab et al.) poses the\nquestion as to whether learning with probability one is still achievable if\nagents are only allowed to communicate samples from their beliefs. We provide a\ndefinite positive answer to this question, assuming a strongly connected\nnetwork and a ``collective distinguishability'' assumption, which are both\nrequired for learning even in full-belief-sharing settings. In our proposed\nbelief update mechanism, each agent's belief is a normalized weighted geometric\ninterpolation between a fully Bayesian private belief -- aggregating\ninformation from the private source -- and an ensemble of empirical\ndistributions of the samples shared by her neighbors over time. By carefully\nconstructing asymptotic almost-sure lower/upper bounds on the frequency of\nshared samples matching the true state/or not, we rigorously prove the\nconvergence of all the beliefs to the true state, with probability one.\n","authors":["Mahyar JafariNodeh","Amir Ajorlou","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2403.17174v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.17164v1","updated":"2024-03-25T20:29:04Z","published":"2024-03-25T20:29:04Z","title":"Multi-Objective Quality-Diversity for Crystal Structure Prediction","summary":"  Crystal structures are indispensable across various domains, from batteries\nto solar cells, and extensive research has been dedicated to predicting their\nproperties based on their atomic configurations. However, prevailing Crystal\nStructure Prediction methods focus on identifying the most stable solutions\nthat lie at the global minimum of the energy function. This approach overlooks\nother potentially interesting materials that lie in neighbouring local minima\nand have different material properties such as conductivity or resistance to\ndeformation. By contrast, Quality-Diversity algorithms provide a promising\navenue for Crystal Structure Prediction as they aim to find a collection of\nhigh-performing solutions that have diverse characteristics. However, it may\nalso be valuable to optimise for the stability of crystal structures alongside\nother objectives such as magnetism or thermoelectric efficiency. Therefore, in\nthis work, we harness the power of Multi-Objective Quality-Diversity algorithms\nin order to find crystal structures which have diverse features and achieve\ndifferent trade-offs of objectives. We analyse our approach on 5 crystal\nsystems and demonstrate that it is not only able to re-discover known real-life\nstructures, but also find promising new ones. Moreover, we propose a method for\nilluminating the objective space to gain an understanding of what trade-offs\ncan be achieved.\n","authors":["Hannah Janmohamed","Marta Wolinska","Shikha Surana","Thomas Pierrot","Aron Walsh","Antoine Cully"],"pdf_url":"https://arxiv.org/pdf/2403.17164v1.pdf","comment":"Accepted GECCO 2024"},{"id":"http://arxiv.org/abs/2309.10275v2","updated":"2024-03-25T20:28:22Z","published":"2023-09-19T03:02:43Z","title":"Optimizing Crowd-Aware Multi-Agent Path Finding through Local\n  Broadcasting with Graph Neural Networks","summary":"  Multi-Agent Path Finding (MAPF) in crowded environments presents a\nchallenging problem in motion planning, aiming to find collision-free paths for\nall agents in the system. MAPF finds a wide range of applications in various\ndomains, including aerial swarms, autonomous warehouse robotics, and\nself-driving vehicles. Current approaches to MAPF generally fall into two main\ncategories: centralized and decentralized planning. Centralized planning\nsuffers from the curse of dimensionality when the number of agents or states\nincreases and thus does not scale well in large and complex environments. On\nthe other hand, decentralized planning enables agents to engage in real-time\npath planning within a partially observable environment, demonstrating implicit\ncoordination. However, they suffer from slow convergence and performance\ndegradation in dense environments. In this paper, we introduce CRAMP, a novel\ncrowd-aware decentralized reinforcement learning approach to address this\nproblem by enabling efficient local communication among agents via Graph Neural\nNetworks (GNNs), facilitating situational awareness and decision-making\ncapabilities in congested environments. We test CRAMP on simulated environments\nand demonstrate that our method outperforms the state-of-the-art decentralized\nmethods for MAPF on various metrics. CRAMP improves the solution quality up to\n59% measured in makespan and collision count, and up to 35% improvement in\nsuccess rate in comparison to previous methods.\n","authors":["Phu Pham","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.10275v2.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2305.18983v3","updated":"2024-03-25T20:21:25Z","published":"2023-05-30T12:27:47Z","title":"SO(2)-Equivariant Downwash Models for Close Proximity Flight","summary":"  Multirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%).\n","authors":["H. Smith","A. Shankar","J. Gielis","J. Blumenkamp","A. Prorok"],"pdf_url":"https://arxiv.org/pdf/2305.18983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17159v1","updated":"2024-03-25T20:16:16Z","published":"2024-03-25T20:16:16Z","title":"Less Is More -- On the Importance of Sparsification for Transformers and\n  Graph Neural Networks for TSP","summary":"  Most of the recent studies tackling routing problems like the Traveling\nSalesman Problem (TSP) with machine learning use a transformer or Graph Neural\nNetwork (GNN) based encoder architecture. However, many of them apply these\nencoders naively by allowing them to aggregate information over the whole TSP\ninstances. We, on the other hand, propose a data preprocessing method that\nallows the encoders to focus on the most relevant parts of the TSP instances\nonly. In particular, we propose graph sparsification for TSP graph\nrepresentations passed to GNNs and attention masking for TSP instances passed\nto transformers where the masks correspond to the adjacency matrices of the\nsparse TSP graph representations. Furthermore, we propose ensembles of\ndifferent sparsification levels allowing models to focus on the most promising\nparts while also allowing information flow between all nodes of a TSP instance.\nIn the experimental studies, we show that for GNNs appropriate sparsification\nand ensembles of different sparsification levels lead to substantial\nperformance increases of the overall architecture. We also design a new,\nstate-of-the-art transformer encoder with ensembles of attention masking. These\ntransformers increase model performance from a gap of $0.16\\%$ to $0.10\\%$ for\nTSP instances of size 100 and from $0.02\\%$ to $0.00\\%$ for TSP instances of\nsize 50.\n","authors":["Attila Lischka","Jiaming Wu","Rafael Basso","Morteza Haghir Chehreghani","Balázs Kulcsár"],"pdf_url":"https://arxiv.org/pdf/2403.17159v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.15798v2","updated":"2024-03-25T20:09:26Z","published":"2023-09-27T17:16:32Z","title":"Node-Aligned Graph-to-Graph (NAG2G): Elevating Template-Free Deep\n  Learning Approaches in Single-Step Retrosynthesis","summary":"  Single-step retrosynthesis (SSR) in organic chemistry is increasingly\nbenefiting from deep learning (DL) techniques in computer-aided synthesis\ndesign. While template-free DL models are flexible and promising for\nretrosynthesis prediction, they often ignore vital 2D molecular information and\nstruggle with atom alignment for node generation, resulting in lower\nperformance compared to the template-based and semi-template-based methods. To\naddress these issues, we introduce Node-Aligned Graph-to-Graph (NAG2G), a\ntransformer-based template-free DL model. NAG2G combines 2D molecular graphs\nand 3D conformations to retain comprehensive molecular details and incorporates\nproduct-reactant atom mapping through node alignment which determines the order\nof the node-by-node graph outputs process in an auto-regressive manner. Through\nrigorous benchmarking and detailed case studies, we have demonstrated that\nNAG2G stands out with its remarkable predictive accuracy on the expansive\ndatasets of USPTO-50k and USPTO-FULL. Moreover, the model's practical utility\nis underscored by its successful prediction of synthesis pathways for multiple\ndrug candidate molecules. This not only proves NAG2G's robustness but also its\npotential to revolutionize the prediction of complex chemical synthesis\nprocesses for future synthetic route design tasks.\n","authors":["Lin Yao","Wentao Guo","Zhen Wang","Shang Xiang","Wentan Liu","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2309.15798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09857v2","updated":"2024-03-25T20:08:07Z","published":"2024-03-14T20:34:53Z","title":"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive\n  Prompt","summary":"  Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn\nnew classes with scarce samples while preserving knowledge of old ones.\nExisting FSCIL methods usually fine-tune the entire backbone, leading to\noverfitting and hindering the potential to learn new classes. On the other\nhand, recent prompt-based CIL approaches alleviate forgetting by training\nprompts with sufficient data in each task. In this work, we propose a novel\nframework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages\ntask-invariant prompts to capture shared knowledge by reducing specific\ninformation from the attention aspect. Additionally, self-adaptive\ntask-specific prompts in ASP provide specific information and transfer\nknowledge from old classes to new classes with an Information Bottleneck\nlearning objective. In summary, ASP prevents overfitting on base task and does\nnot require enormous data in few-shot incremental tasks. Extensive experiments\non three benchmark datasets validate that ASP consistently outperforms\nstate-of-the-art FSCIL and prompt-based CIL methods in terms of both learning\nnew classes and mitigating forgetting.\n","authors":["Chenxi Liu","Zhenyi Wang","Tianyi Xiong","Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2403.09857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01201v2","updated":"2024-03-25T19:56:06Z","published":"2023-12-02T18:42:52Z","title":"PAC Privacy Preserving Diffusion Models","summary":"  Data privacy protection is garnering increased attention among researchers.\nDiffusion models (DMs), particularly with strict differential privacy, can\npotentially produce images with both high privacy and visual quality. However,\nchallenges arise such as in ensuring robust protection in privatizing specific\ndata attributes, areas where current models often fall short. To address these\nchallenges, we introduce the PAC Privacy Preserving Diffusion Model, a model\nleverages diffusion principles and ensure Probably Approximately Correct (PAC)\nprivacy. We enhance privacy protection by integrating a private classifier\nguidance into the Langevin Sampling Process. Additionally, recognizing the gap\nin measuring the privacy of models, we have developed a novel metric to gauge\nprivacy levels. Our model, assessed with this new metric and supported by\nGaussian matrix computations for the PAC bound, has shown superior performance\nin privacy protection over existing leading private generative models according\nto benchmark tests.\n","authors":["Qipan Xu","Youlong Ding","Xinxi Zhang","Jie Gao","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17142v1","updated":"2024-03-25T19:39:17Z","published":"2024-03-25T19:39:17Z","title":"Approximation with Random Shallow ReLU Networks with Applications to\n  Model Reference Adaptive Control","summary":"  Neural networks are regularly employed in adaptive control of nonlinear\nsystems and related methods o reinforcement learning. A common architecture\nuses a neural network with a single hidden layer (i.e. a shallow network), in\nwhich the weights and biases are fixed in advance and only the output layer is\ntrained. While classical results show that there exist neural networks of this\ntype that can approximate arbitrary continuous functions over bounded regions,\nthey are non-constructive, and the networks used in practice have no\napproximation guarantees. Thus, the approximation properties required for\ncontrol with neural networks are assumed, rather than proved. In this paper, we\naim to fill this gap by showing that for sufficiently smooth functions, ReLU\nnetworks with randomly generated weights and biases achieve $L_{\\infty}$ error\nof $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It\nsuffices to generate the weights uniformly over a sphere and the biases\nuniformly over an interval. We show how the result can be used to get\napproximations of required accuracy in a model reference adaptive control\napplication.\n","authors":["Andrew Lamperski","Tyler Lekang"],"pdf_url":"https://arxiv.org/pdf/2403.17142v1.pdf","comment":"Under Review for Conference on Decision and Control"},{"id":"http://arxiv.org/abs/2403.17135v1","updated":"2024-03-25T19:17:59Z","published":"2024-03-25T19:17:59Z","title":"Exploring the Generalization of Cancer Clinical Trial Eligibility\n  Classifiers Across Diseases","summary":"  Clinical trials are pivotal in medical research, and NLP can enhance their\nsuccess, with application in recruitment. This study aims to evaluate the\ngeneralizability of eligibility classification across a broad spectrum of\nclinical trials. Starting with phase 3 cancer trials, annotated with seven\neligibility exclusions, then to determine how well models can generalize to\nnon-cancer and non-phase 3 trials. To assess this, we have compiled eligibility\ncriteria data for five types of trials: (1) additional phase 3 cancer trials,\n(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes\ntrials, and (5) observational trials for any disease, comprising 2,490\nannotated eligibility criteria across seven exclusion types. Our results show\nthat models trained on the extensive cancer dataset can effectively handle\ncriteria commonly found in non-cancer trials, such as autoimmune diseases.\nHowever, they struggle with criteria disproportionately prevalent in cancer\ntrials, like prior malignancy. We also experiment with few-shot learning,\ndemonstrating that a limited number of disease-specific examples can partially\novercome this performance gap. We are releasing this new dataset of annotated\neligibility statements to promote the development of cross-disease\ngeneralization in clinical trial classification.\n","authors":["Yumeng Yang","Ashley Gilliam","Ethan B Ludmir","Kirk Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17130v1","updated":"2024-03-25T19:15:19Z","published":"2024-03-25T19:15:19Z","title":"Exploring the potential of prototype-based soft-labels data distillation\n  for imbalanced data classification","summary":"  Dataset distillation aims at synthesizing a dataset by a small number of\nartificially generated data items, which, when used as training data, reproduce\nor approximate a machine learning (ML) model as if it were trained on the\nentire original dataset. Consequently, data distillation methods are usually\ntied to a specific ML algorithm. While recent literature deals mainly with\ndistillation of large collections of images in the context of neural network\nmodels, tabular data distillation is much less represented and mainly focused\non a theoretical perspective. The current paper explores the potential of a\nsimple distillation technique previously proposed in the context of\nLess-than-one shot learning. The main goal is to push further the performance\nof prototype-based soft-labels distillation in terms of classification\naccuracy, by integrating optimization steps in the distillation process. The\nanalysis is performed on real-world data sets with various degrees of\nimbalance. Experimental studies trace the capability of the method to distill\nthe data, but also the opportunity to act as an augmentation method, i.e. to\ngenerate new data that is able to increase model accuracy when used in\nconjunction with - as opposed to instead of - the original data.\n","authors":["Radu-Andrei Rosu","Mihaela-Elena Breaban","Henri Luchian"],"pdf_url":"https://arxiv.org/pdf/2403.17130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09362v3","updated":"2024-03-25T19:08:53Z","published":"2023-10-13T19:09:31Z","title":"From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment\n  Technique","summary":"  In the wake of the post-pandemic era, marked by social isolation and surging\nrates of depression and anxiety, conversational agents based on digital\npsychotherapy can play an influential role compared to traditional therapy\nsessions. In this work, we develop a voice-capable chatbot in Farsi to guide\nusers through Self-Attachment (SAT), a novel, self-administered, holistic\npsychological technique based on attachment theory. Our chatbot uses a dynamic\narray of rule-based and classification-based modules to comprehend user input\nthroughout the conversation and navigates a dialogue flowchart accordingly,\nrecommending appropriate SAT exercises that depend on the user's emotional and\nmental state. In particular, we collect a dataset of over 6,000 utterances and\ndevelop a novel sentiment-analysis module that classifies user sentiment into\n12 classes, with accuracy above 92%. To keep the conversation novel and\nengaging, the chatbot's responses are retrieved from a large dataset of\nutterances created with the aid of Farsi GPT-2 and a reinforcement learning\napproach, thus requiring minimal human annotation. Our chatbot also offers a\nquestion-answering module, called SAT Teacher, to answer users' questions about\nthe principles of Self-Attachment. Finally, we design a cross-platform\napplication as the bot's user interface. We evaluate our platform in a ten-day\nhuman study with N=52 volunteers from the non-clinical population, who have had\nover 2,000 dialogues in total with the chatbot. The results indicate that the\nplatform was engaging to most users (75%), 72% felt better after the\ninteractions, and 74% were satisfied with the SAT Teacher's performance.\n","authors":["Sina Elahimanesh","Shayan Salehi","Sara Zahedi Movahed","Lisa Alazraki","Ruoyu Hu","Abbas Edalat"],"pdf_url":"https://arxiv.org/pdf/2310.09362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17124v1","updated":"2024-03-25T19:04:59Z","published":"2024-03-25T19:04:59Z","title":"Grounding Language Plans in Demonstrations Through Counterfactual\n  Perturbations","summary":"  Grounding the common-sense reasoning of Large Language Models in physical\ndomains remains a pivotal yet unsolved problem for embodied AI. Whereas prior\nworks have focused on leveraging LLMs directly for planning in symbolic spaces,\nthis work uses LLMs to guide the search of task structures and constraints\nimplicit in multi-step demonstrations. Specifically, we borrow from\nmanipulation planning literature the concept of mode families, which group\nrobot configurations by specific motion constraints, to serve as an abstraction\nlayer between the high-level language representations of an LLM and the\nlow-level physical trajectories of a robot. By replaying a few human\ndemonstrations with synthetic perturbations, we generate coverage over the\ndemonstrations' state space with additional successful executions as well as\ncounterfactuals that fail the task. Our explanation-based learning framework\ntrains an end-to-end differentiable neural network to predict successful\ntrajectories from failures and as a by-product learns classifiers that ground\nlow-level states and images in mode families without dense labeling. The\nlearned grounding classifiers can further be used to translate language plans\ninto reactive policies in the physical domain in an interpretable manner. We\nshow our approach improves the interpretability and reactivity of imitation\nlearning through 2D navigation and simulated and real robot manipulation tasks.\nWebsite: https://sites.google.com/view/grounding-plans\n","authors":["Yanwei Wang","Tsun-Hsuan Wang","Jiayuan Mao","Michael Hagenow","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2403.17124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16731v2","updated":"2024-03-25T18:51:02Z","published":"2024-02-26T16:52:35Z","title":"Accelerating Graph Neural Networks on Real Processing-In-Memory Systems","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML framework that accelerates GNNs\non real PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively, to match\ntheir algorithmic nature. We extensively evaluate PyGim on a real-world PIM\nsystem with 1992 PIM cores using emerging GNN models, and demonstrate that it\noutperforms its state-of-the-art CPU counterpart on Intel Xeon by on average\n3.04x, and achieves higher resource utilization than CPU and GPU systems. Our\nwork provides useful recommendations for software, system and hardware\ndesigners. PyGim will be open-sourced to enable the widespread use of PIM\nsystems in GNNs.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Yu Xin Li","Juan Gomez Luna","Mohammad Sadrosadati","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12151v2","updated":"2024-03-25T18:50:06Z","published":"2024-03-18T18:08:44Z","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification","summary":"  Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.\n","authors":["Filippos Gouidis","Katerina Papantoniou","Konstantinos Papoutsakis Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"pdf_url":"https://arxiv.org/pdf/2403.12151v2.pdf","comment":"Accepted at the AAAI-MAKE 24"},{"id":"http://arxiv.org/abs/2403.17105v1","updated":"2024-03-25T18:43:58Z","published":"2024-03-25T18:43:58Z","title":"Stochastic Gradient Langevin Unlearning","summary":"  ``The right to be forgotten'' ensured by laws for user data privacy becomes\nincreasingly important. Machine unlearning aims to efficiently remove the\neffect of certain data points on the trained model parameters so that it can be\napproximately the same as if one retrains the model from scratch. This work\nproposes stochastic gradient Langevin unlearning, the first unlearning\nframework based on noisy stochastic gradient descent (SGD) with privacy\nguarantees for approximate unlearning problems under convexity assumption. Our\nresults show that mini-batch gradient updates provide a superior\nprivacy-complexity trade-off compared to the full-batch counterpart. There are\nnumerous algorithmic benefits of our unlearning approach, including complexity\nsaving compared to retraining, and supporting sequential and batch unlearning.\nTo examine the privacy-utility-complexity trade-off of our method, we conduct\nexperiments on benchmark datasets compared against prior works. Our approach\nachieves a similar utility under the same privacy constraint while using $2\\%$\nand $10\\%$ of the gradient computations compared with the state-of-the-art\ngradient-based approximate unlearning methods for mini-batch and full-batch\nsettings, respectively.\n","authors":["Eli Chien","Haoyu Wang","Ziang Chen","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2403.17105v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.10371"},{"id":"http://arxiv.org/abs/2403.17094v1","updated":"2024-03-25T18:32:41Z","published":"2024-03-25T18:32:41Z","title":"SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end\n  Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving","summary":"  To advance research in learning-based defogging algorithms, various synthetic\nfog datasets have been developed. However, existing datasets created using the\nAtmospheric Scattering Model (ASM) or real-time rendering engines often\nstruggle to produce photo-realistic foggy images that accurately mimic the\nactual imaging process. This limitation hinders the effective generalization of\nmodels from synthetic to real data. In this paper, we introduce an end-to-end\nsimulation pipeline designed to generate photo-realistic foggy images. This\npipeline comprehensively considers the entire physically-based foggy scene\nimaging process, closely aligning with real-world image capture methods. Based\non this pipeline, we present a new synthetic fog dataset named SynFog, which\nfeatures both sky light and active lighting conditions, as well as three levels\nof fog density. Experimental results demonstrate that models trained on SynFog\nexhibit superior performance in visual perception and detection accuracy\ncompared to others when applied to real-world foggy images.\n","authors":["Yiming Xie","Henglu Wei","Zhenyi Liu","Xiaoyu Wang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.17094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17093v1","updated":"2024-03-25T18:32:22Z","published":"2024-03-25T18:32:22Z","title":"Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep\n  Learning and Explainable AI Analysis","summary":"  In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs),\nthe utmost importance lies in guaranteeing resilient and lucid security\nmeasures. This study highlights the necessity of implementing a Zero Trust\nArchitecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs),\nhence departing from conventional perimeter defences that may expose\nvulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous\nand continuous process of authenticating all network entities and\ncommunications. The accuracy of our methodology in detecting and identifying\nunmanned aerial vehicles (UAVs) is 84.59\\%. This is achieved by utilizing Radio\nFrequency (RF) signals within a Deep Learning framework, a unique method.\nPrecise identification is crucial in Zero Trust Architecture (ZTA), as it\ndetermines network access. In addition, the use of eXplainable Artificial\nIntelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local\nInterpretable Model-agnostic Explanations (LIME) contributes to the improvement\nof the model's transparency and interpretability. Adherence to Zero Trust\nArchitecture (ZTA) standards guarantees that the classifications of unmanned\naerial vehicles (UAVs) are verifiable and comprehensible, enhancing security\nwithin the UAV field.\n","authors":["Ekramul Haque","Kamrul Hasan","Imtiaz Ahmed","Md. Sahabul Alam","Tariqul Islam"],"pdf_url":"https://arxiv.org/pdf/2403.17093v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17091v1","updated":"2024-03-25T18:28:45Z","published":"2024-03-25T18:28:45Z","title":"Offline Reinforcement Learning: Role of State Aggregation and Trajectory\n  Data","summary":"  We revisit the problem of offline reinforcement learning with value function\nrealizability but without Bellman completeness. Previous work by Xie and Jiang\n(2021) and Foster et al. (2022) left open the question whether a bounded\nconcentrability coefficient along with trajectory-based offline data admits a\npolynomial sample complexity. In this work, we provide a negative answer to\nthis question for the task of offline policy evaluation. In addition to\naddressing this question, we provide a rather complete picture for offline\npolicy evaluation with only value function realizability. Our primary findings\nare threefold: 1) The sample complexity of offline policy evaluation is\ngoverned by the concentrability coefficient in an aggregated Markov Transition\nModel jointly determined by the function class and the offline data\ndistribution, rather than that in the original MDP. This unifies and\ngeneralizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The\nconcentrability coefficient in the aggregated Markov Transition Model may grow\nexponentially with the horizon length, even when the concentrability\ncoefficient in the original MDP is small and the offline data is admissible\n(i.e., the data distribution equals the occupancy measure of some policy), 3)\nUnder value function realizability, there is a generic reduction that can\nconvert any hard instance with admissible data to a hard instance with\ntrajectory data, implying that trajectory data offers no extra benefits over\nadmissible data. These three pieces jointly resolve the open problem, though\neach of them could be of independent interest.\n","authors":["Zeyu Jia","Alexander Rakhlin","Ayush Sekhari","Chen-Yu Wei"],"pdf_url":"https://arxiv.org/pdf/2403.17091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16338v2","updated":"2024-03-25T18:18:40Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17083v1","updated":"2024-03-25T18:16:34Z","published":"2024-03-25T18:16:34Z","title":"A Study in Dataset Pruning for Image Super-Resolution","summary":"  In image Super-Resolution (SR), relying on large datasets for training is a\ndouble-edged sword. While offering rich training material, they also demand\nsubstantial computational and storage resources. In this work, we analyze\ndataset pruning as a solution to these challenges. We introduce a novel\napproach that reduces a dataset to a core-set of training samples, selected\nbased on their loss values as determined by a simple pre-trained SR model. By\nfocusing the training on just 50% of the original dataset, specifically on the\nsamples characterized by the highest loss values, we achieve results comparable\nto or even surpassing those obtained from training on the entire dataset.\nInterestingly, our analysis reveals that the top 5% of samples with the highest\nloss values negatively affect the training process. Excluding these samples and\nadjusting the selection to favor easier samples further enhances training\noutcomes. Our work opens new perspectives to the untapped potential of dataset\npruning in image SR. It suggests that careful selection of training data based\non loss-value metrics can lead to better SR models, challenging the\nconventional wisdom that more data inevitably leads to better performance.\n","authors":["Brian B. Moser","Federico Raue","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.17083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17081v1","updated":"2024-03-25T18:12:16Z","published":"2024-03-25T18:12:16Z","title":"Machine Learning on Blockchain Data: A Systematic Mapping Study","summary":"  Context: Blockchain technology has drawn growing attention in the literature\nand in practice. Blockchain technology generates considerable amounts of data\nand has thus been a topic of interest for Machine Learning (ML).\n  Objective: The objective of this paper is to provide a comprehensive review\nof the state of the art on machine learning applied to blockchain data. This\nwork aims to systematically identify, analyze, and classify the literature on\nML applied to blockchain data. This will allow us to discover the fields where\nmore effort should be placed in future research.\n  Method: A systematic mapping study has been conducted to identify the\nrelevant literature. Ultimately, 159 articles were selected and classified\naccording to various dimensions, specifically, the domain use case, the\nblockchain, the data, and the machine learning models.\n  Results: The majority of the papers (49.7%) fall within the Anomaly use case.\nBitcoin (47.2%) was the blockchain that drew the most attention. A dataset\nconsisting of more than 1.000.000 data points was used by 31.4% of the papers.\nAnd Classification (46.5%) was the ML task most applied to blockchain data.\n  Conclusion: The results confirm that ML applied to blockchain data is a\nrelevant and a growing topic of interest both in the literature and in\npractice. Nevertheless, some open challenges and gaps remain, which can lead to\nfuture research directions. Specifically, we identify novel machine learning\nalgorithms, the lack of a standardization framework, blockchain scalability\nissues and cross-chain interactions as areas worth exploring in the future.\n","authors":["Georgios Palaiokrassas","Sarah Bouraga","Leandros Tassiulas"],"pdf_url":"https://arxiv.org/pdf/2403.17081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05401v5","updated":"2024-03-25T18:07:22Z","published":"2023-10-09T04:40:20Z","title":"Entropy-MCMC: Sampling from Flat Basins with Ease","summary":"  Bayesian deep learning counts on the quality of posterior distribution\nestimation. However, the posterior of deep neural networks is highly\nmulti-modal in nature, with local modes exhibiting varying generalization\nperformance. Given a practical budget, targeting at the original posterior can\nlead to suboptimal performance, as some samples may become trapped in \"bad\"\nmodes and suffer from overfitting. Leveraging the observation that \"good\" modes\nwith low generalization error often reside in flat basins of the energy\nlandscape, we propose to bias sampling on the posterior toward these flat\nregions. Specifically, we introduce an auxiliary guiding variable, the\nstationary distribution of which resembles a smoothed posterior free from sharp\nmodes, to lead the MCMC sampler to flat basins. By integrating this guiding\nvariable with the model parameter, we create a simple joint distribution that\nenables efficient sampling with minimal computational overhead. We prove the\nconvergence of our method and further show that it converges faster than\nseveral existing flatness-aware methods in the strongly convex setting.\nEmpirical results demonstrate that our method can successfully sample from flat\nbasins of the posterior, and outperforms all compared baselines on multiple\nbenchmarks including classification, calibration, and out-of-distribution\ndetection.\n","authors":["Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05401v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17064v1","updated":"2024-03-25T18:00:42Z","published":"2024-03-25T18:00:42Z","title":"Continuous, Subject-Specific Attribute Control in T2I Models by\n  Identifying Semantic Directions","summary":"  In recent years, advances in text-to-image (T2I) diffusion models have\nsubstantially elevated the quality of their generated images. However,\nachieving fine-grained control over attributes remains a challenge due to the\nlimitations of natural language prompts (such as no continuous set of\nintermediate descriptions existing between ``person'' and ``old person''). Even\nthough many methods were introduced that augment the model or generation\nprocess to enable such control, methods that do not require a fixed reference\nimage are limited to either enabling global fine-grained attribute expression\ncontrol or coarse attribute expression control localized to specific subjects,\nnot both simultaneously. We show that there exist directions in the commonly\nused token-level CLIP text embeddings that enable fine-grained subject-specific\ncontrol of high-level attributes in text-to-image models. Based on this\nobservation, we introduce one efficient optimization-free and one robust\noptimization-based method to identify these directions for specific attributes\nfrom contrastive text prompts. We demonstrate that these directions can be used\nto augment the prompt text input with fine-grained control over attributes of\nspecific subjects in a compositional manner (control over multiple attributes\nof a single subject) without having to adapt the diffusion model. Project page:\nhttps://compvis.github.io/attribute-control. Code is available at\nhttps://github.com/CompVis/attribute-control.\n","authors":["Stefan Andreas Baumann","Felix Krause","Michael Neumayr","Nick Stracke","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.17064v1.pdf","comment":"Project page: https://compvis.github.io/attribute-control"},{"id":"http://arxiv.org/abs/2403.17010v1","updated":"2024-03-25T17:59:59Z","published":"2024-03-25T17:59:59Z","title":"Calib3D: Calibrating Model Preferences for Reliable 3D Scene\n  Understanding","summary":"  Safety-critical 3D scene understanding tasks necessitate not only accurate\nbut also confident predictions from 3D perception models. This study introduces\nCalib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D\nscene understanding models from an uncertainty estimation viewpoint. We\ncomprehensively evaluate 28 state-of-the-art models across 10 diverse 3D\ndatasets, uncovering insightful phenomena that cope with both the aleatoric and\nepistemic uncertainties in 3D scene understanding. We discover that despite\nachieving impressive levels of accuracy, existing models frequently fail to\nprovide reliable uncertainty estimates -- a pitfall that critically undermines\ntheir applicability in safety-sensitive contexts. Through extensive analysis of\nkey factors such as network capacity, LiDAR representations, rasterization\nresolutions, and 3D data augmentation techniques, we correlate these aspects\ndirectly with the model calibration efficacy. Furthermore, we introduce DeptS,\na novel depth-aware scaling approach aimed at enhancing 3D model calibration.\nExtensive experiments across a wide range of configurations validate the\nsuperiority of our method. We hope this work could serve as a cornerstone for\nfostering reliable 3D scene understanding. Code and benchmark toolkits are\npublicly available.\n","authors":["Lingdong Kong","Xiang Xu","Jun Cen","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17010v1.pdf","comment":"Preprint; 37 pages, 8 figures, 11 tables; Code at\n  https://github.com/ldkong1205/Calib3D"},{"id":"http://arxiv.org/abs/2310.05884v2","updated":"2024-03-25T17:58:36Z","published":"2023-10-09T17:27:36Z","title":"A Meta-Learning Perspective on Transformers for Causal Language Modeling","summary":"  The Transformer architecture has become prominent in developing large causal\nlanguage models. However, mechanisms to explain its capabilities are not well\nunderstood. Focused on the training process, here we establish a meta-learning\nview of the Transformer architecture when trained for the causal language\nmodeling task, by explicating an inner optimization process within the\nTransformer. Further, within the inner optimization, we discover and\ntheoretically analyze a special characteristic of the norms of learned token\nrepresentations within Transformer-based causal language models. Our analysis\nis supported by experiments in various settings.\n","authors":["Xinbo Wu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.05884v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16995v1","updated":"2024-03-25T17:58:22Z","published":"2024-03-25T17:58:22Z","title":"Language Rectified Flow: Advancing Diffusion Language Generation with\n  Probabilistic Flows","summary":"  Recent works have demonstrated success in controlling sentence attributes\n($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the\ndiffusion language model. A key component that drives theimpressive performance\nfor generating high-quality samples from noise is iteratively denoise for\nthousands of steps. While beneficial, the complexity of starting from the noise\nand the learning steps has limited its implementation to many NLP real-world\napplications. This paper proposes Language Rectified Flow ({\\ours}). Our method\nis based on the reformulation of the standard probabilistic flow models.\nLanguage rectified flow learns (neural) ordinary differential equation models\nto transport between the source distribution and the target distribution, hence\nproviding a unified and effective solution to generative modeling and domain\ntransfer. From the source distribution, our language rectified flow yields fast\nsimulation and effectively decreases the inference time. Experiments on three\nchallenging fine-grained control tasks and multiple high-quality text editing\nshow that our method consistently outperforms its baselines. Extensive\nexperiments and ablation studies demonstrate that our method can be general,\neffective, and beneficial for many NLP tasks.\n","authors":["Shujian Zhang","Lemeng Wu","Chengyue Gong","Xingchao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16995v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09863v2","updated":"2024-03-25T17:55:25Z","published":"2024-03-14T20:50:03Z","title":"Towards White Box Deep Learning","summary":"  This paper introduces semantic features as a candidate conceptual framework\nfor building inherently interpretable neural networks. A proof of concept model\nfor informative subproblem of MNIST consists of 4 such layers with the total of\n5K learnable parameters. The model is well-motivated, inherently interpretable,\nrequires little hyperparameter tuning and achieves human-level adversarial test\naccuracy - with no form of adversarial training! These results and the general\nnature of the approach warrant further research on semantic features. The code\nis available at https://github.com/314-Foundation/white-box-nn\n","authors":["Maciej Satkiewicz"],"pdf_url":"https://arxiv.org/pdf/2403.09863v2.pdf","comment":"13 pages, 9 figures, independent research, v2 changes: more adequate\n  title; added: related research in Introduction, Ablation Study, Discussion,\n  examples in Further Research, Appendix C; minor wording changes (including\n  abstract)"},{"id":"http://arxiv.org/abs/2403.16990v1","updated":"2024-03-25T17:52:07Z","published":"2024-03-25T17:52:07Z","title":"Be Yourself: Bounded Attention for Multi-Subject Text-to-Image\n  Generation","summary":"  Text-to-image diffusion models have an unprecedented ability to generate\ndiverse and high-quality images. However, they often struggle to faithfully\ncapture the intended semantics of complex input prompts that include multiple\nsubjects. Recently, numerous layout-to-image extensions have been introduced to\nimprove user control, aiming to localize subjects represented by specific\ntokens. Yet, these methods often produce semantically inaccurate images,\nespecially when dealing with multiple semantically or visually similar\nsubjects. In this work, we study and analyze the causes of these limitations.\nOur exploration reveals that the primary issue stems from inadvertent semantic\nleakage between subjects in the denoising process. This leakage is attributed\nto the diffusion model's attention layers, which tend to blend the visual\nfeatures of different subjects. To address these issues, we introduce Bounded\nAttention, a training-free method for bounding the information flow in the\nsampling process. Bounded Attention prevents detrimental leakage among subjects\nand enables guiding the generation to promote each subject's individuality,\neven with complex multi-subject conditioning. Through extensive\nexperimentation, we demonstrate that our method empowers the generation of\nmultiple subjects that better align with given prompts and layouts.\n","authors":["Omer Dahary","Or Patashnik","Kfir Aberman","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.16990v1.pdf","comment":"Project page: https://omer11a.github.io/bounded-attention/"},{"id":"http://arxiv.org/abs/2403.14092v2","updated":"2024-03-25T17:49:07Z","published":"2024-03-21T02:59:56Z","title":"Carbon Footprint Reduction for Sustainable Data Centers in Real-Time","summary":"  As machine learning workloads significantly increase energy consumption,\nsustainable data centers with low carbon emissions are becoming a top priority\nfor governments and corporations worldwide. This requires a paradigm shift in\noptimizing power consumption in cooling and IT loads, shifting flexible loads\nbased on the availability of renewable energy in the power grid, and leveraging\nbattery storage from the uninterrupted power supply in data centers, using\ncollaborative agents. The complex association between these optimization\nstrategies and their dependencies on variable external factors like weather and\nthe power grid carbon intensity makes this a hard problem. Currently, a\nreal-time controller to optimize all these goals simultaneously in a dynamic\nreal-world setting is lacking. We propose a Data Center Carbon Footprint\nReduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that\noptimizes data centers for the multiple objectives of carbon footprint\nreduction, energy consumption, and energy cost. The results show that the\nDC-CFR MARL agents effectively resolved the complex interdependencies in\noptimizing cooling, load shifting, and energy storage in real-time for various\nlocations under real-world dynamic weather and grid carbon intensity\nconditions. DC-CFR significantly outperformed the industry standard ASHRAE\ncontroller with a considerable reduction in carbon emissions (14.5%), energy\nusage (14.4%), and energy cost (13.7%) when evaluated over one year across\nmultiple geographical regions.\n","authors":["Soumyendu Sarkar","Avisek Naug","Ricardo Luna","Antonio Guillen","Vineet Gundecha","Sahand Ghorbanpour","Sajad Mousavi","Dejan Markovikj","Ashwin Ramesh Babu"],"pdf_url":"https://arxiv.org/pdf/2403.14092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16986v1","updated":"2024-03-25T17:48:06Z","published":"2024-03-25T17:48:06Z","title":"Dynamic Relative Representations for Goal-Oriented Semantic\n  Communications","summary":"  In future 6G wireless networks, semantic and effectiveness aspects of\ncommunications will play a fundamental role, incorporating meaning and\nrelevance into transmissions. However, obstacles arise when devices employ\ndiverse languages, logic, or internal representations, leading to semantic\nmismatches that might jeopardize understanding. In latent space communication,\nthis challenge manifests as misalignment within high-dimensional\nrepresentations where deep neural networks encode data. This paper presents a\nnovel framework for goal-oriented semantic communication, leveraging relative\nrepresentations to mitigate semantic mismatches via latent space alignment. We\npropose a dynamic optimization strategy that adapts relative representations,\ncommunication parameters, and computation resources for energy-efficient,\nlow-latency, goal-oriented semantic communications. Numerical results\ndemonstrate our methodology's effectiveness in mitigating mismatches among\ndevices, while optimizing energy consumption, delay, and effectiveness.\n","authors":["Simone Fiorellino","Claudio Battiloro","Emilio Calvanese Strinati","Paolo Di Lorenzo"],"pdf_url":"https://arxiv.org/pdf/2403.16986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16974v1","updated":"2024-03-25T17:40:32Z","published":"2024-03-25T17:40:32Z","title":"Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution\n  Microscopy","summary":"  The use of fluorescent molecules to create long sequences of low-density,\ndiffraction-limited images enables highly-precise molecule localization.\nHowever, this methodology requires lengthy imaging times, which limits the\nability to view dynamic interactions of live cells on short time scales. Many\ntechniques have been developed to reduce the number of frames needed for\nlocalization, from classic iterative optimization to deep neural networks.\nParticularly, deep algorithm unrolling utilizes both the structure of iterative\nsparse recovery algorithms and the performance gains of supervised deep\nlearning. However, the robustness of this approach is highly dependant on\nhaving sufficient training data. In this paper we introduce deep unrolled\nself-supervised learning, which alleviates the need for such data by training a\nsequence-specific, model-based autoencoder that learns only from given\nmeasurements. Our proposed method exceeds the performance of its supervised\ncounterparts, thus allowing for robust, dynamic imaging well below the\ndiffraction limit without any labeled training samples. Furthermore, the\nsuggested model-based autoencoder scheme can be utilized to enhance\ngeneralization in any sparse recovery framework, without the need for external\ntraining data.\n","authors":["Yair Ben Sahel","Yonina C. Eldar"],"pdf_url":"https://arxiv.org/pdf/2403.16974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01069v3","updated":"2024-03-25T17:39:18Z","published":"2023-09-03T03:54:43Z","title":"Separable Hamiltonian Neural Networks","summary":"  Hamiltonian neural networks (HNNs) are state-of-the-art models that regress\nthe vector field of a dynamical system under the learning bias of Hamilton's\nequations. A recent observation is that embedding a bias regarding the additive\nseparability of the Hamiltonian reduces the regression complexity and improves\nregression performance. We propose separable HNNs that embed additive\nseparability within HNNs using observational, learning, and inductive biases.\nWe show that the proposed models are more effective than the HNN at regressing\nthe Hamiltonian and the vector field, and have the capability to interpret the\nkinetic and potential energy of the system.\n","authors":["Zi-Yu Khoo","Dawen Wu","Jonathan Sze Choong Low","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2309.01069v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16973v1","updated":"2024-03-25T17:38:32Z","published":"2024-03-25T17:38:32Z","title":"VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild","summary":"  We introduce VoiceCraft, a token infilling neural codec language model, that\nachieves state-of-the-art performance on both speech editing and zero-shot\ntext-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft\nemploys a Transformer decoder architecture and introduces a token rearrangement\nprocedure that combines causal masking and delayed stacking to enable\ngeneration within an existing sequence. On speech editing tasks, VoiceCraft\nproduces edited speech that is nearly indistinguishable from unedited\nrecordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,\nour model outperforms prior SotA models including VALLE and the popular\ncommercial model XTTS-v2. Crucially, the models are evaluated on challenging\nand realistic datasets, that consist of diverse accents, speaking styles,\nrecording conditions, and background noise and music, and our model performs\nconsistently well compared to other models and real recordings. In particular,\nfor speech editing evaluation, we introduce a high quality, challenging, and\nrealistic dataset named RealEdit. We encourage readers to listen to the demos\nat https://jasonppy.github.io/VoiceCraft_web.\n","authors":["Puyuan Peng","Po-Yao Huang","Daniel Li","Abdelrahman Mohamed","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2403.16973v1.pdf","comment":"Data, code, and model weights are available at\n  https://github.com/jasonppy/VoiceCraft"},{"id":"http://arxiv.org/abs/2403.16970v1","updated":"2024-03-25T17:31:12Z","published":"2024-03-25T17:31:12Z","title":"Joint chest X-ray diagnosis and clinical visual attention prediction\n  with multi-stage cooperative learning: enhancing interpretability","summary":"  As deep learning has become the state-of-the-art for computer-assisted\ndiagnosis, interpretability of the automatic decisions is crucial for clinical\ndeployment. While various methods were proposed in this domain, visual\nattention maps of clinicians during radiological screening offer a unique asset\nto provide important insights and can potentially enhance the quality of\ncomputer-assisted diagnosis. With this paper, we introduce a novel\ndeep-learning framework for joint disease diagnosis and prediction of\ncorresponding visual saliency maps for chest X-ray scans. Specifically, we\ndesigned a novel dual-encoder multi-task UNet, which leverages both a\nDenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based\nencoder to extract diverse features for saliency map prediction, and a\nmulti-scale feature-fusion classifier to perform disease classification. To\ntackle the issue of asynchronous training schedules of individual tasks in\nmulti-task learning, we proposed a multi-stage cooperative learning strategy,\nwith contrastive learning for feature encoder pretraining to boost performance.\nExperiments show that our proposed method outperformed existing techniques for\nchest X-ray diagnosis and the quality of visual saliency map prediction.\n","authors":["Zirui Qiu","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16952v1","updated":"2024-03-25T17:14:00Z","published":"2024-03-25T17:14:00Z","title":"Data Mixing Laws: Optimizing Data Mixtures by Predicting Language\n  Modeling Performance","summary":"  Pretraining data of large language models composes multiple domains (e.g.,\nweb texts, academic papers, codes), whose mixture proportions crucially impact\nthe competence of outcome models. While existing endeavors rely on heuristics\nor qualitative strategies to tune the proportions, we discover the quantitative\npredictability of model performance regarding the mixture proportions in\nfunction forms, which we refer to as the data mixing laws. Fitting such\nfunctions on sample mixtures unveils model performance on unseen mixtures\nbefore actual runs, thus guiding the selection of an ideal data mixture.\nFurthermore, we propose nested use of the scaling laws of training steps, model\nsizes, and our data mixing law to enable predicting the performance of large\nmodels trained on massive data under various mixtures with only small-scale\ntraining. Moreover, experimental results verify that our method effectively\noptimizes the training mixture of a 1B model trained for 100B tokens in\nRedPajama, reaching a performance comparable to the one trained for 48% more\nsteps on the default mixture. Extending the application of data mixing laws to\ncontinual training accurately predicts the critical mixture proportion that\navoids catastrophic forgetting and outlooks the potential for dynamic data\nschedules\n","authors":["Jiasheng Ye","Peiju Liu","Tianxiang Sun","Yunhua Zhou","Jun Zhan","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.16952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15001v2","updated":"2024-03-25T17:01:08Z","published":"2023-12-22T16:33:50Z","title":"Discovering modular solutions that generalize compositionally","summary":"  Many complex tasks can be decomposed into simpler, independent parts.\nDiscovering such underlying compositional structure has the potential to enable\ncompositional generalization. Despite progress, our most powerful systems\nstruggle to compose flexibly. It therefore seems natural to make models more\nmodular to help capture the compositional nature of many tasks. However, it is\nunclear under which circumstances modular systems can discover hidden\ncompositional structure. To shed light on this question, we study a\nteacher-student setting with a modular teacher where we have full control over\nthe composition of ground truth modules. This allows us to relate the problem\nof compositional generalization to that of identification of the underlying\nmodules. In particular we study modularity in hypernetworks representing a\ngeneral class of multiplicative interactions. We show theoretically that\nidentification up to linear transformation purely from demonstrations is\npossible without having to learn an exponential number of module combinations.\nWe further demonstrate empirically that under the theoretically identified\nconditions, meta-learning from finite data can discover modular policies that\ngeneralize compositionally in a number of complex environments.\n","authors":["Simon Schug","Seijin Kobayashi","Yassir Akram","Maciej Wołczyk","Alexandra Proca","Johannes von Oswald","Razvan Pascanu","João Sacramento","Angelika Steger"],"pdf_url":"https://arxiv.org/pdf/2312.15001v2.pdf","comment":"Published as a conference paper at ICLR 2024; Code available at\n  https://github.com/smonsays/modular-hyperteacher"},{"id":"http://arxiv.org/abs/2403.16933v1","updated":"2024-03-25T16:57:02Z","published":"2024-03-25T16:57:02Z","title":"Backpropagation through space, time, and the brain","summary":"  Effective learning in neuronal networks requires the adaptation of individual\nsynapses given their relative contribution to solving a task. However, physical\nneuronal systems -- whether biological or artificial -- are constrained by\nspatio-temporal locality. How such networks can perform efficient credit\nassignment, remains, to a large extent, an open question. In Machine Learning,\nthe answer is almost universally given by the error backpropagation algorithm,\nthrough both space (BP) and time (BPTT). However, BP(TT) is well-known to rely\non biologically implausible assumptions, in particular with respect to\nspatiotemporal (non-)locality, while forward-propagation models such as\nreal-time recurrent learning (RTRL) suffer from prohibitive memory constraints.\nWe introduce Generalized Latent Equilibrium (GLE), a computational framework\nfor fully local spatio-temporal credit assignment in physical, dynamical\nnetworks of neurons. We start by defining an energy based on neuron-local\nmismatches, from which we derive both neuronal dynamics via stationarity and\nparameter dynamics via gradient descent. The resulting dynamics can be\ninterpreted as a real-time, biologically plausible approximation of BPTT in\ndeep cortical networks with continuous-time neuronal dynamics and continuously\nactive, local synaptic plasticity. In particular, GLE exploits the ability of\nbiological neurons to phase-shift their output rate with respect to their\nmembrane potential, which is essential in both directions of information\npropagation. For the forward computation, it enables the mapping of\ntime-continuous inputs to neuronal space, performing an effective\nspatiotemporal convolution. For the backward computation, it permits the\ntemporal inversion of feedback signals, which consequently approximate the\nadjoint states necessary for useful parameter updates.\n","authors":["Benjamin Ellenberger","Paul Haider","Jakob Jordan","Kevin Max","Ismael Jaras","Laura Kriener","Federico Benitez","Mihai A. Petrovici"],"pdf_url":"https://arxiv.org/pdf/2403.16933v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.16930v1","updated":"2024-03-25T16:49:38Z","published":"2024-03-25T16:49:38Z","title":"FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN","summary":"  Federated Learning (FL) provides a privacy-preserving mechanism for\ndistributed training of machine learning models on networked devices (e.g.,\nmobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the\nedge by creating models without sharing the actual data across the network.\nExisting research works typically focus on generic aspects of non-IID data and\nheterogeneity in client's system characteristics, but they often neglect the\nissue of insufficient data for model development, which can arise from uneven\nclass label distribution and highly variable data volumes across edge nodes. In\nthis work, we propose FLIGAN, a novel approach to address the issue of data\nincompleteness in FL. First, we leverage Generative Adversarial Networks (GANs)\nto adeptly capture complex data distributions and generate synthetic data that\nclosely resemble the real-world data. Then, we use synthetic data to enhance\nthe robustness and completeness of datasets across nodes. Our methodology\nadheres to FL's privacy requirements by generating synthetic data in a\nfederated manner without sharing the actual data in the process. We incorporate\ntechniques such as classwise sampling and node grouping, designed to improve\nthe federated GAN's performance, enabling the creation of high-quality\nsynthetic datasets and facilitating efficient FL training. Empirical results\nfrom our experiments demonstrate that FLIGAN significantly improves the model\naccuracy, especially in scenarios with high class imbalances, achieving up to a\n20% increase in model accuracy over traditional FL baselines.\n","authors":["Paul Joe Maliakel","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.16930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07240v4","updated":"2024-03-25T16:49:18Z","published":"2023-10-11T07:08:20Z","title":"CacheGen: KV Cache Compression and Streaming for Fast Language Model\n  Serving","summary":"  As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.\n","authors":["Yuhan Liu","Hanchen Li","Yihua Cheng","Siddhant Ray","Yuyang Huang","Qizheng Zhang","Kuntai Du","Jiayi Yao","Shan Lu","Ganesh Ananthanarayanan","Michael Maire","Henry Hoffmann","Ari Holtzman","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07240v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16916v1","updated":"2024-03-25T16:36:13Z","published":"2024-03-25T16:36:13Z","title":"SCOD: From Heuristics to Theory","summary":"  This paper addresses the problem of designing reliable prediction models that\nabstain from predictions when faced with uncertain or out-of-distribution\nsamples - a recently proposed problem known as Selective Classification in the\npresence of Out-of-Distribution data (SCOD). We make three key contributions to\nSCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes\nclassifier for in-distribution (ID) data and a selector represented as a\nstochastic linear classifier in a 2D space, using i) the conditional risk of\nthe ID classifier, and ii) the likelihood ratio of ID and out-of-distribution\n(OOD) data as input. This contrasts with suboptimal strategies from current OOD\ndetection methods and the Softmax Information Retaining Combination (SIRC),\nspecifically developed for SCOD. Secondly, we establish that in a\ndistribution-free setting, the SCOD problem is not Probably Approximately\nCorrect learnable when relying solely on an ID data sample. Third, we introduce\nPOSCOD, a simple method for learning a plugin estimate of the optimal SCOD\nstrategy from both an ID data sample and an unlabeled mixture of ID and OOD\ndata. Our empirical results confirm the theoretical findings and demonstrate\nthat our proposed method, POSCOD, out performs existing OOD methods in\neffectively addressing the SCOD problem.\n","authors":["Vojtech Franc","Jakub Paplham","Daniel Prusa"],"pdf_url":"https://arxiv.org/pdf/2403.16916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16899v1","updated":"2024-03-25T16:10:47Z","published":"2024-03-25T16:10:47Z","title":"State Space Models as Foundation Models: A Control Theoretic Overview","summary":"  In recent years, there has been a growing interest in integrating linear\nstate-space models (SSM) in deep neural network architectures of foundation\nmodels. This is exemplified by the recent success of Mamba, showing better\nperformance than the state-of-the-art Transformer architectures in language\ntasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a\nlatent space in order to learn a compressed representation of the data. The\nsame goal has been pursued by control theorists using SSMs to efficiently model\ndynamical systems. Therefore, SSMs can be naturally connected to deep sequence\nmodeling, offering the opportunity to create synergies between the\ncorresponding research areas. This paper is intended as a gentle introduction\nto SSM-based architectures for control theorists and summarizes the latest\nresearch developments. It provides a systematic review of the most successful\nSSM proposals and highlights their main features from a control theoretic\nperspective. Additionally, we present a comparative analysis of these models,\nevaluating their performance on a standardized benchmark designed for assessing\na model's efficiency at learning long sequences.\n","authors":["Carmen Amo Alonso","Jerome Sieber","Melanie N. Zeilinger"],"pdf_url":"https://arxiv.org/pdf/2403.16899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09962v3","updated":"2024-03-25T16:07:31Z","published":"2022-12-20T02:30:13Z","title":"Distributional Robustness Bounds Generalization Errors","summary":"  Bayesian methods, distributionally robust optimization methods, and\nregularization methods are three pillars of trustworthy machine learning\ncombating distributional uncertainty, e.g., the uncertainty of an empirical\ndistribution compared to the true underlying distribution. This paper\ninvestigates the connections among the three frameworks and, in particular,\nexplores why these frameworks tend to have smaller generalization errors.\nSpecifically, first, we suggest a quantitative definition for \"distributional\nrobustness\", propose the concept of \"robustness measure\", and formalize several\nphilosophical concepts in distributionally robust optimization. Second, we show\nthat Bayesian methods are distributionally robust in the probably approximately\ncorrect (PAC) sense; in addition, by constructing a Dirichlet-process-like\nprior in Bayesian nonparametrics, it can be proven that any regularized\nempirical risk minimization method is equivalent to a Bayesian method. Third,\nwe show that generalization errors of machine learning models can be\ncharacterized using the distributional uncertainty of the nominal distribution\nand the robustness measures of these machine learning models, which is a new\nperspective to bound generalization errors, and therefore, explain the reason\nwhy distributionally robust machine learning models, Bayesian models, and\nregularization models tend to have smaller generalization errors in a unified\nmanner.\n","authors":["Shixiong Wang","Haowei Wang"],"pdf_url":"https://arxiv.org/pdf/2212.09962v3.pdf","comment":"Updated Version"},{"id":"http://arxiv.org/abs/2403.10855v2","updated":"2024-03-25T16:07:24Z","published":"2024-03-16T08:30:55Z","title":"Reinforcement Learning with Options and State Representation","summary":"  The current thesis aims to explore the reinforcement learning field and build\non existing methods to produce improved ones to tackle the problem of learning\nin high-dimensional and complex environments. It addresses such goals by\ndecomposing learning tasks in a hierarchical fashion known as Hierarchical\nReinforcement Learning.\n  We start in the first chapter by getting familiar with the Markov Decision\nProcess framework and presenting some of its recent techniques that the\nfollowing chapters use. We then proceed to build our Hierarchical Policy\nlearning as an answer to the limitations of a single primitive policy. The\nhierarchy is composed of a manager agent at the top and employee agents at the\nlower level.\n  In the last chapter, which is the core of this thesis, we attempt to learn\nlower-level elements of the hierarchy independently of the manager level in\nwhat is known as the \"Eigenoption\". Based on the graph structure of the\nenvironment, Eigenoptions allow us to build agents that are aware of the\ngeometric and dynamic properties of the environment. Their decision-making has\na special property: it is invariant to symmetric transformations of the\nenvironment, allowing as a consequence to greatly reduce the complexity of the\nlearning task.\n","authors":["Ayoub Ghriss","Masashi Sugiyama","Alessandro Lazaric"],"pdf_url":"https://arxiv.org/pdf/2403.10855v2.pdf","comment":"Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP"},{"id":"http://arxiv.org/abs/2306.08318v2","updated":"2024-03-25T16:06:34Z","published":"2023-06-14T07:38:01Z","title":"Identification of Energy Management Configuration Concepts from a Set of\n  Pareto-optimal Solutions","summary":"  Implementing resource efficient energy management systems in facilities and\nbuildings becomes increasingly important in the transformation to a sustainable\nsociety. However, selecting a suitable configuration based on multiple,\ntypically conflicting objectives, such as cost, robustness with respect to\nuncertainty of grid operation, or renewable energy utilization, is a difficult\nmulti-criteria decision making problem. The recently developed concept\nidentification technique can facilitate a decision maker by sorting\nconfiguration options into semantically meaningful groups (concepts). In this\nprocess, the partitioning of the objectives and design parameters into\ndifferent sets (called description spaces) is a very important step. In this\nstudy we focus on utilizing the concept identification technique for finding\nrelevant and viable energy management configurations from a very large data set\nof Pareto-optimal solutions. The data set consists of 20000 realistic\nPareto-optimal building energy management configurations generated by a\nmany-objective evolutionary optimization of a high quality Digital Twin energy\nmanagement simulator. We analyze how the choice of description spaces, i.e.,\nthe partitioning of the objectives and parameters, impacts the type of\ninformation that can be extracted. We show that the decision maker can\nintroduce constraints and biases into that process to meet expectations and\npreferences. The iterative approach presented in this work allows for the\ngeneration of valuable insights into trade-offs between specific objectives,\nand constitutes a powerful and flexible tool to support the decision making\nprocess when designing large and complex energy management systems.\n","authors":["Felix Lanfermann","Qiqi Liu","Yaochu Jin","Sebastian Schmitt"],"pdf_url":"https://arxiv.org/pdf/2306.08318v2.pdf","comment":"18 pages, 8 figures, accepted at Energy Conversion and Management: X"},{"id":"http://arxiv.org/abs/2311.10610v2","updated":"2024-03-25T16:05:04Z","published":"2023-11-17T16:04:31Z","title":"A Poincaré Inequality and Consistency Results for Signal Sampling on\n  Large Graphs","summary":"  Large-scale graph machine learning is challenging as the complexity of\nlearning models scales with the graph size. Subsampling the graph is a viable\nalternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\nExisting graph sampling techniques require not only computing the spectra of\nlarge matrices but also repeating these computations when the graph changes,\ne.g., grows. In this paper, we introduce a signal sampling theory for a type of\ngraph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\nsignals and show that complements of node subsets satisfying this inequality\nare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\nconnections with spectral clustering and Gaussian elimination, we prove that\nsuch sampling sets are consistent in the sense that unique sampling sets on a\nconvergent graph sequence converge to unique sampling sets on the graphon. We\nthen propose a related graphon signal sampling algorithm for large graphs, and\ndemonstrate its good empirical performance on graph machine learning tasks.\n","authors":["Thien Le","Luana Ruiz","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2311.10610v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17042v1","updated":"2024-03-25T15:58:26Z","published":"2024-03-25T15:58:26Z","title":"Provably Robust Score-Based Diffusion Posterior Sampling for\n  Plug-and-Play Image Reconstruction","summary":"  In a great number of tasks in science and engineering, the goal is to infer\nan unknown image from a small number of measurements collected from a known\nforward model describing certain sensing or imaging modality. Due to resource\nconstraints, this task is often extremely ill-posed, which necessitates the\nadoption of expressive prior information to regularize the solution space.\nScore-based diffusion models, due to its impressive empirical success, have\nemerged as an appealing candidate of an expressive prior in image\nreconstruction. In order to accommodate diverse tasks at once, it is of great\ninterest to develop efficient, consistent and robust algorithms that\nincorporate {\\em unconditional} score functions of an image prior distribution\nin conjunction with flexible choices of forward models.\n  This work develops an algorithmic framework for employing score-based\ndiffusion models as an expressive data prior in general nonlinear inverse\nproblems. Motivated by the plug-and-play framework in the imaging community, we\nintroduce a diffusion plug-and-play method (\\textsf{DPnP}) that alternatively\ncalls two samplers, a proximal consistency sampler based solely on the\nlikelihood function of the forward model, and a denoising diffusion sampler\nbased solely on the score functions of the image prior. The key insight is that\ndenoising under white Gaussian noise can be solved {\\em rigorously} via both\nstochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using\nthe unconditional score functions. We establish both asymptotic and\nnon-asymptotic performance guarantees of \\textsf{DPnP}, and provide numerical\nexperiments to illustrate its promise in solving both linear and nonlinear\nimage reconstruction tasks. To the best of our knowledge, \\textsf{DPnP} is the\nfirst provably-robust posterior sampling method for nonlinear inverse problems\nusing unconditional diffusion priors.\n","authors":["Xingyu Xu","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2403.17042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01327v2","updated":"2024-03-25T15:55:22Z","published":"2023-10-02T16:45:19Z","title":"TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate\n  Time Series","summary":"  We introduce a new model for multivariate probabilistic time series\nprediction, designed to flexibly address a range of tasks including\nforecasting, interpolation, and their combinations. Building on copula theory,\nwe propose a simplified objective for the recently-introduced transformer-based\nattentional copulas (TACTiS), wherein the number of distributional parameters\nnow scales linearly with the number of variables instead of factorially. The\nnew objective requires the introduction of a training curriculum, which goes\nhand-in-hand with necessary changes to the original architecture. We show that\nthe resulting model has significantly better training dynamics and achieves\nstate-of-the-art performance across diverse real-world forecasting tasks, while\nmaintaining the flexibility of prior work, such as seamless handling of\nunaligned and unevenly-sampled time series. Code is made available at\nhttps://github.com/ServiceNow/TACTiS.\n","authors":["Arjun Ashok","Étienne Marcotte","Valentina Zantedeschi","Nicolas Chapados","Alexandre Drouin"],"pdf_url":"https://arxiv.org/pdf/2310.01327v2.pdf","comment":"28 pages, 15 figures, The Twelfth International Conference on\n  Learning Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2403.16883v1","updated":"2024-03-25T15:53:32Z","published":"2024-03-25T15:53:32Z","title":"Discrete Latent Graph Generative Modeling with Diffusion Bridges","summary":"  Learning graph generative models over latent spaces has received less\nattention compared to models that operate on the original data space and has so\nfar demonstrated lacklustre performance. We present GLAD a latent space graph\ngenerative model. Unlike most previous latent space graph generative models,\nGLAD operates on a discrete latent space that preserves to a significant extent\nthe discrete nature of the graph structures making no unnatural assumptions\nsuch as latent space continuity. We learn the prior of our discrete latent\nspace by adapting diffusion bridges to its structure. By operating over an\nappropriately constructed latent space we avoid relying on decompositions that\nare often used in models that operate in the original data space. We present\nexperiments on a series of graph benchmark datasets which clearly show the\nsuperiority of the discrete latent space and obtain state of the art graph\ngenerative performance, making GLAD the first latent space graph generative\nmodel with competitive performance. Our source code is published at:\n\\url{https://github.com/v18nguye/GLAD}.\n","authors":["Van Khoa Nguyen","Yoann Boget","Frantzeska Lavda","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2403.16883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14589v2","updated":"2024-03-25T15:45:35Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotation or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16877v1","updated":"2024-03-25T15:42:09Z","published":"2024-03-25T15:42:09Z","title":"Proprioception Is All You Need: Terrain Classification for Boreal\n  Forests","summary":"  Recent works in field robotics highlighted the importance of resiliency\nagainst different types of terrains. Boreal forests, in particular, are home to\nmany mobility-impeding terrains that should be considered for off-road\nautonomous navigation. Also, being one of the largest land biomes on Earth,\nboreal forests are an area where autonomous vehicles are expected to become\nincreasingly common. In this paper, we address this issue by introducing\nBorealTC, a publicly available dataset for proprioceptive-based terrain\nclassification (TC). Recorded with a Husky A200, our dataset contains 116 min\nof Inertial Measurement Unit (IMU), motor current, and wheel odometry data,\nfocusing on typical boreal forest terrains, notably snow, ice, and silty loam.\nCombining our dataset with another dataset from the state-of-the-art, we\nevaluate both a Convolutional Neural Network (CNN) and the novel state space\nmodel (SSM)-based Mamba architecture on a TC task. Interestingly, we show that\nwhile CNN outperforms Mamba on each separate dataset, Mamba achieves greater\naccuracy when trained on a combination of both. In addition, we demonstrate\nthat Mamba's learning capacity is greater than a CNN for increasing amounts of\ndata. We show that the combination of two TC datasets yields a latent space\nthat can be interpreted with the properties of the terrains. We also discuss\nthe implications of merging datasets on classification. Our source code and\ndataset are publicly available online:\nhttps://github.com/norlab-ulaval/BorealTC.\n","authors":["Damien LaRocque","William Guimont-Martin","David-Alexandre Duclos","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2403.16877v1.pdf","comment":"Submitted to the 2024 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.16871v1","updated":"2024-03-25T15:37:43Z","published":"2024-03-25T15:37:43Z","title":"Conformal Off-Policy Prediction for Multi-Agent Systems","summary":"  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy\nusing only data collected under a nominal (behavioural) policy, is a paramount\nproblem in data-driven analysis of safety-critical systems where the deployment\nof a new policy may be unsafe. To achieve dependable off-policy predictions,\nrecent work on Conformal Off-Policy Prediction (COPP) leverage the conformal\nprediction framework to derive prediction regions with probabilistic guarantees\nunder the target process. Existing COPP methods can account for the\ndistribution shifts induced by policy switching, but are limited to\nsingle-agent systems and scalar outcomes (e.g., rewards). In this work, we\nintroduce MA-COPP, the first conformal prediction method to solve OPP problems\ninvolving multi-agent systems, deriving joint prediction regions for all\nagents' trajectories when one or more \"ego\" agents change their policies.\nUnlike the single-agent scenario, this setting introduces higher complexity as\nthe distribution shifts affect predictions for all agents, not just the ego\nagents, and the prediction task involves full multi-dimensional trajectories,\nnot just reward values. A key contribution of MA-COPP is to avoid enumeration\nor exhaustive search of the output space of agent trajectories, which is\ninstead required by existing COPP methods to construct the prediction region.\nWe achieve this by showing that an over-approximation of the true JPR can be\nconstructed, without enumeration, from the maximum density ratio of the JPR\ntrajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems\nfrom the PettingZoo library and the F1TENTH autonomous racing environment,\nachieving nominal coverage in higher dimensions and various shift settings.\n","authors":["Tom Kuipers","Renukanandan Tumu","Shuo Yang","Milad Kazemi","Rahul Mangharam","Nicola Paoletti"],"pdf_url":"https://arxiv.org/pdf/2403.16871v1.pdf","comment":"Submitted to the 63rd IEEE Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2403.16862v1","updated":"2024-03-25T15:26:32Z","published":"2024-03-25T15:26:32Z","title":"INPC: Implicit Neural Point Clouds for Radiance Field Rendering","summary":"  We introduce a new approach for reconstruction and novel-view synthesis of\nunbounded real-world scenes. In contrast to previous methods using either\nvolumetric fields, grid-based models, or discrete point cloud proxies, we\npropose a hybrid scene representation, which implicitly encodes a point cloud\nin a continuous octree-based probability field and a multi-resolution hash\ngrid. In doing so, we combine the benefits of both worlds by retaining\nfavorable behavior during optimization: Our novel implicit point cloud\nrepresentation and differentiable bilinear rasterizer enable fast rendering\nwhile preserving fine geometric detail without depending on initial priors like\nstructure-from-motion point clouds. Our method achieves state-of-the-art image\nquality on several common benchmark datasets. Furthermore, we achieve fast\ninference at interactive frame rates, and can extract explicit point clouds to\nfurther enhance performance.\n","authors":["Florian Hahlbohm","Linus Franke","Moritz Kappel","Susana Castillo","Marc Stamminger","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2403.16862v1.pdf","comment":"Project page: https://fhahlbohm.github.io/inpc/"},{"id":"http://arxiv.org/abs/2403.16855v1","updated":"2024-03-25T15:18:23Z","published":"2024-03-25T15:18:23Z","title":"Semantic-Aware Remote Estimation of Multiple Markov Sources Under\n  Constraints","summary":"  This paper studies semantic-aware communication for remote estimation of\nmultiple Markov sources over a lossy and rate-constrained channel. Unlike most\nexisting studies that treat all source states equally, we exploit the semantics\nof information and consider that the remote actuator has different tolerances\nfor the estimation errors of different states. We aim to find an optimal\nscheduling policy that minimizes the long-term state-dependent costs of\nestimation errors under a transmission frequency constraint. We theoretically\nshow the structure of the optimal policy by leveraging the average-cost\nConstrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic\nprogramming. By exploiting the optimal structural results, we develop a novel\npolicy search algorithm, termed intersection search plus relative value\niteration (Insec-RVI), that can find the optimal policy using only a few\niterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an\nonline low-complexity drift-plus-penalty (DPP) scheduling algorithm based on\nthe Lyapunov optimization theorem. We also design an efficient average-cost\nQ-learning algorithm to estimate the optimal policy without knowing a priori\nthe channel and source statistics. Numerical results show that continuous\ntransmission is inefficient, and remarkably, our semantic-aware policies can\nattain the optimum by strategically utilizing fewer transmissions by exploiting\nthe timing of the important information.\n","authors":["Jiping Luo","Nikolaos Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.16855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01295v2","updated":"2024-03-25T15:17:45Z","published":"2024-02-02T10:34:13Z","title":"ExtremeCast: Boosting Extreme Value Prediction for Global Weather\n  Forecast","summary":"  Data-driven weather forecast based on machine learning (ML) has experienced\nrapid development and demonstrated superior performance in the global\nmedium-range forecast compared to traditional physics-based dynamical models.\nHowever, most of these ML models struggle with accurately predicting extreme\nweather, which is closely related to the extreme value prediction. Through\nmathematical analysis, we prove that the use of symmetric losses, such as the\nMean Squared Error (MSE), leads to biased predictions and underestimation of\nextreme values. To address this issue, we introduce Exloss, a novel loss\nfunction that performs asymmetric optimization and highlights extreme values to\nobtain accurate extreme weather forecast. Furthermore, we introduce a\ntraining-free extreme value enhancement strategy named ExEnsemble, which\nincreases the variance of pixel values and improves the forecast robustness.\nCombined with an advanced global weather forecast model, extensive experiments\nshow that our solution can achieve state-of-the-art performance in extreme\nweather prediction, while maintaining the overall forecast accuracy comparable\nto the top medium-range forecast models.\n","authors":["Wanghan Xu","Kang Chen","Tao Han","Hao Chen","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2402.01295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16851v1","updated":"2024-03-25T15:15:09Z","published":"2024-03-25T15:15:09Z","title":"Can ChatGPT predict article retraction based on Twitter mentions?","summary":"  Detecting problematic research articles timely is a vital task. This study\nexplores whether Twitter mentions of retracted articles can signal potential\nproblems with the articles prior to retraction, thereby playing a role in\npredicting future retraction of problematic articles. A dataset comprising\n3,505 retracted articles and their associated Twitter mentions is analyzed,\nalongside 3,505 non-retracted articles with similar characteristics obtained\nusing the Coarsened Exact Matching method. The effectiveness of Twitter\nmentions in predicting article retraction is evaluated by four prediction\nmethods, including manual labelling, keyword identification, machine learning\nmodels, and ChatGPT. Manual labelling results indicate that there are indeed\nretracted articles with their Twitter mentions containing recognizable evidence\nsignaling problems before retraction, although they represent only a limited\nshare of all retracted articles with Twitter mention data (approximately 16%).\nUsing the manual labelling results as the baseline, ChatGPT demonstrates\nsuperior performance compared to other methods, implying its potential in\nassisting human judgment for predicting article retraction. This study uncovers\nboth the potential and limitation of social media events as an early warning\nsystem for article retraction, shedding light on a potential application of\ngenerative artificial intelligence in promoting research integrity.\n","authors":["Er-Te Zheng","Hui-Zhen Fu","Zhichao Fang"],"pdf_url":"https://arxiv.org/pdf/2403.16851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07556v2","updated":"2024-03-25T15:11:56Z","published":"2022-11-14T17:22:50Z","title":"Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF\n  Magnetic Marker Localization","summary":"  Tracking passive magnetic markers plays a vital role in advancing healthcare\nand robotics, offering the potential to significantly improve the precision and\nefficiency of systems. This technology is key to developing smarter, more\nresponsive tools and devices, such as enhanced surgical instruments, precise\ndiagnostic tools, and robots with improved environmental interaction\ncapabilities. However, traditionally, the tracking of magnetic markers is\ncomputationally expensive due to the requirement for iterative optimization\nprocedures. Moreover, these methods depend on the magnetic dipole model for\ntheir optimization function, which can yield imprecise outcomes due to the\nmodel's significant inaccuracies when dealing with short distances between\nnon-spherical magnet and sensor.Our paper introduces a novel approach that\nleverages neural networks to bypass these limitations, directly inferring the\nmarker's position and orientation to accurately determine the magnet's 5 DoF in\na single step without initial estimation. Although our method demands an\nextensive supervised training phase, we mitigate this by introducing a\ncomputationally more efficient method to generate synthetic, yet realistic data\nusing Finite Element Methods simulations. The benefits of fast and accurate\ninference significantly outweigh the offline training preparation. In our\nevaluation, we use different cylindrical magnets, tracked with a square array\nof 16 sensors. We perform the sensors' reading and position inference on a\nportable, neural networks-oriented single-board computer, ensuring a compact\nsetup. We benchmark our prototype against vision-based ground truth data,\nachieving a mean positional error of 4 mm and an orientation error of 8 degrees\nwithin a 0.2x0.2x0.15 m working volume. These results showcase our prototype's\nability to balance accuracy and compactness effectively in tracking 5 DoF.\n","authors":["Mengfan Wu","Thomas Langerak","Otmar Hilliges","Juan Zarate"],"pdf_url":"https://arxiv.org/pdf/2211.07556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11118v5","updated":"2024-03-25T15:10:57Z","published":"2023-01-26T14:13:37Z","title":"Dual Box Embeddings for the Description Logic EL++","summary":"  OWL ontologies, whose formal semantics are rooted in Description Logic (DL),\nhave been widely used for knowledge representation. Similar to Knowledge Graphs\n(KGs), ontologies are often incomplete, and maintaining and constructing them\nhas proved challenging. While classical deductive reasoning algorithms use the\nprecise formal semantics of an ontology to predict missing facts, recent years\nhave witnessed growing interest in inductive reasoning techniques that can\nderive probable facts from an ontology. Similar to KGs, a promising approach is\nto learn ontology embeddings in a latent vector space, while additionally\nensuring they adhere to the semantics of the underlying DL. While a variety of\napproaches have been proposed, current ontology embedding methods suffer from\nseveral shortcomings, especially that they all fail to faithfully model\none-to-many, many-to-one, and many-to-many relations and role inclusion axioms.\nTo address this problem and improve ontology completion performance, we propose\na novel ontology embedding method named Box$^2$EL for the DL EL++, which\nrepresents both concepts and roles as boxes (i.e., axis-aligned\nhyperrectangles), and models inter-concept relationships using a bumping\nmechanism. We theoretically prove the soundness of Box$^2$EL and conduct an\nextensive experimental evaluation, achieving state-of-the-art results across a\nvariety of datasets on the tasks of subsumption prediction, role assertion\nprediction, and approximating deductive reasoning.\n","authors":["Mathias Jackermeier","Jiaoyan Chen","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2301.11118v5.pdf","comment":"Updated license information"},{"id":"http://arxiv.org/abs/2403.16846v1","updated":"2024-03-25T15:07:50Z","published":"2024-03-25T15:07:50Z","title":"GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs","summary":"  Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs\nwith time-varying interactions, face a significant challenge in explainability\ndue to their complex model structure. Counterfactual explanations, crucial for\nunderstanding model decisions, examine how input graph changes affect outcomes.\nThis paper introduces two novel counterfactual explanation methods for TGNNs:\nGreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer\nfor Dynamic Graphs). They treat explanations as a search problem, seeking input\ngraph alterations that alter model predictions. GreeDy uses a simple, greedy\napproach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm.\nExperiments show both methods effectively generate clear explanations. Notably,\nCoDy outperforms GreeDy and existing factual methods, with up to 59\\% higher\nsuccess rate in finding significant counterfactual inputs. This highlights\nCoDy's potential in clarifying TGNN decision-making, increasing their\ntransparency and trustworthiness in practice.\n","authors":["Zhan Qu","Daniel Gomm","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2403.16846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16843v1","updated":"2024-03-25T15:04:11Z","published":"2024-03-25T15:04:11Z","title":"Do LLM Agents Have Regret? A Case Study in Online Learning and Games","summary":"  Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.\n","authors":["Chanwoo Park","Xiangyu Liu","Asuman Ozdaglar","Kaiqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16829v1","updated":"2024-03-25T14:54:42Z","published":"2024-03-25T14:54:42Z","title":"Convergence of a model-free entropy-regularized inverse reinforcement\n  learning algorithm","summary":"  Given a dataset of expert demonstrations, inverse reinforcement learning\n(IRL) aims to recover a reward for which the expert is optimal. This work\nproposes a model-free algorithm to solve entropy-regularized IRL problem. In\nparticular, we employ a stochastic gradient descent update for the reward and a\nstochastic soft policy iteration update for the policy. Assuming access to a\ngenerative model, we prove that our algorithm is guaranteed to recover a reward\nfor which the expert is $\\varepsilon$-optimal using\n$\\mathcal{O}(1/\\varepsilon^{2})$ samples of the Markov decision process (MDP).\nFurthermore, with $\\mathcal{O}(1/\\varepsilon^{4})$ samples we prove that the\noptimal policy corresponding to the recovered reward is $\\varepsilon$-close to\nthe expert policy in total variation distance.\n","authors":["Titouan Renard","Andreas Schlaginhaufen","Tingting Ni","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2403.16829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16825v1","updated":"2024-03-25T14:49:01Z","published":"2024-03-25T14:49:01Z","title":"Weak Convergence Analysis of Online Neural Actor-Critic Algorithms","summary":"  We prove that a single-layer neural network trained with the online actor\ncritic algorithm converges in distribution to a random ordinary differential\nequation (ODE) as the number of hidden units and the number of training steps\n$\\rightarrow \\infty$. In the online actor-critic algorithm, the distribution of\nthe data samples dynamically changes as the model is updated, which is a key\nchallenge for any convergence analysis. We establish the geometric ergodicity\nof the data samples under a fixed actor policy. Then, using a Poisson equation,\nwe prove that the fluctuations of the model updates around the limit\ndistribution due to the randomly-arriving data samples vanish as the number of\nparameter updates $\\rightarrow \\infty$. Using the Poisson equation and weak\nconvergence techniques, we prove that the actor neural network and critic\nneural network converge to the solutions of a system of ODEs with random\ninitial conditions. Analysis of the limit ODE shows that the limit critic\nnetwork will converge to the true value function, which will provide the actor\nan asymptotically unbiased estimate of the policy gradient. We then prove that\nthe limit actor network will converge to a stationary point.\n","authors":["Samuel Chun-Hei Lam","Justin Sirignano","Ziheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12851v2","updated":"2024-03-25T14:48:55Z","published":"2023-07-24T14:51:54Z","title":"Early Neuron Alignment in Two-layer ReLU Networks with Small\n  Initialization","summary":"  This paper studies the problem of training a two-layer ReLU network for\nbinary classification using gradient flow with small initialization. We\nconsider a training dataset with well-separated input vectors: Any pair of\ninput data with the same label are positively correlated, and any pair with\ndifferent labels are negatively correlated. Our analysis shows that, during the\nearly phase of training, neurons in the first layer try to align with either\nthe positive data or the negative data, depending on its corresponding weight\non the second layer. A careful analysis of the neurons' directional dynamics\nallows us to provide an $\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$ upper bound on\nthe time it takes for all neurons to achieve good alignment with the input\ndata, where $n$ is the number of data points and $\\mu$ measures how well the\ndata are separated. After the early alignment phase, the loss converges to zero\nat a $\\mathcal{O}(\\frac{1}{t})$ rate, and the weight matrix on the first layer\nis approximately low-rank. Numerical experiments on the MNIST dataset\nillustrate our theoretical findings.\n","authors":["Hancheng Min","Enrique Mallada","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2307.12851v2.pdf","comment":"iclr 2024 camera-ready"},{"id":"http://arxiv.org/abs/2403.16823v1","updated":"2024-03-25T14:48:00Z","published":"2024-03-25T14:48:00Z","title":"Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A\n  User-Centric Learning Approach","summary":"  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)\nare an emerging indoor wireless communication paradigm, which combines the\nadvantages of the capacious optical spectra of LiFi and ubiquitous coverage of\nWiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource\nmanagement for such hybrid networks. The existing LB methods are mostly\nnetwork-centric, relying on a central unit to make a solution for the users all\nat once. Consequently, the solution needs to be updated for all users at the\nsame pace, regardless of their moving status. This would affect the network\nperformance in two aspects: i) when the update frequency is low, it would\ncompromise the connectivity of fast-moving users; ii) when the update frequency\nis high, it would cause unnecessary handovers as well as hefty feedback costs\nfor slow-moving users. Motivated by this, we investigate user-centric LB which\nallows users to update their solutions at different paces. The research is\ndeveloped upon our previous work on adaptive target-condition neural network\n(ATCNN), which can conduct LB for individual users in quasi-static channels. In\nthis paper, a deep neural network (DNN) model is designed to enable an adaptive\nupdate interval for each individual user. This new model is termed as\nmobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a\nuser-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is\nproposed to handle resource management and mobility management simultaneously.\nResults show that at the same level of average update interval, MS-ATCNN can\nachieve a network throughput up to 215\\% higher than conventional LB methods\nsuch as game theory, especially for a larger number of users. In addition,\nMS-ATCNN costs an ultra low runtime at the level of 100s $\\mu$s, which is two\nto three orders of magnitude lower than game theory.\n","authors":["Han Ji","Xiping Wu"],"pdf_url":"https://arxiv.org/pdf/2403.16823v1.pdf","comment":"12 pages, 12 figures, 3 tables, submitted to IEEE TWC"},{"id":"http://arxiv.org/abs/2403.16818v1","updated":"2024-03-25T14:46:24Z","published":"2024-03-25T14:46:24Z","title":"Multiple-Source Localization from a Single-Snapshot Observation Using\n  Graph Bayesian Optimization","summary":"  Due to the significance of its various applications, source localization has\ngarnered considerable attention as one of the most important means to confront\ndiffusion hazards. Multi-source localization from a single-snapshot observation\nis especially relevant due to its prevalence. However, the inherent\ncomplexities of this problem, such as limited information, interactions among\nsources, and dependence on diffusion models, pose challenges to resolution.\nCurrent methods typically utilize heuristics and greedy selection, and they are\nusually bonded with one diffusion model. Consequently, their effectiveness is\nconstrained. To address these limitations, we propose a simulation-based method\ntermed BOSouL. Bayesian optimization (BO) is adopted to approximate the results\nfor its sample efficiency. A surrogate function models uncertainty from the\nlimited information. It takes sets of nodes as the input instead of individual\nnodes. BOSouL can incorporate any diffusion model in the data acquisition\nprocess through simulations. Empirical studies demonstrate that its performance\nis robust across graph structures and diffusion models. The code is available\nat https://github.com/XGraph-Team/BOSouL.\n","authors":["Zonghan Zhang","Zijian Zhang","Zhiqian Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16818v1.pdf","comment":"Proceedings of the AAAI Conference on Artificial Intelligence, 2024"},{"id":"http://arxiv.org/abs/2403.16809v1","updated":"2024-03-25T14:32:28Z","published":"2024-03-25T14:32:28Z","title":"An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems","summary":"  The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.\n","authors":["Hanqing Yang","Marie Siew","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16809v1.pdf","comment":"Accepted at International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at\n  CPS-IoT Week 2024"},{"id":"http://arxiv.org/abs/2403.16798v1","updated":"2024-03-25T14:17:38Z","published":"2024-03-25T14:17:38Z","title":"Cluster-Based Normalization Layer for Neural Networks","summary":"  Deep learning faces significant challenges during the training of neural\nnetworks, including internal covariate shift, label shift, vanishing/exploding\ngradients, overfitting, and computational complexity. While conventional\nnormalization methods, such as Batch Normalization, aim to tackle some of these\nissues, they often depend on assumptions that constrain their adaptability.\nMixture Normalization faces computational hurdles in its pursuit of handling\nmultiple Gaussian distributions. This paper introduces Cluster-Based\nNormalization (CB-Norm) in two variants - Supervised Cluster-Based\nNormalization (SCB-Norm) and Unsupervised Cluster-Based Normalization\n(UCB-Norm) - proposing a groundbreaking one-step normalization approach.\nCB-Norm leverages a Gaussian mixture model to specifically address challenges\nrelated to gradient stability and learning acceleration. For SCB-Norm, a\nsupervised variant, the novel mechanism involves introducing predefined data\npartitioning, termed clusters, to normalize activations based on the assigned\ncluster. This cluster-driven approach creates a space that conforms to a\nGaussian mixture model. On the other hand, UCB-Norm, an unsupervised\ncounterpart, dynamically clusters neuron activations during training, adapting\nto task-specific challenges without relying on predefined data partitions\n(clusters). This dual approach ensures flexibility in addressing diverse\nlearning scenarios. CB-Norm innovatively uses a one-step normalization\napproach, where parameters of each mixture component (cluster in activation\nspace) serve as weights for deep neural networks. This adaptive clustering\nprocess tackles both clustering and resolution of deep neural network tasks\nconcurrently during training, signifying a notable advancement in the field.\n","authors":["Bilal Faye","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2403.16798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13254v2","updated":"2024-03-25T14:09:32Z","published":"2022-06-27T12:39:36Z","title":"Sample compression schemes for balls in graphs","summary":"  One of the open problems in machine learning is whether any set-family of\nVC-dimension $d$ admits a sample compression scheme of size $O(d)$. In this\npaper, we study this problem for balls in graphs. For a ball $B=B_r(x)$ of a\ngraph $G=(V,E)$, a realizable sample for $B$ is a signed subset $X=(X^+,X^-)$\nof $V$ such that $B$ contains $X^+$ and is disjoint from $X^-$. A proper sample\ncompression scheme of size $k$ consists of a compressor and a reconstructor.\nThe compressor maps any realizable sample $X$ to a subsample $X'$ of size at\nmost $k$. The reconstructor maps each such subsample $X'$ to a ball $B'$ of $G$\nsuch that $B'$ includes $X^+$ and is disjoint from $X^-$.\n  For balls of arbitrary radius $r$, we design proper labeled sample\ncompression schemes of size $2$ for trees, of size $3$ for cycles, of size $4$\nfor interval graphs, of size $6$ for trees of cycles, and of size $22$ for\ncube-free median graphs. For balls of a given radius, we design proper labeled\nsample compression schemes of size $2$ for trees and of size $4$ for interval\ngraphs. We also design approximate sample compression schemes of size 2 for\nballs of $\\delta$-hyperbolic graphs.\n","authors":["Jérémie Chalopin","Victor Chepoi","Fionn Mc Inerney","Sébastien Ratel","Yann Vaxès"],"pdf_url":"https://arxiv.org/pdf/2206.13254v2.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16790v1","updated":"2024-03-25T14:05:52Z","published":"2024-03-25T14:05:52Z","title":"Iso-Diffusion: Improving Diffusion Probabilistic Models Using the\n  Isotropy of the Additive Gaussian Noise","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in\nthe realm of generative AI. Despite their high performance, there is room for\nimprovement, especially in terms of sample fidelity by utilizing statistical\nproperties that impose structural integrity, such as isotropy. Minimizing the\nmean squared error between the additive and predicted noise alone does not\nimpose constraints on the predicted noise to be isotropic. Thus, we were\nmotivated to utilize the isotropy of the additive noise as a constraint on the\nobjective function to enhance the fidelity of DDPMs. Our approach is simple and\ncan be applied to any DDPM variant. We validate our approach by presenting\nexperiments conducted on four synthetic 2D datasets as well as on unconditional\nimage generation. As demonstrated by the results, the incorporation of this\nconstraint improves the fidelity metrics, Precision and Density for the 2D\ndatasets as well as for the unconditional image generation.\n","authors":["Dilum Fernando","Dhananjaya jayasundara","Roshan Godaliyadda","Chaminda Bandara","Parakrama Ekanayake","Vijitha Herath"],"pdf_url":"https://arxiv.org/pdf/2403.16790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16782v1","updated":"2024-03-25T13:57:45Z","published":"2024-03-25T13:57:45Z","title":"The Anatomy of Adversarial Attacks: Concept-based XAI Dissection","summary":"  Adversarial attacks (AAs) pose a significant threat to the reliability and\nrobustness of deep neural networks. While the impact of these attacks on model\npredictions has been extensively studied, their effect on the learned\nrepresentations and concepts within these models remains largely unexplored. In\nthis work, we perform an in-depth analysis of the influence of AAs on the\nconcepts learned by convolutional neural networks (CNNs) using eXplainable\nartificial intelligence (XAI) techniques. Through an extensive set of\nexperiments across various network architectures and targeted AA techniques, we\nunveil several key findings. First, AAs induce substantial alterations in the\nconcept composition within the feature space, introducing new concepts or\nmodifying existing ones. Second, the adversarial perturbation itself can be\nlinearly decomposed into a set of latent vector components, with a subset of\nthese being responsible for the attack's success. Notably, we discover that\nthese components are target-specific, i.e., are similar for a given target\nclass throughout different AA techniques and starting classes. Our findings\nprovide valuable insights into the nature of AAs and their impact on learned\nrepresentations, paving the way for the development of more robust and\ninterpretable deep learning models, as well as effective defenses against\nadversarial threats.\n","authors":["Georgii Mikriukov","Gesina Schwalbe","Franz Motzkus","Korinna Bade"],"pdf_url":"https://arxiv.org/pdf/2403.16782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16776v1","updated":"2024-03-25T13:52:48Z","published":"2024-03-25T13:52:48Z","title":"Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases","summary":"  Anatomical atlases are widely used for population analysis. Conditional\natlases target a particular sub-population defined via certain conditions (e.g.\ndemographics or pathologies) and allow for the investigation of fine-grained\nanatomical differences - such as morphological changes correlated with age.\nExisting approaches use either registration-based methods that are unable to\nhandle large anatomical variations or generative models, which can suffer from\ntraining instabilities and hallucinations. To overcome these limitations, we\nuse latent diffusion models to generate deformation fields, which transform a\ngeneral population atlas into one representing a specific sub-population. By\ngenerating a deformation field and registering the conditional atlas to a\nneighbourhood of images, we ensure structural plausibility and avoid\nhallucinations, which can occur during direct image synthesis. We compare our\nmethod to several state-of-the-art atlas generation methods in experiments\nusing 5000 brain as well as whole-body MR images from UK Biobank. Our method\ngenerates highly realistic atlases with smooth transformations and high\nanatomical fidelity, outperforming the baselines.\n","authors":["Sophie Starck","Vasiliki Sideri-Lampretsa","Bernhard Kainz","Martin Menten","Tamara Mueller","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.16776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16771v1","updated":"2024-03-25T13:50:11Z","published":"2024-03-25T13:50:11Z","title":"Synthetic Data Generation and Joint Learning for Robust Code-Mixed\n  Translation","summary":"  The widespread online communication in a modern multilingual world has\nprovided opportunities to blend more than one language (aka code-mixed\nlanguage) in a single utterance. This has resulted a formidable challenge for\nthe computational models due to the scarcity of annotated data and presence of\nnoise. A potential solution to mitigate the data scarcity problem in\nlow-resource setup is to leverage existing data in resource-rich language\nthrough translation. In this paper, we tackle the problem of code-mixed\n(Hinglish and Bengalish) to English machine translation. First, we\nsynthetically develop HINMIX, a parallel corpus of Hinglish to English, with\n~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation\nbased joint-training model that learns to handle noise in the real-world\ncode-mixed text by parameter sharing across clean and noisy words. Further, we\nshow the adaptability of RCMT in a zero-shot setup for Bengalish to English\ntranslation. Our evaluation and comprehensive analyses qualitatively and\nquantitatively demonstrate the superiority of RCMT over state-of-the-art\ncode-mixed and robust translation methods.\n","authors":[" Kartik","Sanjana Soni","Anoop Kunchukuttan","Tanmoy Chakraborty","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2403.16771v1.pdf","comment":"9 pages, 2 figures, to be published in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16768v1","updated":"2024-03-25T13:46:09Z","published":"2024-03-25T13:46:09Z","title":"DeepKnowledge: Generalisation-Driven Deep Learning Testing","summary":"  Despite their unprecedented success, DNNs are notoriously fragile to small\nshifts in data distribution, demanding effective testing techniques that can\nassess their dependability. Despite recent advances in DNN testing, there is a\nlack of systematic testing approaches that assess the DNN's capability to\ngeneralise and operate comparably beyond data in their training distribution.\nWe address this gap with DeepKnowledge, a systematic testing methodology for\nDNN-based systems founded on the theory of knowledge generalisation, which aims\nto enhance DNN robustness and reduce the residual risk of 'black box' models.\nConforming to this theory, DeepKnowledge posits that core computational DNN\nunits, termed Transfer Knowledge neurons, can generalise under domain shift.\nDeepKnowledge provides an objective confidence measurement on testing\nactivities of DNN given data distribution shifts and uses this information to\ninstrument a generalisation-informed test adequacy criterion to check the\ntransfer knowledge capacity of a test set. Our empirical evaluation of several\nDNNs, across multiple datasets and state-of-the-art adversarial generation\ntechniques demonstrates the usefulness and effectiveness of DeepKnowledge and\nits ability to support the engineering of more dependable DNNs. We report\nimprovements of up to 10 percentage points over state-of-the-art coverage\ncriteria for detecting adversarial attacks on several benchmarks, including\nMNIST, SVHN, and CIFAR.\n","authors":["Sondess Missaoui","Simos Gerasimou","Nikolaos Matragkas"],"pdf_url":"https://arxiv.org/pdf/2403.16768v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2310.13895v2","updated":"2024-03-25T13:41:32Z","published":"2023-10-21T02:46:03Z","title":"RTSUM: Relation Triple-based Interpretable Summarization with\n  Multi-level Salience Visualization","summary":"  In this paper, we present RTSUM, an unsupervised summarization framework that\nutilizes relation triples as the basic unit for summarization. Given an input\ndocument, RTSUM first selects salient relation triples via multi-level salience\nscoring and then generates a concise summary from the selected relation triples\nby using a text-to-text language model. On the basis of RTSUM, we also develop\na web demo for an interpretable summarizing tool, providing fine-grained\ninterpretations with the output summary. With support for customization\noptions, our tool visualizes the salience for textual units at three distinct\nlevels: sentences, relation triples, and phrases. The codes,are publicly\navailable.\n","authors":["Seonglae Cho","Yonggi Cho","HoonJae Lee","Myungha Jang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2310.13895v2.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.11722v2","updated":"2024-03-25T13:34:40Z","published":"2024-03-18T12:22:11Z","title":"Time Series Compression using Quaternion Valued Neural Networks and\n  Quaternion Backpropagation","summary":"  We propose a novel quaternionic time-series compression methodology where we\ndivide a long time-series into segments of data, extract the min, max, mean and\nstandard deviation of these chunks as representative features and encapsulate\nthem in a quaternion, yielding a quaternion valued time-series. This\ntime-series is processed using quaternion valued neural network layers, where\nwe aim to preserve the relation between these features through the usage of the\nHamilton product. To train this quaternion neural network, we derive quaternion\nbackpropagation employing the GHR calculus, which is required for a valid\nproduct and chain rule in quaternion space. Furthermore, we investigate the\nconnection between the derived update rules and automatic differentiation. We\napply our proposed compression method on the Tennessee Eastman Dataset, where\nwe perform fault classification using the compressed data in two settings: a\nfully supervised one and in a semi supervised, contrastive learning setting.\nBoth times, we were able to outperform real valued counterparts as well as two\nbaseline models: one with the uncompressed time-series as the input and the\nother with a regular downsampling using the mean. Further, we could improve the\nclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.\n","authors":["Johannes Pöppelbaum","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2403.11722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04570v3","updated":"2024-03-25T13:31:33Z","published":"2023-07-10T14:02:31Z","title":"A Call to Reflect on Evaluation Practices for Age Estimation:\n  Comparative Analysis of the State-of-the-Art and a Unified Benchmark","summary":"  Comparing different age estimation methods poses a challenge due to the\nunreliability of published results stemming from inconsistencies in the\nbenchmarking process. Previous studies have reported continuous performance\nimprovements over the past decade using specialized methods; however, our\nfindings challenge these claims. This paper identifies two trivial, yet\npersistent issues with the currently used evaluation protocol and describes how\nto resolve them. We offer an extensive comparative analysis for\nstate-of-the-art facial age estimation methods. Surprisingly, we find that the\nperformance differences between the methods are negligible compared to the\neffect of other factors, such as facial alignment, facial coverage, image\nresolution, model architecture, or the amount of data used for pretraining. We\nuse the gained insights to propose using FaRL as the backbone model and\ndemonstrate its effectiveness on all public datasets. We make the source code\nand exact data splits public on GitHub.\n","authors":["Jakub Paplham","Vojtech Franc"],"pdf_url":"https://arxiv.org/pdf/2307.04570v3.pdf","comment":"CVPR 2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2202.00992v3","updated":"2024-03-25T13:31:05Z","published":"2022-02-02T12:24:03Z","title":"Tight Convergence Rate Bounds for Optimization Under Power Law Spectral\n  Conditions","summary":"  Performance of optimization on quadratic problems sensitively depends on the\nlow-lying part of the spectrum. For large (effectively infinite-dimensional)\nproblems, this part of the spectrum can often be naturally represented or\napproximated by power law distributions, resulting in power law convergence\nrates for iterative solutions of these problems by gradient-based algorithms.\nIn this paper, we propose a new spectral condition providing tighter upper\nbounds for problems with power law optimization trajectories. We use this\ncondition to build a complete picture of upper and lower bounds for a wide\nrange of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy\nBall, and Conjugate Gradients -- with an emphasis on the underlying schedules\nof learning rate and momentum. In particular, we demonstrate how an optimally\naccelerated method, its schedule, and convergence upper bound can be obtained\nin a unified manner for a given shape of the spectrum. Also, we provide first\nproofs of tight lower bounds for convergence rates of Steepest Descent and\nConjugate Gradients under spectral power laws with general exponents. Our\nexperiments show that the obtained convergence bounds and acceleration\nstrategies are not only relevant for exactly quadratic optimization problems,\nbut also fairly accurate when applied to the training of neural networks.\n","authors":["Maksim Velikanov","Dmitry Yarotsky"],"pdf_url":"https://arxiv.org/pdf/2202.00992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04599v2","updated":"2024-03-25T13:30:37Z","published":"2024-02-07T05:47:31Z","title":"Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via\n  Temporal-Viewpoint Alignment","summary":"  Video sequences exhibit significant nuisance variations (undesired effects)\nof speed of actions, temporal locations, and subjects' poses, leading to\ntemporal-viewpoint misalignment when comparing two sets of frames or evaluating\nthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmera\nviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D\nskeleton sequences whose camera and subjects' poses can be easily manipulated\nin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where\nmatching well temporal blocks (temporal chunks that make up a sequence) of\nsupport-query sequence pairs (by factoring out nuisance variations) is\nessential due to limited samples of novel classes. Given a query sequence, we\ncreate its several views by simulating several camera locations. For a support\nsequence, we match it with view-simulated query sequences, as in the popular\nDynamic Time Warping (DTW). Specifically, each support temporal block can be\nmatched to the query temporal block with the same or adjacent (next) temporal\nindex, and adjacent camera views to achieve joint local temporal-viewpoint\nwarping. JEANIE selects the smallest distance among matching paths with\ndifferent temporal-viewpoint warping patterns, an advantage over DTW which only\nperforms temporal alignment. We also propose an unsupervised FSAR akin to\nclustering of sequences with JEANIE as a distance measure. JEANIE achieves\nstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D\nMultiview Activity II on supervised and unsupervised FSAR, and their\nmeta-learning inspired fusion.\n","authors":["Lei Wang","Jun Liu","Liang Zheng","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2402.04599v2.pdf","comment":"Accepted by the International Journal of Computer Vision (IJCV). An\n  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was\n  distinguished by the Sang Uk Lee Best Student Paper Award"},{"id":"http://arxiv.org/abs/2402.02423v2","updated":"2024-03-25T13:20:46Z","published":"2024-02-04T09:40:22Z","title":"Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement\n  Learning with Diverse Human Feedback","summary":"  Reinforcement Learning with Human Feedback (RLHF) has received significant\nattention for performing tasks without the need for costly manual reward design\nby aligning human preferences. It is crucial to consider diverse human feedback\ntypes and various learning methods in different environments. However,\nquantifying progress in RLHF with diverse feedback is challenging due to the\nlack of standardized annotation platforms and widely used unified benchmarks.\nTo bridge this gap, we introduce Uni-RLHF, a comprehensive system\nimplementation tailored for RLHF. It aims to provide a complete workflow from\nreal human feedback, fostering progress in the development of practical\nproblems. Uni-RLHF contains three packages: 1) a universal multi-feedback\nannotation platform, 2) large-scale crowdsourced feedback datasets, and 3)\nmodular offline RLHF baseline implementations. Uni-RLHF develops a\nuser-friendly annotation interface tailored to various feedback types,\ncompatible with a wide range of mainstream RL environments. We then establish a\nsystematic pipeline of crowdsourced annotations, resulting in large-scale\nannotated datasets comprising more than 15 million steps across 30+ popular\ntasks. Through extensive experiments, the results in the collected datasets\ndemonstrate competitive performance compared to those from well-designed manual\nrewards. We evaluate various design choices and offer insights into their\nstrengths and potential areas of improvement. We wish to build valuable\nopen-source platforms, datasets, and baselines to facilitate the development of\nmore robust and reliable RLHF solutions based on realistic human feedback. The\nwebsite is available at https://uni-rlhf.github.io/.\n","authors":["Yifu Yuan","Jianye Hao","Yi Ma","Zibin Dong","Hebin Liang","Jinyi Liu","Zhixin Feng","Kai Zhao","Yan Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.02423v2.pdf","comment":"Published as a conference paper at ICLR 2024. The website is\n  available at https://uni-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2302.08347v3","updated":"2024-03-25T13:18:39Z","published":"2023-02-16T15:05:37Z","title":"The autoregressive neural network architecture of the Boltzmann\n  distribution of pairwise interacting spins systems","summary":"  Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated\nexceptional results in image and language generation tasks, contributing to the\ngrowing popularity of generative models in both scientific and commercial\napplications. This work presents an exact mapping of the Boltzmann distribution\nof binary pairwise interacting systems into autoregressive form. The resulting\nARNN architecture has weights and biases of its first layer corresponding to\nthe Hamiltonian's couplings and external fields, featuring widely used\nstructures such as the residual connections and a recurrent architecture with\nclear physical meanings. Moreover, its architecture's explicit formulation\nenables the use of statistical physics techniques to derive new ARNNs for\nspecific systems. As examples, new effective ARNN architectures are derived\nfrom two well-known mean-field systems, the Curie-Weiss and\nSherrington-Kirkpatrick models, showing superior performance in approximating\nthe Boltzmann distributions of the corresponding physics model compared to\nother commonly used architectures. The connection established between the\nphysics of the system and the neural network architecture provides a means to\nderive new architectures for different interacting systems and interpret\nexisting ones from a physical perspective.\n","authors":["Indaco Biazzo"],"pdf_url":"https://arxiv.org/pdf/2302.08347v3.pdf","comment":"20 pages, 10 figure plus the Supplementary Information"},{"id":"http://arxiv.org/abs/2306.10180v3","updated":"2024-03-25T13:02:27Z","published":"2023-06-16T21:20:49Z","title":"Samplet basis pursuit: Multiresolution scattered data approximation with\n  sparsity constraints","summary":"  We consider scattered data approximation in samplet coordinates with\n$\\ell_1$-regularization. The application of an $\\ell_1$-regularization term\nenforces sparsity of the coefficients with respect to the samplet basis.\nSamplets are wavelet-type signed measures, which are tailored to scattered\ndata. They provide similar properties as wavelets in terms of localization,\nmultiresolution analysis, and data compression. By using the Riesz isometry, we\nembed samplets into reproducing kernel Hilbert spaces and discuss the\nproperties of the resulting functions. We argue that the class of signals that\nare sparse with respect to the embedded samplet basis is considerably larger\nthan the class of signals that are sparse with respect to the basis of kernel\ntranslates. Vice versa, every signal that is a linear combination of only a few\nkernel translates is sparse in samplet coordinates. Therefore, samplets enable\nthe use of well-established multiresolution techniques on general scattered\ndata sets.\n  We propose the rapid solution of the problem under consideration by combining\nsoft-shrinkage with the semi-smooth Newton method. Leveraging on the sparse\nrepresentation of kernel matrices in samplet coordinates, this approach\nconverges faster than the fast iterative shrinkage thresholding algorithm and\nis feasible for large-scale data. Numerical benchmarks are presented and\ndemonstrate the superiority of the multiresolution approach over the\nsingle-scale approach. As large-scale applications, the surface reconstruction\nfrom scattered data and the reconstruction of scattered temperature data using\na dictionary of multiple kernels are considered.\n","authors":["Davide Baroli","Helmut Harbrecht","Michael Multerer"],"pdf_url":"https://arxiv.org/pdf/2306.10180v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07310v2","updated":"2024-03-25T12:58:45Z","published":"2024-02-11T21:16:42Z","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis","summary":"  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n","authors":["Leandro A. Passos","Douglas Rodrigues","Danilo Jodas","Kelton A. P. Costa","Ahsan Adeel","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2402.07310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17919v3","updated":"2024-03-25T12:52:42Z","published":"2024-01-31T15:33:37Z","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","summary":"  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n","authors":["Florian Le Bronnec","Song Duong","Mathieu Ravaut","Alexandre Allauzen","Nancy F. Chen","Vincent Guigue","Alberto Lumbreras","Laure Soulier","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2401.17919v3.pdf","comment":"9 pages, 5 figures, 7 tables, EACL 2024 conference"},{"id":"http://arxiv.org/abs/2403.16707v1","updated":"2024-03-25T12:44:52Z","published":"2024-03-25T12:44:52Z","title":"One-Shot Domain Incremental Learning","summary":"  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n","authors":["Yasushi Esaki","Satoshi Koide","Takuro Kutsuna"],"pdf_url":"https://arxiv.org/pdf/2403.16707v1.pdf","comment":"accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2024"},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2112.15400v4","updated":"2024-03-25T12:22:53Z","published":"2021-12-31T11:56:40Z","title":"A Theoretical Understanding of Gradient Bias in Meta-Reinforcement\n  Learning","summary":"  Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level\noptimisation procedures wherein the outer-loop meta-learner guides the\ninner-loop gradient-based reinforcement learner to achieve fast adaptations. In\nthis paper, we develop a unified framework that describes variations of GMRL\nalgorithms and points out that existing stochastic meta-gradient estimators\nadopted by GMRL are actually \\textbf{biased}. Such meta-gradient bias comes\nfrom two sources: 1) the compositional bias incurred by the two-level problem\nstructure, which has an upper bound of\n$\\mathcal{O}\\big(K\\alpha^{K}\\hat{\\sigma}_{\\text{In}}|\\tau|^{-0.5}\\big)$\n\\emph{w.r.t.} inner-loop update step $K$, learning rate $\\alpha$, estimate\nvariance $\\hat{\\sigma}^{2}_{\\text{In}}$ and sample size $|\\tau|$, and 2) the\nmulti-step Hessian estimation bias $\\hat{\\Delta}_{H}$ due to the use of\nautodiff, which has a polynomial impact\n$\\mathcal{O}\\big((K-1)(\\hat{\\Delta}_{H})^{K-1}\\big)$ on the meta-gradient bias.\nWe study tabular MDPs empirically and offer quantitative evidence that\ntestifies our theoretical findings on existing stochastic meta-gradient\nestimators. Furthermore, we conduct experiments on Iterated Prisoner's Dilemma\nand Atari games to show how other methods such as off-policy learning and\nlow-bias estimator can help fix the gradient bias for GMRL algorithms in\ngeneral.\n","authors":["Xidong Feng","Bo Liu","Jie Ren","Luo Mai","Rui Zhu","Haifeng Zhang","Jun Wang","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2112.15400v4.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.17245v4","updated":"2024-03-25T12:20:02Z","published":"2023-03-30T09:22:17Z","title":"Investigating and Mitigating the Side Effects of Noisy Views for\n  Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios","summary":"  Multi-view clustering (MVC) aims at exploring category structures among\nmulti-view data in self-supervised manners. Multiple views provide more\ninformation than single views and thus existing MVC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical multi-view scenarios. In this paper, we\nformally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MVC method (namely MVCAN) to address this issue.\nSpecifically, we propose a novel MVC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a two-level multi-view\niterative optimization is designed to generate robust learning targets for\nrefining individual views' representation learning. Theoretical analysis\nreveals that MVCAN works by achieving the multi-view consistency,\ncomplementarity, and noise robustness. Finally, experiments on extensive public\ndatasets demonstrate that MVCAN outperforms state-of-the-art methods and is\nrobust against the existence of noisy views.\n","authors":["Jie Xu","Yazhou Ren","Xiaolong Wang","Lei Feng","Zheng Zhang","Gang Niu","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17245v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16681v1","updated":"2024-03-25T12:15:55Z","published":"2024-03-25T12:15:55Z","title":"A note on generalization bounds for losses with finite moments","summary":"  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n","authors":["Borja Rodríguez-Gálvez","Omar Rivasplata","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2403.16681v1.pdf","comment":"9 pages: 5 of main text, 1 of references, and 3 of appendices"},{"id":"http://arxiv.org/abs/2403.16680v1","updated":"2024-03-25T12:15:47Z","published":"2024-03-25T12:15:47Z","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","summary":"  Learning physical simulations has been an essential and central aspect of\nmany recent research efforts in machine learning, particularly for\nNavier-Stokes-based fluid mechanics. Classic numerical solvers have\ntraditionally been computationally expensive and challenging to use in inverse\nproblems, whereas Neural solvers aim to address both concerns through machine\nlearning. We propose a general formulation for continuous convolutions using\nseparable basis functions as a superset of existing methods and evaluate a\nlarge set of basis functions in the context of (a) a compressible 1D SPH\nsimulation, (b) a weakly compressible 2D SPH simulation, and (c) an\nincompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\nincluded in the basis functions are key aspects of stability and accuracy. Our\nbroad evaluation shows that Fourier-based continuous convolutions outperform\nall other architectures regarding accuracy and generalization. Finally, using\nthese Fourier-based networks, we show that prior inductive biases, such as\nwindow functions, are no longer necessary. An implementation of our approach,\nas well as complete datasets and solver implementations, is available at\nhttps://github.com/tum-pbs/SFBC.\n","authors":["Rene Winchenbach","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2403.16680v1.pdf","comment":"Published at International Conference on Learning Representation\n  (ICLR) 2024, 54 pages, 39 figures"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17040v1","updated":"2024-03-25T12:15:10Z","published":"2024-03-25T12:15:10Z","title":"Enhancing Graph Representation Learning with Attention-Driven Spiking\n  Neural Networks","summary":"  Graph representation learning has become a crucial task in machine learning\nand data mining due to its potential for modeling complex structures such as\nsocial networks, chemical compounds, and biological systems. Spiking neural\nnetworks (SNNs) have recently emerged as a promising alternative to traditional\nneural networks for graph learning tasks, benefiting from their ability to\nefficiently encode and process temporal and spatial information. In this paper,\nwe propose a novel approach that integrates attention mechanisms with SNNs to\nimprove graph representation learning. Specifically, we introduce an attention\nmechanism for SNN that can selectively focus on important nodes and\ncorresponding features in a graph during the learning process. We evaluate our\nproposed method on several benchmark datasets and show that it achieves\ncomparable performance compared to existing graph learning techniques.\n","authors":["Huifeng Yin","Mingkun Xu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.17040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16674v1","updated":"2024-03-25T12:13:20Z","published":"2024-03-25T12:13:20Z","title":"Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks","summary":"  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n","authors":["Huifeng Yin","Hanle Zheng","Jiayi Mao","Siyuan Ding","Xing Liu","Mingkun Xu","Yifan Hu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.16674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14587v2","updated":"2024-03-25T12:00:19Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12581v2","updated":"2024-03-25T11:58:04Z","published":"2023-08-24T05:49:58Z","title":"A Huber Loss Minimization Approach to Byzantine Robust Federated\n  Learning","summary":"  Federated learning systems are susceptible to adversarial attacks. To combat\nthis, we introduce a novel aggregator based on Huber loss minimization, and\nprovide a comprehensive theoretical analysis. Under independent and identically\ndistributed (i.i.d) assumption, our approach has several advantages compared to\nexisting methods. Firstly, it has optimal dependence on $\\epsilon$, which\nstands for the ratio of attacked clients. Secondly, our approach does not need\nprecise knowledge of $\\epsilon$. Thirdly, it allows different clients to have\nunequal data sizes. We then broaden our analysis to include non-i.i.d data,\nsuch that clients have slightly different distributions.\n","authors":["Puning Zhao","Fei Yu","Zhiguo Wan"],"pdf_url":"https://arxiv.org/pdf/2308.12581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.16654v1","updated":"2024-03-25T11:42:01Z","published":"2024-03-25T11:42:01Z","title":"A Novel Loss Function-based Support Vector Machine for Binary\n  Classification","summary":"  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n","authors":["Yan Li","Liping Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11922v3","updated":"2024-03-25T11:39:57Z","published":"2024-02-19T08:11:26Z","title":"Spatio-Temporal Few-Shot Learning via Diffusive Neural Network\n  Generation","summary":"  Spatio-temporal modeling is foundational for smart city applications, yet it\nis often hindered by data scarcity in many cities and regions. To bridge this\ngap, we propose a novel generative pre-training framework, GPD, for\nspatio-temporal few-shot learning with urban knowledge transfer. Unlike\nconventional approaches that heavily rely on common feature extraction or\nintricate few-shot learning designs, our solution takes a novel approach by\nperforming generative pre-training on a collection of neural network parameters\noptimized with data from source cities. We recast spatio-temporal few-shot\nlearning as pre-training a generative diffusion model, which generates tailored\nneural networks guided by prompts, allowing for adaptability to diverse data\ndistributions and city-specific characteristics. GPD employs a\nTransformer-based denoising diffusion model, which is model-agnostic to\nintegrate with powerful spatio-temporal neural networks. By addressing\nchallenges arising from data gaps and the complexity of generalizing knowledge\nacross cities, our framework consistently outperforms state-of-the-art\nbaselines on multiple real-world datasets for tasks such as traffic speed\nprediction and crowd flow prediction. The implementation of our approach is\navailable: https://github.com/tsinghua-fib-lab/GPD.\n","authors":["Yuan Yuan","Chenyang Shao","Jingtao Ding","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.11922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03472v2","updated":"2024-03-25T10:58:22Z","published":"2023-08-07T11:02:44Z","title":"Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure","summary":"  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n","authors":["Lucas English","Mahdi Abolghasemi"],"pdf_url":"https://arxiv.org/pdf/2308.03472v2.pdf","comment":"41 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/1902.05605v4","updated":"2024-03-25T10:20:18Z","published":"2019-02-14T21:05:50Z","title":"CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater\n  Sample Efficiency and Simplicity","summary":"  Sample efficiency is a crucial problem in deep reinforcement learning. Recent\nalgorithms, such as REDQ and DroQ, found a way to improve the sample efficiency\nby increasing the update-to-data (UTD) ratio to 20 gradient update steps on the\ncritic per environment sample. However, this comes at the expense of a greatly\nincreased computational cost. To reduce this computational burden, we introduce\nCrossQ: A lightweight algorithm for continuous control tasks that makes careful\nuse of Batch Normalization and removes target networks to surpass the current\nstate-of-the-art in sample efficiency while maintaining a low UTD ratio of 1.\nNotably, CrossQ does not rely on advanced bias-reduction schemes used in\ncurrent methods. CrossQ's contributions are threefold: (1) it matches or\nsurpasses current state-of-the-art methods in terms of sample efficiency, (2)\nit substantially reduces the computational cost compared to REDQ and DroQ, (3)\nit is easy to implement, requiring just a few lines of code on top of SAC.\n","authors":["Aditya Bhatt","Daniel Palenicek","Boris Belousov","Max Argus","Artemij Amiranashvili","Thomas Brox","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/1902.05605v4.pdf","comment":"Published at ICLR 2024. Project page at\n  http://aditya.bhatts.org/CrossQ and code release at\n  https://github.com/adityab/CrossQ"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v1","updated":"2024-03-25T10:06:45Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.16576v1","updated":"2024-03-25T09:41:49Z","published":"2024-03-25T09:41:49Z","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference\n  Optimization","summary":"  Antibody design, a crucial task with significant implications across various\ndisciplines such as therapeutics and biology, presents considerable challenges\ndue to its intricate nature. In this paper, we tackle antigen-specific antibody\ndesign as a protein sequence-structure co-design problem, considering both\nrationality and functionality. Leveraging a pre-trained conditional diffusion\nmodel that jointly models sequences and structures of\ncomplementarity-determining regions (CDR) in antibodies with equivariant neural\nnetworks, we propose direct energy-based preference optimization to guide the\ngeneration of antibodies with both rational structures and considerable binding\naffinities to given antigens. Our method involves fine-tuning the pre-trained\ndiffusion model using a residue-level decomposed energy preference.\nAdditionally, we employ gradient surgery to address conflicts between various\ntypes of energy, such as attraction and repulsion. Experiments on RAbD\nbenchmark show that our approach effectively optimizes the energy of generated\nantibodies and achieves state-of-the-art performance in designing high-quality\nantibodies with low total energy and high binding affinity, demonstrating the\nsuperiority of our approach.\n","authors":["Xiangxin Zhou","Dongyu Xue","Ruizhe Chen","Zaixiang Zheng","Liang Wang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16561v1","updated":"2024-03-25T09:24:05Z","published":"2024-03-25T09:24:05Z","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","summary":"  Federated Learning (FL) heavily depends on label quality for its performance.\nHowever, the label distribution among individual clients is always both noisy\nand heterogeneous. The high loss incurred by client-specific samples in\nheterogeneous label noise poses challenges for distinguishing between\nclient-specific and noisy label samples, impacting the effectiveness of\nexisting label noise learning approaches. To tackle this issue, we propose\nFedFixer, where the personalized model is introduced to cooperate with the\nglobal model to effectively select clean client-specific samples. In the dual\nmodels, updating the personalized model solely at a local level can lead to\noverfitting on noisy data due to limited samples, consequently affecting both\nthe local and global models' performance. To mitigate overfitting, we address\nthis concern from two perspectives. Firstly, we employ a confidence regularizer\nto alleviate the impact of unconfident predictions caused by label noise.\nSecondly, a distance regularizer is implemented to constrain the disparity\nbetween the personalized and global models. We validate the effectiveness of\nFedFixer through extensive experiments on benchmark datasets. The results\ndemonstrate that FedFixer can perform well in filtering noisy label samples on\ndifferent clients, especially in highly heterogeneous label noise scenarios.\n","authors":["Xinyuan Ji","Zhaowei Zhu","Wei Xi","Olga Gadyatskaya","Zilong Song","Yong Cai","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16561v1.pdf","comment":"accepted by AAA24"},{"id":"http://arxiv.org/abs/2403.16557v1","updated":"2024-03-25T09:16:59Z","published":"2024-03-25T09:16:59Z","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local\n  Gradients","summary":"  Federated Learning (FL) is a distributed machine learning framework in\ncommunication network systems. However, the systems' Non-Independent and\nIdentically Distributed (Non-IID) data negatively affect the convergence\nefficiency of the global model, since only a subset of these data samples are\nbeneficial for model convergence. In pursuit of this subset, a reliable\napproach involves determining a measure of validity to rank the samples within\nthe dataset. In this paper, We propose the BHerd strategy which selects a\nbeneficial herd of local gradients to accelerate the convergence of the FL\nmodel. Specifically, we map the distribution of the local dataset to the local\ngradients and use the Herding strategy to obtain a permutation of the set of\ngradients, where the more advanced gradients in the permutation are closer to\nthe average of the set of gradients. These top portion of the gradients will be\nselected and sent to the server for global aggregation. We conduct experiments\non different datasets, models and scenarios by building a prototype system, and\nexperimental results demonstrate that our BHerd strategy is effective in\nselecting beneficial local gradients to mitigate the effects brought by the\nNon-IID dataset, thus accelerating model convergence.\n","authors":["Ping Luo","Xiaoge Deng","Ziqing Wen","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14714v4","updated":"2024-03-25T08:58:39Z","published":"2023-10-23T08:51:05Z","title":"BatteryML:An Open-source platform for Machine Learning on Battery\n  Degradation","summary":"  Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps://github.com/microsoft/BatteryML.\n","authors":["Han Zhang","Xiaofan Gui","Shun Zheng","Ziheng Lu","Yuqi Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.14714v4.pdf","comment":null}]}}