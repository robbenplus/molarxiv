<!DOCTYPE html>
<html lang="en">

<head>
    <title>DrugAI MolArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="drugAI.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
                <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div style="display: inline-block;">


            <a href="https://biohai.github.io" style="text-decoration: none;">
                <div class="header-title">
                    <span class="header-title-preffix">DrugAI MolArxiv
                </div>
            </a>
          </div>
            <div class=icons style="display: inline-block;">
                <label class="theme-switch" for="checkbox">
                    <input type="checkbox" id="checkbox"/>
                    <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
                </label>
            </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear model reduction for operator learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a Tiny Paper at ICLR 2024 (Notable)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Traffic Flow Prediction using Cellular Automata-based
  Model and CNN-LSTM architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper supersedes arXiv:2310.13433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InceptionTime vs. Wavelet -- A comparison for time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Klenkert, Daniel Schaeffer, Julian Stauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representatividad Muestral en la Incertidumbre Simétrica Multivariada
  para la Selección de Atributos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Sosa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers-based architectures for stroke segmentation: A review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion approaches for emotion recognition from speech using acoustic and
  text-based features <span class="chip">ICASSP 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted in ICASSP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Rucci, Gonzalo Tittarelli, Franco Ronchetti, Jorge F. Elgart, Laura Lanzarini, Juan José Gagliardino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Science - CACIC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One flow to correct them all: improving simulations in high-energy
  physics with a single normalising flow and a switch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Hyperparameters for Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinGen: Generating Fidelitous and Diverse Graph Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gesine Reinert, Wenkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-Robust Keyword Spotting through Self-supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skscope: Fast Sparsity-Constrained Optimization in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, Yu Zheng, Junxian Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content prediction from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in PINNs: Phase transition, total diffusion, and generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRESCO: Federated Reinforcement Energy System for Cooperative
  Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tiny Paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Policy Learning for Smart Grids: FL TRPO Approach <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Vegetation Modeling with Pre-Trained Weather Transformers <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Active Learning in Conditional Trust Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Topos of Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Jacopo Villani, Peter McBurney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Multiple Kernel Learning approaches for multi-omics data
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dataset for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse <span class="highlight-title">Drug</span> Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithms: A New Frontier in Financial Crime Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Itzhak Weinberg, Alessio Faccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Workshop Paper at TS4H@ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Change Sign Detection by Fusing Mixture Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Urano, Ryo Yuki, Kenji Yamanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Image Transformers for Prostate Cancer Detection from
  Ultrasound Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early draft, 7 pages; Accepted to SPIE Medical Imaging 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Fair Classification with Bounded Demographic Disparity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianli Zeng, Guang Cheng, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual
  Pretraining and Multi-level Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Huo, Yikai Wang, Xuelin Qian, Yun Wang, Chong Li, Jianfeng Feng, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent fMRI-to-image approaches mainly focused on associating fMRI signals
with specific conditions of pre-trained diffusion models. These approaches,
while producing high-quality images, capture only a limited aspect of the
complex information in fMRI signals and offer little detailed control over
image creation. In contrast, this paper proposes to directly modulate the
generation process of diffusion models using fMRI signals. Our approach,
NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI
calibrated-encoding, to tackle multi-individual pre-training for a shared
latent space to minimize individual difference and enable the subsequent
cross-subject training; ii) fMRI-to-image cross-subject pre-training,
perceptually learning to guide diffusion model with high- and low-level
conditions across different individuals; iii) fMRI-to-image single-subject
refining, similar with step ii but focus on adapting to particular individual.
NeuroPictor extracts high-level semantic features from fMRI signals that
characterizing the visual stimulus and incrementally fine-tunes the diffusion
model with a low-level manipulation network to provide precise structural
instructions. By training with over 60,000 fMRI-image pairs from various
individuals, our model enjoys superior fMRI-to-image decoding capacity,
particularly in the within-subject setting, as evidenced in benchmark datasets.
Project page: https://jingyanghuo.github.io/neuropictor/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe Reinforcement Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV CVAMD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Zhou, Bin Liu, Jin Wang, Grigorios Tsoumakas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network models have demonstrated their effectiveness in
classifying multi-label data from various domains. Typically, they employ a
training mode that combines mini-batches with optimizers, where each sample is
randomly selected with equal probability when constructing mini-batches.
However, the intrinsic class imbalance in multi-label data may bias the model
towards majority labels, since samples relevant to minority labels may be
underrepresented in each mini-batch. Meanwhile, during the training process, we
observe that instances associated with minority labels tend to induce greater
losses. Existing heuristic batch selection methods, such as priority selection
of samples with high contribution to the objective function, i.e., samples with
high loss, have been proven to accelerate convergence while reducing the loss
and test error in single-label data. However, batch selection methods have not
yet been applied and validated in multi-label data. In this study, we introduce
a simple yet effective adaptive batch selection algorithm tailored to
multi-label deep learning models. It adaptively selects each batch by
prioritizing hard samples related to minority labels. A variant of our method
also takes informative label correlations into consideration. Comprehensive
experiments combining five multi-label deep learning models on thirteen
benchmark datasets show that our method converges faster and performs better
than random batch selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression of the Koopman matrix for nonlinear physical models via
  hierarchical clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Nishikata, Jun Ohkubo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods allow the prediction of nonlinear dynamical systems
from data alone. The Koopman operator is one of them, which enables us to
employ linear analysis for nonlinear dynamical systems. The linear
characteristics of the Koopman operator are hopeful to understand the nonlinear
dynamics and perform rapid predictions. The extended dynamic mode decomposition
(EDMD) is one of the methods to approximate the Koopman operator as a
finite-dimensional matrix. In this work, we propose a method to compress the
Koopman matrix using hierarchical clustering. Numerical demonstrations for the
cart-pole model and comparisons with the conventional singular value
decomposition (SVD) are shown; the results indicate that the hierarchical
clustering performs better than the naive SVD compressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mistake, Manipulation and Margin Guarantees in Online Strategic
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingqing Shen, Nam Ho-Nguyen, Khanh-Hung Giang-Tran, Fatma Kılınç-Karzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an online strategic classification problem where each arriving
agent can manipulate their true feature vector to obtain a positive predicted
label, while incurring a cost that depends on the amount of manipulation. The
learner seeks to predict the agent's true label given access to only the
manipulated features. After the learner releases their prediction, the agent's
true label is revealed. Previous algorithms such as the strategic perceptron
guarantee finitely many mistakes under a margin assumption on agents' true
feature vectors. However, these are not guaranteed to encourage agents to be
truthful. Promoting truthfulness is intimately linked to obtaining adequate
margin on the predictions, thus we provide two new algorithms aimed at
recovering the maximum margin classifier in the presence of strategic agent
behavior. We prove convergence, finite mistake and finite manipulation
guarantees for a variety of agent cost structures. We also provide generalized
versions of the strategic perceptron with mistake guarantees for different
costs. Our numerical study on real and synthetic data demonstrates that the new
algorithms outperform previous ones in terms of margin, number of manipulation
and number of mistakes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks - a popular
class of neural network architectures - have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nesting Particle Filters for Experimental Design in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to Bayesian experimental design
for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential
Monte Carlo technique to infer optimal designs, and embed it into a particle
Markov chain Monte Carlo framework to perform gradient-based policy
amortization. Our approach is distinct from other amortized experimental design
techniques, as it does not rely on contrastive estimators. Numerical validation
on a set of dynamical systems showcases the efficacy of our method in
comparison to other state-of-the-art strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Review of Community Detection in Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a thorough exposition of various
community detection methods from perspectives of modularity-based method,
spectral clustering, probabilistic modelling, and deep learning. Along with the
methods, a new community detection method designed by us is also presented.
Additionally, the performance of these methods on the datasets with and without
ground truth is compared. In conclusion, this comprehensive review provides a
deep understanding of community detection in graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Data Mesh with Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected the dataset, but we show this can
unnecessarily limit policy performance if the behavior policy is far from
optimal. Instead, we forgo constraints and frame OtO RL as an exploration
problem that aims to maximize the benefit of online data-collection. We first
study the major online RL exploration methods based on intrinsic rewards and
UCB in the OtO setting, showing that intrinsic rewards add training instability
through reward-function modification, and UCB methods are myopic and it is
unclear which learned-component's ensemble to use for action selection. We then
introduce an algorithm for planning to go out-of-distribution (PTGOOD) that
avoids these issues. PTGOOD uses a non-myopic planning procedure that targets
exploration in relatively high-reward regions of the state-action space
unlikely to be visited by the behavior policy. By leveraging concepts from the
Conditional Entropy Bottleneck, PTGOOD encourages data collected online to
provide new information relevant to improving the final deployment policy
without altering rewards. We show empirically in several continuous control
tasks that PTGOOD significantly improves agent returns during online
fine-tuning and avoids the suboptimal policy convergence that many of our
baselines exhibit in several environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. In addition to reviewing state-of-the-art studies, this
paper also identifies key challenges and applications in this field, while also
highlighting promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Journal of Machine Learning and
  Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-embedded Deep Learning Framework for Cloth Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A derivation is incomplete, and updations are being processed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of sample complexity of online reinforcement learning is often
studied in the literature without taking into account any partial knowledge
about the system dynamics that could potentially accelerate the learning
process. In this paper, we study the sample complexity of online Q-learning
methods when some prior knowledge about the dynamics is available or can be
learned efficiently. We focus on systems that evolve according to an additive
disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$
represents the underlying system dynamics, and $W_h$ are unknown disturbances
independent of states and actions. In the setting of finite episodic Markov
decision processes with $S$ states, $A$ actions, and episode length $H$, we
present an optimistic Q-learning algorithm that achieves
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of
$f$, where $T$ is the total number of interactions with the system. This is in
contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret
for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of
$f$ is available, our method can learn an approximately optimal policy in a
number of samples that is independent of the cardinalities of state and action
spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$,
as well as the Lipschitz constant of the corresponding optimal value function.
Our approach does not require modeling of the transition probabilities and
enjoys the same memory complexity as model-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 38th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose Simple Policy Optimization (SPO)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. Extensive experimental results in Atari 2600
environments indicate that, compared to the mainstream variants of PPO, SPO
achieves better sample efficiency, extremely low KL divergence, and higher
policy entropy, and is robust to the increase in network depth or complexity.
More importantly, SPO maintains the simplicity of an unconstrained first-order
algorithm. Code is available at
https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and Mitigating Visual Biases through Keyword Explanation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing biases in computer vision models is crucial for real-world AI
deployments. However, mitigating visual biases is challenging due to their
unexplainable nature, often identified indirectly through visualization or
sample statistics, which necessitates additional human supervision for
interpretation. To tackle this issue, we propose the Bias-to-Text (B2T)
framework, which interprets visual biases as keywords. Specifically, we extract
common keywords from the captions of mispredicted images to identify potential
biases in the model. We then validate these keywords by measuring their
similarity to the mispredicted images using a vision-language scoring model.
The keyword explanation form of visual bias offers several advantages, such as
a clear group naming for bias discovery and a natural extension for debiasing
using these group names. Our experiments demonstrate that B2T can identify
known biases, such as gender bias in CelebA, background bias in Waterbirds, and
distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in
larger datasets, such as Dollar Street and ImageNet. For example, we discovered
a contextual bias between "bee" and "flower" in ImageNet. We also highlight
various applications of B2T keywords, including debiased training, CLIP
prompting, and model comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale
  Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution Self-supervised ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Cost-Constrained Behaviors in Reinforcement Learning <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans W. A. Hanley, Zakir Durumeric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to disinformation and propaganda from Russian online media
following the invasion of Ukraine, Russian media outlets such as Russia Today
and Sputnik News were banned throughout Europe. To maintain viewership, many of
these Russian outlets began to heavily promote their content on messaging
services like Telegram. In this work, we study how 16 Russian media outlets
interacted with and utilized 732 Telegram channels throughout 2022. Leveraging
the foundational model MPNet, DP-means clustering, and Hawkes processes, we
trace how narratives spread between news sites and Telegram channels. We show
that news outlets not only propagate existing narratives through Telegram but
that they source material from the messaging platform. For example, across the
websites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of
articles discussed content that originated/resulted from activity on Telegram.
Finally, tracking the spread of individual topics, we measure the rate at which
news outlets and Telegram channels disseminate content within the Russian media
ecosystem, finding that websites like ura.news and Telegram channels such as
@genshab are the most effective at disseminating their content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICWSM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning-Driven Approach for Handwritten Chinese Character
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Kriuk, Fedor Kriuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwritten character recognition (HCR) is a challenging problem for machine
learning researchers. Unlike printed text data, handwritten character datasets
have more variation due to human-introduced bias. With numerous unique
character classes present, some data, such as Logographic Scripts or
Sino-Korean character sequences, bring new complications to the HCR problem.
The classification task on such datasets requires the model to learn
high-complexity details of the images that share similar features. With recent
advances in computational resource availability and further computer vision
theory development, some research teams have effectively addressed the arising
challenges. Although known for achieving high accuracy while keeping the number
of parameters small, many common approaches are still not generalizable and use
dataset-specific solutions to achieve better results. Due to complex structure,
existing methods frequently prevent the solutions from gaining popularity. This
paper proposes a highly scalable approach for detailed character image
classification by introducing the model architecture, data preprocessing steps,
and testing design instructions. We also perform experiments to compare the
performance of our method with that of existing ones to show the improvements
achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 9 figures, 2 tables, preprint v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Act without Actions <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schmidt, Minqi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large models on vast amounts of web data has proven to be an
effective approach for obtaining powerful, general models in domains such as
language and vision. However, this paradigm has not yet taken hold in
reinforcement learning. This is because videos, the most abundant form of
embodied behavioral data on the web, lack the action labels required by
existing methods for imitating behavior from demonstrations. We introduce
Latent Action Policies (LAPO), a method for recovering latent action
information, and thereby latent-action policies, world models, and inverse
dynamics models, purely from videos. LAPO is the first method able to recover
the structure of the true action space just from observed dynamics, even in
challenging procedurally-generated environments. LAPO enables training
latent-action policies that can be rapidly fine-tuned into expert-level
policies, either offline using a small action-labeled dataset, or online with
rewards. LAPO takes a first step towards pre-training powerful, generalist
policies and world models on the vast amounts of videos readily available on
the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide, Conquer, Combine Bayesian Decision Tree Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jodie A. Cochrane, Adrian Wills, Sarah J. Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision trees are commonly used predictive models due to their flexibility
and interpretability. This paper is directed at quantifying the uncertainty of
decision tree predictions by employing a Bayesian inference approach. This is
challenging because these approaches need to explore both the tree structure
space and the space of decision parameters associated with each tree structure.
This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a
Markov Chain is constructed to provide samples from the desired Bayesian
estimate. Importantly, the structure and the decision parameters are tightly
coupled; small changes in the tree structure can demand vastly different
decision parameters to provide accurate predictions. A challenge for existing
MCMC approaches is proposing joint changes in both the tree structure and the
decision parameters that result in efficient sampling. This paper takes a
different approach, where each distinct tree structure is associated with a
unique set of decision parameters. The proposed approach, entitled DCC-Tree, is
inspired by the work in Zhou et al. [23] for probabilistic programs and
Cochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for
decision trees. Results show that DCC-Tree performs comparably to other
HMC-based methods and better than existing Bayesian tree methods while
improving on consistency and reducing the per-proposal complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Yang, Jiaming Yang, Wei Hu, Michał Dereziński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced
interpretability and flexibility over traditional designs. Nevertheless, they
still suffer from scalability challenges when it comes to the training cost.
Although many methods have been proposed to address the scalability issues,
they mostly focus on per-iteration efficiency, without worst-case convergence
guarantees. Moreover, those methods typically add components to or modify the
original model, thus possibly breaking the interpretability of Unfolded GNNs.
In this paper, we propose HERTA: a High-Efficiency and Rigorous Training
Algorithm for Unfolded GNNs that accelerates the whole training process,
achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA
converges to the optimum of the original model, thus preserving the
interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we
propose a new spectral sparsification method applicable to normalized and
regularized graph Laplacians that ensures tighter bounds for our algorithm than
existing spectral sparsifiers do. Experiments on real-world datasets verify the
superiority of HERTA as well as its adaptability to various loss functions and
optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AE SemRL: Learning Semantic Association Rules with Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Victoria Degeler, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation of data-free class-incremental learning algorithms by
  simulating future data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Feillet, Adrian Popescu, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HealthGAT: Node Classifications in Electronic Health Records using Graph
  Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahmida Liza Piya, Mehak Gupta, Rahmatollah Beheshti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While electronic health records (EHRs) are widely used across various
applications in healthcare, most applications use the EHRs in their raw
(tabular) format. Relying on raw or simple data pre-processing can greatly
limit the performance or even applicability of downstream tasks using EHRs. To
address this challenge, we present HealthGAT, a novel graph attention network
framework that utilizes a hierarchical approach to generate embeddings from
EHR, surpassing traditional graph-based methods. Our model iteratively refines
the embeddings for medical codes, resulting in improved EHR data analysis. We
also introduce customized EHR-centric auxiliary pre-training tasks to leverage
the rich medical knowledge embedded within the data. This approach provides a
comprehensive analysis of complex medical relationships and offers significant
advancement over standard data representation techniques. HealthGAT has
demonstrated its effectiveness in various healthcare scenarios through
comprehensive evaluations against established methodologies. Specifically, our
model shows outstanding performance in node classification and downstream tasks
such as predicting readmissions and diagnosis classifications.
  Our code is available at https://github.com/healthylaife/HealthGAT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Correction of Pseudo Log-Likelihood Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Feng, Nuoya Xiong, Zhijie Zhang, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method
used in various fields including contextual bandits, influence maximization of
social networks, and causal bandits. However, in previous literature
\citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function
may not be bounded, which may result in the algorithm they proposed not
well-defined. In this paper, we give a counterexample that the maximum pseudo
log-likelihood estimation fails and then provide a solution to correct the
algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematical Foundation and Corrections for Full Range Head Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huei-Chung Hu, Xuyang Wu, Yuan Wang, Yi Fang, Hsin-Tai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous works concerning head pose estimation (HPE) offer algorithms or
proposed neural network-based approaches for extracting Euler angles from
either facial key points or directly from images of the head region. However,
many works failed to provide clear definitions of the coordinate systems and
Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation
matrices depend on coordinate systems, and yaw, roll, and pitch angles are
sensitive to their application order. Without precise definitions, it becomes
challenging to validate the correctness of the output head pose and drawing
routines employed in prior works. In this paper, we thoroughly examined the
Euler angles defined in the 300W-LP dataset, head pose estimation such as
3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of
the Euler angles. When necessary, we infer their coordinate system and sequence
of yaw, roll, pitch from provided code. This paper presents (1) code and
algorithms for inferring coordinate system from provided source code, code for
Euler angle application order and extracting precise rotation matrices and the
Euler angles, (2) code and algorithms for converting poses from one rotation
system to another, (3) novel formulae for 2D augmentations of the rotation
matrices, and (4) derivations and code for the correct drawing routines for
rotation matrices and poses. This paper also addresses the feasibility of
defining rotations with right-handed coordinate system in Wikipedia and SciPy,
which makes the Euler angle extraction much easier for full-range head pose
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tutorial on Diffusion Models for Imaging and Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanley H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The astonishing growth of generative tools in recent years has empowered many
exciting applications in text-to-image generation and text-to-video generation.
The underlying principle behind these generative tools is the concept of
diffusion, a particular sampling mechanism that has overcome some shortcomings
that were deemed difficult in the previous approaches. The goal of this
tutorial is to discuss the essential ideas underlying the diffusion models. The
target audience of this tutorial includes undergraduate and graduate students
who are interested in doing research on diffusion models or applying these
models to solve other problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainable Clustering: A Constrained Declarative based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Guilbert, Christel Vrain, Thi-Bich-Hanh Dao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The domain of explainable AI is of interest in all Machine Learning fields,
and it is all the more important in clustering, an unsupervised task whose
result must be validated by a domain expert. We aim at finding a clustering
that has high quality in terms of classic clustering criteria and that is
explainable, and we argue that these two dimensions must be considered when
building the clustering. We consider that a good global explanation of a
clustering should give the characteristics of each cluster taking into account
their abilities to describe its objects (coverage) while distinguishing it from
the other clusters (discrimination). Furthermore, we aim at leveraging expert
knowledge, at different levels, on the structure of the expected clustering or
on its explanations. In our framework an explanation of a cluster is a set of
patterns, and we propose a novel interpretable constrained clustering method
called ECS for declarative clustering with Explainabilty-driven Cluster
Selection that integrates structural or domain expert knowledge expressed by
means of constraints. It is based on the notion of coverage and discrimination
that are formalized at different levels (cluster / clustering), each allowing
for exceptions through parameterized thresholds. Our method relies on four
steps: generation of a set of partitions, computation of frequent patterns for
each cluster, pruning clusters that violates some constraints, and selection of
clusters and associated patterns to build an interpretable clustering. This
last step is combinatorial and we have developed a Constraint-Programming (CP)
model to solve it. The method can integrate prior knowledge in the form of user
constraints, both before or in the CP model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driving Intelligent IoT Monitoring and Control through Cloud Computing
  and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhe Li, Xiangxiang Wang, Yuan Feng, Yaqian Qi, Jingxiao Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores how to drive intelligent iot monitoring and control
through cloud computing and machine learning. As iot and the cloud continue to
generate large and diverse amounts of data as sensor devices in the network,
the collected data is sent to the cloud for statistical analysis, prediction,
and data analysis to achieve business objectives. However, because the cloud
computing model is limited by distance, it can be problematic in environments
where the quality of the Internet connection is not ideal for critical
operations. Therefore, edge computing, as a distributed computing architecture,
moves the location of processing applications, data and services from the
central node of the network to the logical edge node of the network to reduce
the dependence on cloud processing and analysis of data, and achieve near-end
data processing and analysis. The combination of iot and edge computing can
reduce latency, improve efficiency, and enhance security, thereby driving the
development of intelligent systems. The paper also introduces the development
of iot monitoring and control technology, the application of edge computing in
iot monitoring and control, and the role of machine learning in data analysis
and fault detection. Finally, the application and effect of intelligent
Internet of Things monitoring and control system in industry, agriculture,
medical and other fields are demonstrated through practical cases and
experimental studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personalized Video-Based Hand Taxonomy: Application for Individuals
  with Spinal Cord Injury 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdy Dousty, David J. Fleet, José Zariffa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand function is critical for our interactions and quality of life. Spinal
cord injuries (SCI) can impair hand function, reducing independence. A
comprehensive evaluation of function in home and community settings requires a
hand grasp taxonomy for individuals with impaired hand function. Developing
such a taxonomy is challenging due to unrepresented grasp types in standard
taxonomies, uneven data distribution across injury levels, and limited data.
This study aims to automatically identify the dominant distinct hand grasps in
egocentric video using semantic clustering. Egocentric video recordings
collected in the homes of 19 individual with cervical SCI were used to cluster
grasping actions with semantic significance. A deep learning model integrating
posture and appearance data was employed to create a personalized hand
taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with
with 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful
clusters in video content. This methodology provides a flexible and effective
strategy to analyze hand function in the wild. It offers researchers and
clinicians an efficient tool for evaluating hand function, aiding sensitive
assessments and tailored intervention plans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paths to Equilibrium in Normal-Form Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent reinforcement learning (MARL), agents repeatedly interact
across time and revise their strategies as new data arrives, producing a
sequence of strategy profiles. This paper studies sequences of strategies
satisfying a pairwise constraint inspired by policy updating in reinforcement
learning, where an agent who is best responding in period $t$ does not switch
its strategy in the next period $t+1$. This constraint merely requires that
optimizing agents do not switch strategies, but does not constrain the other
non-optimizing agents in any way, and thus allows for exploration. Sequences
with this property are called satisficing paths, and arise naturally in many
MARL algorithms. A fundamental question about strategic dynamics is such: for a
given game and initial strategy profile, is it always possible to construct a
satisficing path that terminates at an equilibrium strategy? The resolution of
this question has implications about the capabilities or limitations of a class
of MARL algorithms. We answer this question in the affirmative for mixed
extensions of finite normal-form games.%
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models
  using Markov Chain Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhong, Wanggang Shen, Tommie Catanach, Xun Huan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal experimental design (OED) provides a systematic approach to quantify
and maximize the value of experimental data. Under a Bayesian approach,
conventional OED maximizes the expected information gain (EIG) on model
parameters. However, we are often interested in not the parameters themselves,
but predictive quantities of interest (QoIs) that depend on the parameters in a
nonlinear manner. We present a computational framework of predictive
goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction
models, which seeks the experimental design providing the greatest EIG on the
QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG,
featuring Markov chain Monte Carlo for posterior sampling and kernel density
estimation for evaluating the posterior-predictive density and its
Kullback-Leibler divergence from the prior-predictive. The GO-OED design is
then found by maximizing the EIG over the design space using Bayesian
optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED
method, and illustrate its differences versus conventional non-GO-OED, through
various test problems and an application of sensor placement for source
inversion in a convection-diffusion field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State of the art applications of deep learning within tracking and
  detecting marine debris: A survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zoe Moorton, Dr. Zeyneb Kurt, Dr. Wai Lok Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have been explored within the marine litter problem
for approximately 20 years but the majority of the research has developed
rapidly in the last five years. We provide an in-depth, up to date, summary and
analysis of 28 of the most recent and significant contributions of deep
learning in marine debris. From cross referencing the research paper results,
the YOLO family significantly outperforms all other methods of object detection
but there are many respected contributions to this field that have
categorically agreed that a comprehensive database of underwater debris is not
currently available for machine learning. Using a small dataset curated and
labelled by us, we tested YOLOv5 on a binary classification task and found the
accuracy was low and the rate of false positives was high; highlighting the
importance of a comprehensive database. We conclude this survey with over 40
future research recommendations and open challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Review paper, 60 pages including references, 1 figure, 3 tables, 1
  supplementary data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Convolutional Transformer: Harmonizing Real vs. Complex
  Multi-View Spectral Operators for Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers used in vision have been investigated through diverse
architectures - ViT, PVT, and Swin. These have worked to improve the attention
mechanism and make it more efficient. Differently, the need for including local
information was felt, leading to incorporating convolutions in transformers
such as CPVT and CvT. Global information is captured using a complex Fourier
basis to achieve global token mixing through various methods, such as AFNO,
GFNet, and Spectformer. We advocate combining three diverse views of data -
local, global, and long-range dependence. We also investigate the simplest
global representation using only the real domain spectral representation -
obtained through the Hartley transform. We use a convolutional operator in the
initial layers to capture local information. Through these two contributions,
we are able to optimize and obtain a spectral convolution transformer (SCT)
that provides improved performance over the state-of-the-art methods while
reducing the number of parameters. Through extensive experiments, we show that
SCT-C-small gives state-of-the-art performance on the ImageNet dataset and
reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge
reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as
CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on
downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project
page is available on this webpage.\url{https://github.com/badripatro/sct}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R2D2 image reconstruction with model uncertainty quantification in radio
  astronomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)
approach was recently introduced for Radio-Interferometric (RI) imaging in
astronomy. R2D2's reconstruction is formed as a series of residual images,
iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the
previous iteration's image estimate and associated data residual as inputs. In
this work, we investigate the robustness of the R2D2 image estimation process,
by studying the uncertainty associated with its series of learned models.
Adopting an ensemble averaging approach, multiple series can be trained,
arising from different random DNN initializations of the training process at
each iteration. The resulting multiple R2D2 instances can also be leveraged to
generate ``R2D2 samples'', from which empirical mean and standard deviation
endow the algorithm with a joint estimation and uncertainty quantification
functionality. Focusing on RI imaging, and adopting a telescope-specific
approach, multiple R2D2 instances were trained to encompass the most general
observation setting of the Very Large Array (VLA). Simulations and real-data
experiments confirm that: (i) R2D2's image estimation capability is superior to
that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction
capability (arising from series with only few DNNs) makes the computation of
multiple reconstruction samples and of uncertainty maps practical even at large
image dimension; (iii) it is characterized by a very low model uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep polytopic autoencoders for low-dimensional linear parameter-varying
  approximations and nonlinear feedback design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Heiland, Yongho Kim, Steffen W. R. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polytopic autoencoders provide low-dimensional parametrizations of states in
a polytope. For nonlinear PDEs, this is readily applied to low-dimensional
linear parameter-varying (LPV) approximations as they have been exploited for
efficient nonlinear controller design via series expansions of the solution to
the state-dependent Riccati equation. In this work, we develop a polytopic
autoencoder for control applications and show how it outperforms standard
linear approaches in view of LPV approximations of nonlinear systems and how
the particular architecture enables higher order series expansions at little
extra computational effort. We illustrate the properties and potentials of this
approach to computational nonlinear controller design for large-scale systems
with a thorough numerical study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bidirectional Consistency Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Li, Jiajun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) are capable of generating remarkably high-quality
samples by iteratively denoising a random vector, a process that corresponds to
moving along the probability flow ordinary differential equation (PF ODE).
Interestingly, DMs can also invert an input image to noise by moving backward
along the PF ODE, a key operation for downstream tasks such as interpolation
and image editing. However, the iterative nature of this process restricts its
speed, hindering its broader application. Recently, Consistency Models (CMs)
have emerged to address this challenge by approximating the integral of the PF
ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE
solver complicates the inversion process. To resolve this, we introduce the
Bidirectional Consistency Model (BCM), which learns a single neural network
that enables both forward and backward traversal along the PF ODE, efficiently
unifying generation and inversion tasks within one framework. Notably, our
proposed method enables one-step generation and inversion while also allowing
the use of additional steps to enhance generation quality or reduce
reconstruction error. Furthermore, by leveraging our model's bidirectional
consistency, we introduce a sampling strategy that can enhance FID while
preserving the generated image content. We further showcase our model's
capabilities in several downstream tasks, such as interpolation and inpainting,
and present demonstrations of potential applications, including blind
restoration of compressed images and defending black-box adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting species occurrence patterns from partial observations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hager Radi Abdelwahed, Mélisande Teng, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the interlinked biodiversity and climate crises, we need an
understanding of where species occur and how these patterns are changing.
However, observational data on most species remains very limited, and the
amount of data available varies greatly between taxonomic groups. We introduce
the problem of predicting species occurrence patterns given (a) satellite
imagery, and (b) known information on the occurrence of other species. To
evaluate algorithms on this task, we introduce SatButterfly, a dataset of
satellite images, environmental data and observational data for butterflies,
which is designed to pair with the existing SatBird dataset of bird
observational data. To address this task, we propose a general model, R-Tran,
for predicting species occurrence patterns that enables the use of partial
observational data wherever found. We find that R-Tran outperforms other
methods in predicting species encounter rates with partial information both
within a taxon (birds) and across taxa (birds and butterflies). Our approach
opens new perspectives to leveraging insights from species with abundant data
to other species with scarce data, by modelling the ecosystems in which they
co-occur.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning workshop at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-system biological image quality enhancement based on the
  generative adversarial network as a foundation for establishing a
  multi-institute microscopy cooperative network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Panek, Carina Rząca, Maksymilian Szczypior, Joanna Sorysz, Krzysztof Misztal, Zbigniew Baster, Zenon Rajfur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality fluorescence imaging of biological systems is limited by
processes like photobleaching and phototoxicity, and also in many cases, by
limited access to the latest generations of microscopes. Moreover, low temporal
resolution can lead to a motion blur effect in living systems. Our work
presents a deep learning (DL) generative-adversarial approach to the problem of
obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents.
We propose a generative-adversarial network (GAN) for contrast transfer between
two different separate microscopy systems: a confocal microscope (producing HQ
images) and a wide-field fluorescence microscope (producing LQ images). Our
model proves that such transfer is possible, allowing us to receive
HQ-generated images characterized by low mean squared error (MSE) values, high
structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR)
values. For our best model in the case of comparing HQ-generated images and
HQ-ground truth images, the median values of the metrics are 6x10-4, 0.9413,
and 31.87, for MSE, SSIM, and PSNR, respectively. In contrast, in the case of
comparison between LQ and HQ ground truth median values of the metrics are
equal to 0.0071, 0.8304, and 21.48 for MSE, SSIM, and PSNR respectively.
Therefore, we observe a significant increase ranging from 14% to 49% for SSIM
and PSNR respectively. These results, together with other single-system
cross-modality studies, provide proof of concept for further implementation of
a cross-system biological image quality enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 5 Figures, 1 Table, 3 pages Supplementary Materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Pre-trained Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DORE: A Dataset For Portuguese Definition Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Frédéric Blain, Ruslan Mitkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Definition modelling (DM) is the task of automatically generating a
dictionary definition for a specific word. Computational systems that are
capable of DM can have numerous applications benefiting a wide range of
audiences. As DM is considered a supervised natural language generation
problem, these systems require large annotated datasets to train the machine
learning (ML) models. Several DM datasets have been released for English and
other high-resource languages. While Portuguese is considered a
mid/high-resource language in most natural language processing tasks and is
spoken by more than 200 million native speakers, there is no DM dataset
available for Portuguese. In this research, we fill this gap by introducing
DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more
than 100,000 definitions. We also evaluate several deep learning based DM
models on DORE and report the results. The dataset and the findings of this
paper will facilitate research and study of Portuguese in wider contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Need for Speed: Pruning Transformers with One Recipe <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Khaki, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the International Conference on Learning Representations
  (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IISE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large scale paired antibody language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Kenlay, Frédéric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa, Douglas Pires, Charlotte M. Deane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are proteins produced by the immune system that can identify and
neutralise a wide variety of antigens with high specificity and affinity, and
constitute the most successful class of biotherapeutics. With the advent of
next-generation sequencing, billions of antibody sequences have been collected
in recent years, though their application in the design of better therapeutics
has been constrained by the sheer volume and complexity of the data. To address
this challenge, we present IgBert and IgT5, the best performing
antibody-specific language models developed to date which can consistently
handle both paired and unpaired variable region sequences as input. These
models are trained comprehensively using the more than two billion unpaired
sequences and two million paired sequences of light and heavy chains present in
the Observed Antibody Space dataset. We show that our models outperform
existing antibody and protein language models on a diverse range of design and
regression tasks relevant to antibody engineering. This advancement marks a
significant leap forward in leveraging machine learning, large scale data sets
and high-performance computing for enhancing antibody design for therapeutic
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 6 tables, model weights available at
  https://zenodo.org/doi/10.5281/zenodo.10876908</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Multi-task embeddings for Data-Efficient Downstream training
  and inference in Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gomes, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As repositories of large scale data in earth observation (EO) have grown, so
have transfer and storage costs for model training and inference, expending
significant resources. We introduce Neural Embedding Compression (NEC), based
on the transfer of compressed embeddings to data consumers instead of raw data.
We adapt foundation models (FM) through learned neural compression to generate
multi-task embeddings while navigating the tradeoff between compression rate
and embedding utility. We update only a small fraction of the FM parameters
(10%) for a short training period (1% of the iterations of pre-training). We
evaluate NEC on two EO tasks: scene classification and semantic segmentation.
Compared with applying traditional compression to the raw data, NEC achieves
similar accuracy with a 75% to 90% reduction in data. Even at 99.7%
compression, performance drops by only 5% on the scene classification task.
Overall, NEC is a data-efficient yet performant approach for multi-task EO
modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample complexity of quantum hypothesis testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Fairness through Transforming Data Orthogonal to Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Chen, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TractOracle: towards an anatomically-informed reward function for
  RL-based tractography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Théberge, Maxime Descoteaux, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based tractography is a competitive alternative
to machine learning and classical tractography algorithms due to its high
anatomical accuracy obtained without the need for any annotated data. However,
the reward functions so far used to train RL agents do not encapsulate
anatomical knowledge which causes agents to generate spurious false positives
tracts. In this paper, we propose a new RL tractography system, TractOracle,
which relies on a reward network trained for streamline classification. This
network is used both as a reward function during training as well as a mean for
stopping the tracking process early and thus reduce the number of false
positive streamlines. This makes our system a unique method that evaluates and
reconstructs WM streamlines at the same time. We report an improvement of true
positive ratios by almost 20\% and a reduction of 3x of false positive ratios
on one dataset and an increase between 2x and 7x in the number true positive
streamlines on another dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Design and Scaling of Hybrid Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, Stefano Massaroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of deep learning architectures is a resource-demanding
process, due to a vast design space, long prototyping times, and high compute
costs associated with at-scale model training and evaluation. We set out to
simplify this process by grounding it in an end-to-end mechanistic architecture
design (MAD) pipeline, encompassing small-scale capability unit tests
predictive of scaling laws. Through a suite of synthetic token manipulation
tasks such as compression and recall, designed to probe capabilities, we
identify and test new hybrid architectures constructed from a variety of
computational primitives. We experimentally validate the resulting
architectures via an extensive compute-optimal and a new state-optimal scaling
law analysis, training over 500 language models between 70M to 7B parameters.
Surprisingly, we find MAD synthetics to correlate with compute-optimal
perplexity, enabling accurate evaluation of new architectures via isolated
proxy tasks. The new architectures found via MAD, based on simple ideas such as
hybridization and sparsity, outperform state-of-the-art Transformer,
convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in
scaling, both at compute-optimal budgets and in overtrained regimes. Overall,
these results provide evidence that performance on curated synthetic tasks can
be predictive of scaling laws, and that an optimal architecture should leverage
specialized layers via a hybrid topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time-consuming. Therefore, the challenging task of reconstructing
visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is
gaining attention in the vision research community. A major challenge in this
research problem is the lack of datasets, which capture diverse scene
conditions (e.g., lighting, shadows, weather, locations, landscapes, objects,
humans, buildings) and various image features (e.g., color, contrast,
saturation, hue, luminance, brightness, radiance). To address this gap, in this
paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic
HDR images sampled from the GTA-V video game. We perform thorough evaluation of
the proposed dataset, which demonstrates significant qualitative and
quantitative improvements of the state-of-the-art HDR image reconstruction
methods. Furthermore, we demonstrate the effectiveness of the proposed dataset
and its impact on additional computer vision tasks including 3D human pose
estimation, human body part segmentation, and holistic scene segmentation. The
dataset, data collection pipeline, and evaluation code are available at:
https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Na, Yuzhi Liang, Siu-Ming Yiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Aggregation is Not Private Against Membership Inference Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khac-Hoang Ngo, Johan Östman, Giuseppe Durisi, Alexandre Graell i Amat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in
federated learning, affording the server access only to the aggregate of model
updates while safeguarding the confidentiality of individual updates. Despite
widespread claims regarding SecAgg's privacy-preserving capabilities, a formal
analysis of its privacy is lacking, making such presumptions unjustified. In
this paper, we delve into the privacy implications of SecAgg by treating it as
a local differential privacy (LDP) mechanism for each local update. We design a
simple attack wherein an adversarial server seeks to discern which update
vector a client submitted, out of two possible ones, in a single training round
of federated learning under SecAgg. By conducting privacy auditing, we assess
the success probability of this attack and quantify the LDP guarantees provided
by SecAgg. Our numerical results unveil that, contrary to prevailing claims,
SecAgg offers weak privacy against membership inference attacks even in a
single training round. Indeed, it is difficult to hide a local update by adding
other independent local updates when the updates are of high dimension. Our
findings underscore the imperative for additional privacy-enhancing mechanisms,
such as noise injection, in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A Dataset
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric M. John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Image Captioning Considering Wasserstein Graph Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning can automatically generate captions for the given images,
and the key challenge is to learn a mapping function from visual features to
natural language features. Existing approaches are mostly supervised ones,
i.e., each image has a corresponding sentence in the training set. However,
considering that describing images always requires a huge of manpower, we
usually have limited amount of described images (i.e., image-text pairs) and a
large number of undescribed images in real-world applications. Thereby, a
dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we
propose a novel Semi-Supervised Image Captioning method considering Wasserstein
Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to
supervise the generated sentences. Different from traditional single modal
semi-supervised methods, the difficulty of semi-supervised cross-modal learning
lies in constructing intermediately comparable information among heterogeneous
modalities. In this paper, SSIC-WGM adopts the successful scene graphs as
intermediate information, and constrains the generated sentences from two
aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of
the raw image and generated sentence respectively, then employs the wasserstein
distance to better measure the similarity between region embeddings of
different graphs. 2) intra-modal consistency. SSIC-WGM takes the data
augmentation techniques for the raw images, then constrains the consistency
among augmented images and generated sentences. Consequently, SSIC-WGM combines
the cross-modal pseudo supervision and structure invariant measure for
efficiently using the undescribed images, and learns more reasonable mapping
function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream
  Enhanced Rectified Transformer Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Shao, Michael G. H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate, and effective traffic forecasting is vital for smart traffic
systems, crucial in urban traffic planning and management. Current
Spatio-Temporal Transformer models, despite their prediction capabilities,
struggle with balancing computational efficiency and accuracy, favoring global
over local information, and handling spatial and temporal data separately,
limiting insight into complex interactions. We introduce the Criss-Crossed
Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes
three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),
Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified
Temporal Self-attention (ReTSA). These modules aim to lower computational needs
via sparse attention, focus on local information for better traffic dynamics
understanding, and merge spatial and temporal insights through a unique
learning method. Extensive tests on six real-world datasets highlight
CCDSReFormer's superior performance. An ablation study also confirms the
significant impact of each component on the model's predictive accuracy,
showcasing our model's ability to forecast traffic flow effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No Patient Behind: Enhancing Medication Recommendation for Rare
  Disease Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommendation systems have gained significant attention in
healthcare as a means of providing tailored and effective drug combinations
based on patients' clinical information. However, existing approaches often
suffer from fairness issues, as recommendations tend to be more accurate for
patients with common diseases compared to those with rare conditions. In this
paper, we propose a novel model called Robust and Accurate REcommendations for
Medication (RAREMed), which leverages the pretrain-finetune learning paradigm
to enhance accuracy for rare diseases. RAREMed employs a transformer encoder
with a unified input sequence approach to capture complex relationships among
disease and procedure codes. Additionally, it introduces two self-supervised
pre-training tasks, namely Sequence Matching Prediction (SMP) and Self
Reconstruction (SR), to learn specialized medication needs and interrelations
among clinical codes. Experimental results on two real-world datasets
demonstrate that RAREMed provides accurate drug sets for both rare and common
disease patients, thereby mitigating unfairness in medication recommendation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are PDE Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Zhou, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural solvers for partial differential equations (PDEs) have great
potential, yet their practicality is currently limited by their
generalizability. PDEs evolve over broad scales and exhibit diverse behaviors;
predicting these phenomena will require learning representations across a wide
variety of inputs, which may encompass different coefficients, geometries, or
equations. As a step towards generalizable PDE modeling, we adapt masked
pretraining for PDEs. Through self-supervised learning across PDEs, masked
autoencoders can learn useful latent representations for downstream tasks. In
particular, masked pretraining can improve coefficient regression and
timestepping performance of neural solvers on unseen equations. We hope that
masked pretraining can emerge as a unifying method across large, unlabeled, and
heterogeneous datasets to learn latent physics at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solution for Point Tracking Task of ICCV 1st Perception Test Challenge
  2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongpeng Pan, Yang Yang, Zhongtian Fu, Yuxuan Zhang, Shian Du, Yi Xu, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report proposes an improved method for the Tracking Any Point (TAP)
task, which tracks any physical surface through a video. Several existing
approaches have explored the TAP by considering the temporal relationships to
obtain smooth point motion trajectories, however, they still suffer from the
cumulative error caused by temporal prediction. To address this issue, we
propose a simple yet effective approach called TAP with confident static points
(TAPIR+), which focuses on rectifying the tracking of the static point in the
videos shot by a static camera. To clarify, our approach contains two key
components: (1) Multi-granularity Camera Motion Detection, which could identify
the video sequence by the static camera shot. (2) CMR-based point trajectory
prediction with one moving object segmentation approach to isolate the static
point from the moving object. Our approach ranked first in the final test with
a score of 0.46.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding
  Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When the predicted sequence length exceeds the length seen during training,
the transformer's inference accuracy diminishes. Existing relative position
encoding methods, such as those based on the ALiBi technique, address the
length extrapolation challenge exclusively through the implementation of a
single kernel function, which introduces a constant bias to every post-softmax
attention scores according to their distance. These approaches do not
investigate or employ multiple kernel functions to address the extrapolation
challenge. Drawing on the ALiBi approach, this study proposes a novel relative
positional encoding method, called MEP, which employs a weighted average to
combine distinct kernel functions(such as the exponential kernel and the
Gaussian kernel) to generate a bias that is applied to post-softmax attention
scores. Initially, the framework utilizes various kernel functions to construct
multiple kernel functions. Each kernel function adheres to a consistent mean
weight coefficient, harnessing the synergistic advantages of different kernels
to formulate an innovative bias function. Subsequently, specific slopes are
tailored for each kernel function, applying penalties at varying rates, to
enhance the model's extrapolation capabilities. Finally, this bias is
seamlessly incorporated as a penalty to the post-softmax scores. We present two
distinct versions of our method: a parameter-free variant that requires no new
learnable parameters, which enhances length extrapolation capabilities without
compromising training efficiency, and a parameterized variant capable of
integrating state-of-the-art techniques. Empirical evaluations across diverse
datasets have demonstrated that both variants of our method achieve
state-of-the-art performance, outperforming traditional parameter-free and
parameterized approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Private is DP-SGD? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate a substantial gap between the privacy guarantees of the
Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch
sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) follows by
interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is
more commonly used in practical implementations, it is neither analytically nor
numerically amenable to easy privacy analysis. On the other hand, Poisson
subsampling based DP-SGD is challenging to scalably implement, but has a
well-understood privacy analysis, with multiple open-source numerically tight
privacy accountants available. This has led to a common practice of using
shuffling based DP-SGD in practice, but using the privacy analysis for the
corresponding Poisson subsampling version. Our result shows that there can be a
substantial gap between the privacy analysis when using the two types of batch
sampling, and thus advises caution in reporting privacy parameters for DP-SGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Piloto, Sofia Liguori, Sephora Madjiheurem, Miha Zgubic, Sean Lovett, Hamish Tomlinson, Sophie Elster, Chris Apps, Sims Witherspoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Power Flow (OPF) refers to a wide range of related optimization
problems with the goal of operating power systems efficiently and securely. In
the simplest setting, OPF determines how much power to generate in order to
minimize costs while meeting demand for power and satisfying physical and
operational constraints. In even the simplest case, power grid operators use
approximations of the AC-OPF problem because solving the exact problem is
prohibitively slow with state-of-the-art solvers. These approximations
sacrifice accuracy and operational feasibility in favor of speed. This
trade-off leads to costly "uplift payments" and increased carbon emissions,
especially for large power grids. In the present work, we train a deep learning
system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF
cost) without compromising speed (running in as little as 33--65 ms).
Importantly, CANOS scales to realistic grid sizes with promising empirical
results on grids containing as many as 10,000 buses. Finally, because CANOS is
a Graph Neural Network, it is robust to changes in topology. We show that CANOS
is accurate across N-1 topological perturbations of a base grid typically used
in security-constrained analysis. This paves the way for more efficient
optimization of more complex OPF problems which alter grid connectivity such as
unit commitment, topology optimization and security-constrained OPF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixing Artificial and Natural Intelligence: From Statistical Mechanics
  to AI and Back to Turbulence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Michael,  Chertkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper reflects on the future role of AI in scientific research, with a
special focus on turbulence studies, and examines the evolution of AI,
particularly through Diffusion Models rooted in non-equilibrium statistical
mechanics. It underscores the significant impact of AI on advancing reduced,
Lagrangian models of turbulence through innovative use of deep neural networks.
Additionally, the paper reviews various other AI applications in turbulence
research and outlines potential challenges and opportunities in the concurrent
advancement of AI and statistical hydrodynamics. This discussion sets the stage
for a future where AI and turbulence research are intricately intertwined,
leading to more profound insights and advancements in both fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGHormer: An Energy-Saving Graph Transformer Driven by Spikes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Transformers (GTs) with powerful representation learning ability make a
huge success in wide range of graph tasks. However, the costs behind
outstanding performances of GTs are higher energy consumption and computational
overhead. The complex structure and quadratic complexity during attention
calculation in vanilla transformer seriously hinder its scalability on the
large-scale graph data. Though existing methods have made strides in
simplifying combinations among blocks or attention-learning paradigm to improve
GTs' efficiency, a series of energy-saving solutions originated from
biologically plausible structures are rarely taken into consideration when
constructing GT framework. To this end, we propose a new spiking-based graph
transformer (SGHormer). It turns full-precision embeddings into sparse and
binarized spikes to reduce memory and computational costs. The spiking graph
self-attention and spiking rectify blocks in SGHormer explicitly capture global
structure information and recover the expressive power of spiking embeddings,
respectively. In experiments, SGHormer achieves comparable performances to
other full-precision GTs with extremely low computational energy consumption.
The results show that SGHomer makes a remarkable progress in the field of
low-energy GTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Distributional Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocong Chen, Siyu Wang, Tong Yu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) presents distinct challenges as it relies
solely on observational data. A central concern in this context is ensuring the
safety of the learned policy by quantifying uncertainties associated with
various actions and environmental stochasticity. Traditional approaches
primarily emphasize mitigating epistemic uncertainty by learning risk-averse
policies, often overlooking environmental stochasticity. In this study, we
propose an uncertainty-aware distributional offline RL method to simultaneously
address both epistemic uncertainty and environmental stochasticity. We propose
a model-free offline RL algorithm capable of learning risk-averse policies and
characterizing the entire distribution of discounted cumulative rewards, as
opposed to merely maximizing the expected value of accumulated discounted
returns. Our method is rigorously evaluated through comprehensive experiments
in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable cancer cell detection with phonon microscopy using
  multi-task conditional neural networks for inter-batch calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Zheng, Rafael Fuentes-Dominguez, Matt Clark, George S. D. Gordon, Fernando Perez-Cota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in artificial intelligence (AI) show great potential in revealing
underlying information from phonon microscopy (high-frequency ultrasound) data
to identify cancerous cells. However, this technology suffers from the 'batch
effect' that comes from unavoidable technical variations between each
experiment, creating confounding variables that the AI model may inadvertently
learn. We therefore present a multi-task conditional neural network framework
to simultaneously achieve inter-batch calibration, by removing confounding
variables, and accurate cell classification of time-resolved phonon-derived
signals. We validate our approach by training and validating on different
experimental batches, achieving a balanced precision of 89.22% and an average
cross-validated precision of 89.07% for classifying background, healthy and
cancerous regions. Classification can be performed in 0.5 seconds with only
simple prior batch information required for multiple batch corrections.
Further, we extend our model to reconstruct denoised signals, enabling physical
interpretation of salient features indicating disease state including sound
velocity, sound attenuation and cell-adhesion to substrate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeersimGym: An Environment for Solving the Task Offloading Problem with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederico Metelo, Stevo Racković, Pedro Ákos, Cláudia Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task offloading, crucial for balancing computational loads across devices in
networks such as the Internet of Things, poses significant optimization
challenges, including minimizing latency and energy usage under strict
communication and storage constraints. While traditional optimization falls
short in scalability; and heuristic approaches lack in achieving optimal
outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the
learning of optimal offloading strategies through iterative interactions.
However, the efficacy of RL hinges on access to rich datasets and
custom-tailored, realistic training environments. To address this, we introduce
PeersimGym, an open-source, customizable simulation environment tailored for
developing and optimizing task offloading strategies within computational
networks. PeersimGym supports a wide range of network topologies and
computational constraints and integrates a \textit{PettingZoo}-based interface
for RL agent deployment in both solo and multi-agent setups. Furthermore, we
demonstrate the utility of the environment through experiments with Deep
Reinforcement Learning agents, showcasing the potential of RL-based approaches
to significantly enhance offloading strategies in distributed computing
settings. PeersimGym thus bridges the gap between theoretical RL models and
their practical applications, paving the way for advancements in efficient task
offloading methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision Transformer with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven Energy Consumption Modelling for Electric Micromobility
  using an Open Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating challenges of traffic congestion and environmental degradation
underscore the critical importance of embracing E-Mobility solutions in urban
spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,
play a pivotal role in this transition, offering sustainable alternatives for
urban commuters. However, the energy consumption patterns for these tools are a
critical aspect that impacts their effectiveness in real-world scenarios and is
essential for trip planning and boosting user confidence in using these. To
this effect, recent studies have utilised physical models customised for
specific mobility tools and conditions, but these models struggle with
generalization and effectiveness in real-world scenarios due to a notable
absence of open datasets for thorough model evaluation and verification. To
fill this gap, our work presents an open dataset, collected in Dublin, Ireland,
specifically designed for energy modelling research related to E-Scooters and
E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption
modelling based on the dataset using a set of representative machine learning
algorithms and compare their performance against the contemporary mathematical
models as a baseline. Our results demonstrate a notable advantage for
data-driven models in comparison to the corresponding mathematical models for
estimating energy consumption. Specifically, data-driven models outperform
physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for
E-Scooters based on an in-depth analysis of the dataset under certain
assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables. This manuscript has been accepted by
  the IEEE ITEC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASIL: Learner-Aware Supervised Imitation Learning For Long-term
  Microscopic Traffic Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic traffic simulation plays a crucial role in transportation
engineering by providing insights into individual vehicle behavior and overall
traffic flow. However, creating a realistic simulator that accurately
replicates human driving behaviors in various traffic conditions presents
significant challenges. Traditional simulators relying on heuristic models
often fail to deliver accurate simulations due to the complexity of real-world
traffic environments. Due to the covariate shift issue, existing imitation
learning-based simulators often fail to generate stable long-term simulations.
In this paper, we propose a novel approach called learner-aware supervised
imitation learning to address the covariate shift problem in multi-agent
imitation learning. By leveraging a variational autoencoder simultaneously
modeling the expert and learner state distribution, our approach augments
expert states such that the augmented state is aware of learner state
distribution. Our method, applied to urban traffic simulation, demonstrates
significant improvements over existing state-of-the-art baselines in both
short-term microscopic and long-term macroscopic realism when evaluated on the
real-world dataset pNEUMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cvpr 2024. arXiv admin note: text overlap with
  arXiv:2306.06401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of Over-parameterization for Out-of-Distribution
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, machine learning models have achieved success based on the
independently and identically distributed assumption. However, this assumption
can be easily violated in real-world applications, leading to the
Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized
DNNs behave under non-trivial natural distributional shifts is essential, as
current theoretical understanding is insufficient. Existing theoretical works
often provide meaningless results for over-parameterized models in OOD
scenarios or even contradict empirical findings. To this end, we are
investigating the performance of the over-parameterized model in terms of OOD
generalization under the general benign overfitting conditions. Our analysis
focuses on a random feature model and examines non-trivial natural
distributional shifts, where the benign overfitting estimators demonstrate a
constant excess OOD loss, despite achieving zero excess in-distribution (ID)
loss. We demonstrate that in this scenario, further increasing the model's
parameterization can significantly reduce the OOD loss. Intuitively, the
variance term of ID loss remains low due to orthogonality of long-tail
features, meaning overfitting noise during training generally doesn't raise
testing loss. However, in OOD cases, distributional shift increases the
variance term. Thankfully, the inherent shift is unrelated to individual x,
maintaining the orthogonality of long-tail features. Expanding the hidden
dimension can additionally improve this orthogonality by mapping the features
into higher-dimensional spaces, thereby reducing the variance term. We further
show that model ensembles also improve OOD loss, akin to increasing model
capacity. These insights explain the empirical phenomenon of enhanced OOD
generalization through model ensembles, supported by consistent simulations
with theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haddouchi Maissae, Berrado Abdelaziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forest (RF) is well-known as an efficient ensemble learning method in
terms of predictive performance. It is also considered a Black Box because of
its hundreds of deep decision trees. This lack of interpretability can be a
real drawback for acceptance of RF models in several real-world applications,
especially those affecting one's lives, such as in healthcare, security, and
law. In this work, we present Forest-ORE, a method that makes RF interpretable
via an optimized rule ensemble (ORE) for local and global interpretation.
Unlike other rule-based approaches aiming at interpreting the RF model, this
method simultaneously considers several parameters that influence the choice of
an interpretable rule ensemble. Existing methods often prioritize predictive
performance over interpretability coverage and do not provide information about
existing overlaps or interactions between rules. Forest-ORE uses a
mixed-integer optimization program to build an ORE that considers the trade-off
between predictive performance, interpretability coverage, and model size (size
of the rule ensemble, rule lengths, and rule overlaps). In addition to
providing an ORE competitive in predictive performance with RF, this method
enriches the ORE through other rules that afford complementary information. It
also enables monitoring of the rule selection process and delivers various
metrics that can be used to generate a graphical representation of the final
model. This framework is illustrated through an example, and its robustness is
assessed through 36 benchmark datasets. A comparative analysis of well-known
methods shows that Forest-ORE provides an excellent trade-off between
predictive performance, interpretability coverage, and model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Privacy in Federated Learning through Local Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Bastianello, Changxin Liu, Karl H. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose the federated private local training algorithm
(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive
communications and (ii) privacy preservation. We address (i) by allowing for
both partial participation and local training, which significantly reduce the
number of communication rounds between the central coordinator and computing
agents. The algorithm matches the state of the art in the sense that the use of
local training demonstrably does not impact accuracy. Additionally, agents have
the flexibility to choose from various local training solvers, such as
(stochastic) gradient descent and accelerated gradient descent. Further, we
investigate how employing local training can enhance privacy, addressing point
(ii). In particular, we derive differential privacy bounds and highlight their
dependence on the number of local training epochs. We assess the effectiveness
of the proposed algorithm by comparing it to alternative techniques,
considering both theoretical analysis and numerical results from a
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Survey on Deep Learning and State-of-the-arts Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohd Halim Mohd Noor, Ayokunle Olalekan Ige
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning, a branch of artificial intelligence, is a computational model
that uses multiple layers of interconnected units (neurons) to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is a challenging task due to the algorithm`s
complexity and the dynamic nature of real-world problems. Several studies have
reviewed deep learning concepts and applications. However, the studies mostly
focused on the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art of deep
learning models and their applications in solving complex problems across
different domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing. We highlight the key features of the models and their effectiveness
in solving the problems within each domain. Furthermore, this study presents
the fundamentals of deep learning, various deep learning model types and
prominent convolutional neural network architectures. Finally, challenges and
future directions in deep learning research are discussed to offer a broader
perspective for future researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VDSC: Enhancing Exploration Timing with Value Discrepancy and State
  Counts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Captari, Remo Sasso, Matthia Sabatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable attention given to the questions of \textit{how
much} and \textit{how to} explore in deep reinforcement learning, the
investigation into \textit{when} to explore remains relatively less researched.
While more sophisticated exploration strategies can excel in specific, often
sparse reward environments, existing simpler approaches, such as
$\epsilon$-greedy, persist in outperforming them across a broader spectrum of
domains. The appeal of these simpler strategies lies in their ease of
implementation and generality across a wide range of domains. The downside is
that these methods are essentially a blind switching mechanism, which
completely disregards the agent's internal state. In this paper, we propose to
leverage the agent's internal state to decide \textit{when} to explore,
addressing the shortcomings of blind switching mechanisms. We present Value
Discrepancy and State Counts through homeostasis (VDSC), a novel approach for
efficient exploration timing. Experimental results on the Atari suite
demonstrate the superiority of our strategy over traditional methods such as
$\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like
Noisy Nets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range
  Air Combat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvards Scukins, Markus Klein, Lars Kroon, Petter Ögren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating new air combat tactics and discovering novel maneuvers can require
numerous hours of expert pilots' time. Additionally, for each different combat
scenario, the same strategies may not work since small changes in equipment
performance may drastically change the air combat outcome. For this reason, we
created a reinforcement learning environment to help investigate potential air
combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR
Gym. This type of air combat is important since long-range missiles are often
the first weapon to be used in aerial combat. Some existing environments
provide high-fidelity simulations but are either not open source or are not
adapted to the BVR air combat domain. Other environments are open source but
use less accurate simulation models. Our work provides a high-fidelity
environment based on the open-source flight dynamics simulator JSBSim and is
adapted to the BVR air combat domain. This article describes the building
blocks of the environment and some use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-sharing During Training and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangchen Yin, Yue Yin, Yuda W. Tang, Hai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning force fields (MLFFs) have emerged as a promising approach to
bridge the accuracy of quantum mechanical methods and the efficiency of
classical force fields. However, the abundance of MLFF models and the challenge
of accurately predicting atomic forces pose significant obstacles in their
practical application. In this paper, we propose a novel ensemble learning
framework, EL-MLFFs, which leverages the stacking method to integrate
predictions from diverse MLFFs and enhance force prediction accuracy. By
constructing a graph representation of molecular structures and employing a
graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures
atomic interactions and refines force predictions. We evaluate our approach on
two distinct datasets: methane molecules and methanol adsorbed on a Cu(100)
surface. The results demonstrate that EL-MLFFs significantly improves force
prediction accuracy compared to individual MLFFs, with the ensemble of all
eight models yielding the best performance. Moreover, our ablation study
highlights the crucial roles of the residual network and graph attention layers
in the model's architecture. The EL-MLFFs framework offers a promising solution
to the challenges of model selection and force prediction accuracy in MLFFs,
paving the way for more reliable and efficient molecular simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Graph Auto-Encoder Based Inductive Learning Method for
  Semi-Supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is a fundamental research issue in various
domains of applications, of which the inductive learning problem is
particularly challenging as it requires models to generalize to unseen graph
structures during inference. In recent years, graph neural networks (GNNs) have
emerged as powerful graph models for inductive learning tasks such as node
classification, whereas they typically heavily rely on the annotated nodes
under a fully supervised training setting. Compared with the GNN-based methods,
variational graph auto-encoders (VGAEs) are known to be more generalizable to
capture the internal structural information of graphs independent of node
labels and have achieved prominent performance on multiple unsupervised
learning tasks. However, so far there is still a lack of work focusing on
leveraging the VGAE framework for inductive learning, due to the difficulties
in training the model in a supervised manner and avoiding over-fitting the
proximity information of graphs. To solve these problems and improve the model
performance of VGAEs for inductive graph representation learning, in this work,
we propose the Self-Label Augmented VGAE model. To leverage the label
information for training, our model takes node labels as one-hot encoded inputs
and then performs label reconstruction in model training. To overcome the
scarcity problem of node labels for semi-supervised settings, we further
propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels
generated by our model with a node-wise masking approach to enhance the label
information. Experiments on benchmark inductive learning graph datasets verify
that our proposed model archives promising results on node classification with
particular superiority under semi-supervised learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capacity Provisioning Motivated Online Non-Convex Optimization Problem
  with Memory and Switching Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Vaze, Jayakrishnan Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An online non-convex optimization problem is considered where the goal is to
minimize the flow time (total delay) of a set of jobs by modulating the number
of active servers, but with a switching cost associated with changing the
number of active servers over time. Each job can be processed by at most one
fixed speed server at any time. Compared to the usual online convex
optimization (OCO) problem with switching cost, the objective function
considered is non-convex and more importantly, at each time, it depends on all
past decisions and not just the present one. Both worst-case and stochastic
inputs are considered; for both cases, competitive algorithms are derived.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Requirements Testability Measurement Based on
  Requirement Smells 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Zakeri-Nasrabadi, Saeed Parsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Requirements form the basis for defining software systems' obligations and
tasks. Testable requirements help prevent failures, reduce maintenance costs,
and make it easier to perform acceptance tests. However, despite the importance
of measuring and quantifying requirements testability, no automatic approach
for measuring requirements testability has been proposed based on the
requirements smells, which are at odds with the requirements testability. This
paper presents a mathematical model to evaluate and rank the natural language
requirements testability based on an extensive set of nine requirements smells,
detected automatically, and acceptance test efforts determined by requirement
length and its application domain. Most of the smells stem from uncountable
adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary
is required to detect such words. We offer a neural word-embedding technique to
generate such a dictionary automatically. Using the dictionary, we could
automatically detect Polysemy smell (domain-specific ambiguity) for the first
time in 10 application domains. Our empirical study on nearly 1000 software
requirements from six well-known industrial and academic projects demonstrates
that the proposed smell detection approach outperforms Smella, a
state-of-the-art tool, in detecting requirements smells. The precision and
recall of smell detection are improved with an average of 0.03 and 0.33,
respectively, compared to the state-of-the-art. The proposed requirement
testability model measures the testability of 985 requirements with a mean
absolute error of 0.12 and a mean squared error of 0.03, demonstrating the
model's potential for practical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 16 figures, and 13 tables; submitted as a journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Kernel for Neural Network Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao-Qun Zhang, Zong-Yi Chen, Yong-Ming Tian, Xun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past decades have witnessed a great interest in the distinction and
connection between neural network learning and kernel learning. Recent
advancements have made theoretical progress in connecting infinite-wide neural
networks and Gaussian processes. Two predominant approaches have emerged: the
Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The
former, rooted in Bayesian inference, represents a zero-order kernel, while the
latter, grounded in the tangent space of gradient descents, is a first-order
kernel. In this paper, we present the Unified Neural Kernel (UNK), which
characterizes the learning dynamics of neural networks with gradient descents
and parameter initialization. The proposed UNK kernel maintains the limiting
properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite
learning step and converging to NNGP as the learning step approaches infinity.
Besides, we also theoretically characterize the uniform tightness and learning
convergence of the UNK kernel, providing comprehensive insights into this
unified kernel. Experimental results underscore the effectiveness of our
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Particle identification with machine learning from incomplete data in
  the ALICE experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ALICE experiment at the LHC measures properties of the strongly
interacting matter formed in ultrarelativistic heavy-ion collisions. Such
studies require accurate particle identification (PID). ALICE provides PID
information via several detectors for particles with momentum from about 100
MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular
cuts. Acmuch better performance can be achieved with machine learning (ML)
methods. Our solution uses multiple neural networks (NN) serving as binary
classifiers. Moreover, we extended our particle classifier with Feature Set
Embedding and attention in order to train on data with incomplete samples. We
also present the integration of the ML project with the ALICE analysis
software, and we discuss domain adaptation, the ML technique needed to transfer
the knowledge between simulated and real experimental data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of 3rd Artificial Intelligence for the Electron Ion
  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to
  JINST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On permutation-invariant neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional machine learning algorithms have traditionally been designed
under the assumption that input data follows a vector-based format, with an
emphasis on vector-centric paradigms. However, as the demand for tasks
involving set-based inputs has grown, there has been a paradigm shift in the
research community towards addressing these challenges. In recent years, the
emergence of neural network architectures such as Deep Sets and Transformers
has presented a significant advancement in the treatment of set-based data.
These architectures are specifically engineered to naturally accommodate sets
as input, enabling more effective representation and processing of set
structures. Consequently, there has been a surge of research endeavors
dedicated to exploring and harnessing the capabilities of these architectures
for various tasks involving the approximation of set functions. This
comprehensive survey aims to provide an overview of the diverse problem
settings and ongoing research efforts pertaining to neural networks that
approximate set functions. By delving into the intricacies of these approaches
and elucidating the associated challenges, the survey aims to equip readers
with a comprehensive understanding of the field. Through this comprehensive
perspective, we hope that researchers can gain valuable insights into the
potential applications, inherent limitations, and future directions of
set-based neural networks. Indeed, from this survey we gain two insights: i)
Deep Sets and its variants can be generalized by differences in the aggregation
function, and ii) the behavior of Deep Sets is sensitive to the choice of the
aggregation function. From these observations, we show that Deep Sets, one of
the well-known permutation-invariant neural networks, can be generalized in the
sense of a quasi-arithmetic mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Error Analysis for Sparse Mixture-of-Experts: A
  Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Zhao, Peihao Wang, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates
predictions from several specialized sub-models (referred to as experts). This
fusion is accomplished through a router mechanism, dynamically assigning
weights to each expert's contribution based on the input data. Conventional MoE
mechanisms select all available experts, incurring substantial computational
costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages
only a limited number, or even just one expert, significantly reducing
computation overhead while empirically preserving, and sometimes even
enhancing, performance. Despite its wide-ranging applications and these
advantageous characteristics, MoE's theoretical underpinnings have remained
elusive. In this paper, we embark on an exploration of Sparse MoE's
generalization error concerning various critical factors. Specifically, we
investigate the impact of the number of data samples, the total number of
experts, the sparsity in expert selection, the complexity of the routing
mechanism, and the complexity of individual experts. Our analysis sheds light
on \textit{how \textbf{sparsity} contributes to the MoE's generalization},
offering insights from the perspective of classical learning theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application-Driven Innovation in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rolnick, Alan Aspuru-Guzik, Sara Beery, Bistra Dilkina, Priya L. Donti, Marzyeh Ghassemi, Hannah Kerner, Claire Monteleoni, Esther Rolf, Milind Tambe, Adam White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As applications of machine learning proliferate, innovative algorithms
inspired by specific real-world challenges have become increasingly important.
Such work offers the potential for significant impact not merely in domains of
application but also in machine learning itself. In this paper, we describe the
paradigm of application-driven research in machine learning, contrasting it
with the more standard paradigm of methods-driven research. We illustrate the
benefits of application-driven machine learning and how this approach can
productively synergize with methods-driven work. Despite these benefits, we
find that reviewing, hiring, and teaching practices in machine learning often
hold back application-driven innovation. We outline how these processes may be
improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDE: An Automatic Data Engine for Object Detection in Autonomous
  Driving <span class="chip">CVPR-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicle (AV) systems rely on robust perception models as a
cornerstone of safety assurance. However, objects encountered on the road
exhibit a long-tailed distribution, with rare or unseen categories posing
challenges to a deployed perception model. This necessitates an expensive
process of continuously curating and annotating data with significant human
effort. We propose to leverage recent advances in vision-language and large
language models to design an Automatic Data Engine (AIDE) that automatically
identifies issues, efficiently curates data, improves the model through
auto-labeling, and verifies the model through generation of diverse scenarios.
This process operates iteratively, allowing for continuous self-improvement of
the model. We further establish a benchmark for open-world detection on AV
datasets to comprehensively evaluate various learning paradigms, demonstrating
our method's superior performance at a reduced cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moreau Envelope Approach for LQR Meta-Policy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Trajectory Planning with Dual-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Jiahao Xu, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Pursuit of Fairness in Artificial Intelligence Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) models are now being utilized in all facets of
our lives such as healthcare, education and employment. Since they are used in
numerous sensitive environments and make decisions that can be life altering,
potential biased outcomes are a pressing matter. Developers should ensure that
such models don't manifest any unexpected discriminatory practices like
partiality for certain genders, ethnicities or disabled people. With the
ubiquitous dissemination of AI systems, researchers and practitioners are
becoming more aware of unfair models and are bound to mitigate bias in them.
Significant research has been conducted in addressing such issues to ensure
models don't intentionally or unintentionally perpetuate bias. This survey
offers a synopsis of the different ways researchers have promoted fairness in
AI systems. We explore the different definitions of fairness existing in the
current literature. We create a comprehensive taxonomy by categorizing
different types of bias and investigate cases of biased AI in different
application domains. A thorough study is conducted of the approaches and
techniques employed by researchers to mitigate bias in AI models. Moreover, we
also delve into the impact of biased models on user experience and the ethical
considerations to contemplate when developing and deploying such models. We
hope this survey helps researchers and practitioners understand the intricate
details of fairness and bias in AI systems. By sharing this thorough survey, we
aim to promote additional discourse in the domain of equitable and responsible
AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Support Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the success of deep learning is commonly attributed to its theoretical
equivalence with Support Vector Machines (SVM), the practical implications of
this relationship have not been thoroughly explored. This paper pioneers an
exploration in this domain, specifically focusing on the identification of Deep
Support Vectors (DSVs) within deep learning models. We introduce the concept of
DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)
conditions tailored for deep learning. Through empirical investigations, we
illustrate that DSVs exhibit similarities to support vectors in SVM, offering a
tangible method to interpret the decision-making criteria of models.
Additionally, our findings demonstrate that models can be effectively
reconstructed using DSVs, resembling the process in SVM. The code will be
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV
  Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youpeng Zhao, Di Wu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture has significantly advanced natural language
processing (NLP) and has been foundational in developing large language models
(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP
tasks. Despite their superior accuracy, LLMs present unique challenges in
practical inference, concerning the compute and memory-intensive nature. Thanks
to the autoregressive characteristic of LLM inference, KV caching for the
attention layers in Transformers can effectively accelerate LLM inference by
substituting quadratic-complexity computation with linear-complexity memory
accesses. Yet, this approach requires increasing memory as demand grows for
processing longer sequences. The overhead leads to reduced throughput due to
I/O bottlenecks and even out-of-memory errors, particularly on
resource-constrained systems like a single commodity GPU. In this paper, we
propose ALISA, a novel algorithm-system co-design solution to address the
challenges imposed by KV caching. On the algorithm level, ALISA prioritizes
tokens that are most important in generating a new token via a Sparse Window
Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and
reduces the memory footprint of KV caching at negligible accuracy loss. On the
system level, ALISA employs three-phase token-level dynamical scheduling and
optimizes the trade-off between caching and recomputation, thus maximizing the
overall performance in resource-constrained systems. In a single GPU-CPU
system, we demonstrate that under varying workloads, ALISA improves the
throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Multimodal Topic Modeling: A Comprehensive Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe González-Pizarro, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural topic models can successfully find coherent and diverse topics in
textual data. However, they are limited in dealing with multimodal datasets
(e.g., images and text). This paper presents the first systematic and
comprehensive evaluation of multimodal topic modeling of documents containing
both text and images. In the process, we propose two novel topic modeling
solutions and two novel evaluation metrics. Overall, our evaluation on an
unprecedented rich and diverse collection of datasets indicates that both of
our models generate coherent and diverse topics. Nevertheless, the extent to
which one method outperforms the other depends on the metrics and dataset
combinations, which suggests further exploration of hybrid solutions in the
future. Notably, our succinct human evaluation aligns with the outcomes
determined by our proposed metrics. This alignment not only reinforces the
credibility of our metrics but also highlights the potential for their
application in guiding future multimodal topic modeling endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-Ready for LREC-COLING 2024 (Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure
  Lookup Table Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Saleem, Amir Ziashahabi, Muhammad Naveed, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning models on data from multiple entities without
direct data sharing can unlock applications otherwise hindered by business,
legal, or ethical constraints. In this work, we design and implement new
privacy-preserving machine learning protocols for logistic regression and
neural network models. We adopt a two-server model where data owners
secret-share their data between two servers that train and evaluate the model
on the joint data. A significant source of inefficiency and inaccuracy in
existing methods arises from using Yao's garbled circuits to compute non-linear
activation functions. We propose new methods for computing non-linear functions
based on secret-shared lookup tables, offering both computational efficiency
and improved accuracy.
  Beyond introducing leakage-free techniques, we initiate the exploration of
relaxed security measures for privacy-preserving machine learning. Instead of
claiming that the servers gain no knowledge during the computation, we contend
that while some information is revealed about access patterns to lookup tables,
it maintains epsilon-dX-privacy. Leveraging this relaxation significantly
reduces the computational resources needed for training. We present new
cryptographic protocols tailored to this relaxed security paradigm and define
and analyze the leakage. Our evaluations show that our logistic regression
protocol is up to 9x faster, and the neural network training is up to 688x
faster than SecureML. Notably, our neural network achieves an accuracy of 96.6%
on MNIST in 15 epochs, outperforming prior benchmarks that capped at 93.4%
using the same architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Privacy Enhancing Technologies Symposium (PETS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Federated Learning Algorithms Are Created Equal: A Performance
  Evaluation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav A. Baumgart, Jaemin Shin, Ali Payani, Myungjin Lee, Ramana Rao Kompella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) emerged as a practical approach to training a model
from decentralized data. The proliferation of FL led to the development of
numerous FL algorithms and mechanisms. Many prior efforts have given their
primary focus on accuracy of those approaches, but there exists little
understanding of other aspects such as computational overheads, performance and
training stability, etc. To bridge this gap, we conduct extensive performance
evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi,
FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning
framework called Flame. Our comprehensive measurement study reveals that no
single algorithm works best across different performance metrics. A few key
observations are: (1) While some state-of-the-art algorithms achieve higher
accuracy than others, they incur either higher computation overheads (FedDyn)
or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller
standard deviation in accuracy across clients than FedAvg, indicating that the
advanced algorithms' performances are stable. (3) However, algorithms such as
FedDyn and SCAFFOLD are more prone to catastrophic failures without the support
of additional techniques such as gradient clipping. We hope that our empirical
study can help the community to build best practices in evaluating FL
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Switchback Designs in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianglin Wen, Chengchun Shi, Ying Yang, Niansheng Tang, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper offers a detailed investigation of switchback designs in A/B
testing, which alternate between baseline and new policies over time. Our aim
is to thoroughly evaluate the effects of these designs on the accuracy of their
resulting average treatment effect (ATE) estimators. We propose a novel "weak
signal analysis" framework, which substantially simplifies the calculations of
the mean squared errors (MSEs) of these ATEs in Markov decision process
environments. Our findings suggest that (i) when the majority of reward errors
are positively correlated, the switchback design is more efficient than the
alternating-day design which switches policies in a daily basis. Additionally,
increasing the frequency of policy switches tends to reduce the MSE of the ATE
estimator. (ii) When the errors are uncorrelated, however, all these designs
become asymptotically equivalent. (iii) In cases where the majority of errors
are negative correlated, the alternating-day design becomes the optimal choice.
These insights are crucial, offering guidelines for practitioners on designing
experiments in A/B testing. Our analysis accommodates a variety of policy value
estimators, including model-based estimators, least squares temporal difference
learning estimators, and double reinforcement learning estimators, thereby
offering a comprehensive understanding of optimal design strategies for policy
evaluation in reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEDDAP: Medical Dataset Enhancement via Diversified Augmentation
  Pipeline <span class="chip">MICCAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the
abundance and accuracy of available training data. However, collecting and
annotating data on a large scale is often both costly and time-intensive,
particularly in medical cases where practitioners are already occupied with
their duties. Moreover, ensuring that the model remains robust across various
scenarios of image capture is crucial in medical domains, especially when
dealing with ultrasound images that vary based on the settings of different
devices and the manual operation of the transducer. To address this challenge,
we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion
(SD) models to augment existing small datasets by automatically generating new
informative labeled samples. Pretrained checkpoints for SD are typically based
on natural images, and training them for medical images requires significant
GPU resources due to their heavy parameters. To overcome this challenge, we
introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method
tailored specifically for ultrasound applications. USLoRA allows for selective
fine-tuning of weights within SD, requiring fewer than 0.1\% of parameters
compared to fully fine-tuning only the UNet portion of SD. To enhance dataset
diversity, we incorporate different adjectives into the generation process
prompts, thereby desensitizing the classifiers to intensity changes across
different images. This approach is inspired by clinicians' decision-making
processes regarding breast tumors, where tumor shape often plays a more crucial
role than intensity. In conclusion, our pipeline not only outperforms
classifiers trained on the original dataset but also demonstrates superior
performance when encountering unseen datasets. The source code is available at
https://github.com/yasamin-med/MEDDAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOOD: Real-Time Human Presence and Out-of-Distribution Detection Using
  FMCW Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting human presence indoors with millimeter-wave frequency-modulated
continuous-wave (FMCW) radar faces challenges from both moving and stationary
clutter. This work proposes a robust and real-time capable human presence and
out-of-distribution (OOD) detection method using 60 GHz short-range FMCW radar.
HOOD solves the human presence and OOD detection problems simultaneously in a
single pipeline. Our solution relies on a reconstruction-based architecture and
works with radar macro and micro range-Doppler images (RDIs). HOOD aims to
accurately detect the presence of humans in the presence or absence of moving
and stationary disturbers. Since HOOD is also an OOD detector, it aims to
detect moving or stationary clutters as OOD in humans' absence and predicts the
current scene's output as "no presence." HOOD performs well in diverse
scenarios, demonstrating its effectiveness across different human activities
and situations. On our dataset collected with a 60 GHz short-range FMCW radar,
we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations
and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD
detection methods in terms of common OOD detection metrics. Importantly, HOOD
also perfectly fits on Raspberry Pi 3B+ with an ARM Cortex-A53 CPU, which
showcases its versatility across different hardware environments. Videos of our
human presence detection experiments are available at:
https://muskahya.github.io/HOOD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, project page: https://muskahya.github.io/HOOD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERM++: An Improved Baseline for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01973v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01973v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Kate Saenko, Bryan A. Plummer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) measures a classifier's ability to generalize to
new distributions of data it was not trained on. Recent work has shown that a
hyperparameter-tuned Empirical Risk Minimization (ERM) training procedure, that
is simply minimizing the empirical risk on the source domains, can outperform
most existing DG methods. ERM has achieved such strong results while only
tuning hyper-parameters such as learning rate, weight decay, batch size, and
dropout. However there are additional hyperparameters which further limit
overfitting and catastrophic forgetting. We therefore focus on tuning
previously untuned hyper-parameters, including training amount, initialization,
and additional regularizers. We call the resulting stronger baseline ERM++.
ERM++ improves the performance of DG by over 5% compared to prior ERM baselines
on a standard benchmark of 5 datasets with a ResNet-50 and over 15% with a
ViT-B/16, and outperforms all SOTA methods on DomainBed with both
architectures. We also explore the relationship between DG performance and
similarity to pre-training data, and find that similarity to pre-training data
distributions is an important driver of performance, but that ERM++ with
stronger initializations can deliver strong performance even on dissimilar
datasets.Code is released at https://github.com/piotr-teterwak/erm_plusplus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An improved baseline for Domain Generalization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Whole-Body Control for Legged Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely Visual Whole-Body Control(VBC), is composed of a low-level
policy using all degrees of freedom to track the end-effector manipulator
position and a high-level policy proposing the end-effector position based on
visual inputs. We train both levels of policies in simulation and perform
Sim2Real transfer for real robot deployment. We perform extensive experiments
and show significant improvements over baselines in picking up diverse objects
in different configurations (heights, locations, orientations) and
environments. Project page: https://wholebody-b1.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyDCM: Custom Data Center Models with Reinforcement Learning for
  Sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03906v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03906v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avisek Naug, Antonio Guillen, Ricardo Luna Gutiérrez, Vineet Gundecha, Dejan Markovikj, Lekhapriya Dheeraj Kashyap, Lorenz Krause, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Soumyendu Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing global emphasis on sustainability and reducing carbon
emissions is pushing governments and corporations to rethink their approach to
data center design and operation. Given their high energy consumption and
exponentially large computational workloads, data centers are prime candidates
for optimizing power consumption, especially in areas such as cooling and IT
energy usage. A significant challenge in this pursuit is the lack of a
configurable and scalable thermal data center model that offers an end-to-end
pipeline. Data centers consist of multiple IT components whose geometric
configuration and heat dissipation make thermal modeling difficult. This paper
presents PyDCM, a customizable Data Center Model implemented in Python, that
allows users to create unique configurations of IT equipment with custom server
specifications and geometric arrangements of IT cabinets. The use of vectorized
thermal calculations makes PyDCM orders of magnitude faster (30 times) than
current Energy Plus modeling implementations and scales sublinearly with the
number of CPUs. Also, PyDCM enables the use of Deep Reinforcement Learning via
the Gymnasium wrapper to optimize data center cooling and offers a
user-friendly platform for testing various data center design prototypes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 10th ACM International Conference on Systems for Energy-Efficient
  Buildings, Cities, and Transportation (BuildSys '23), November 15-16, 2023,
  Istanbul, Turkey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Rate Phase Association with Travel Time Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Shi, Maarten V. de Hoop, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our understanding of regional seismicity from multi-station seismograms
relies on the ability to associate arrival phases with their originating
earthquakes. Deep-learning-based phase detection now detects small, high-rate
arrivals from seismicity clouds, even at negative magnitudes. This new data
could give important insight into earthquake dynamics, but it is presents a
challenging association task. Existing techniques relying on coarsely
approximated, fixed wave speed models fail in this unexplored dense regime
where the complexity of unknown wave speed cannot be ignored. We introduce
Harpa, a high-rate association framework built on deep generative modeling and
neural fields. Harpa incorporates wave physics by using optimal transport to
compare arrival sequences. It is thus robust to unknown wave speeds and
estimates the wave speed model as a by-product of association. Experiments with
realistic, complex synthetic models show that Harpa is the first seismic phase
association framework which is accurate in the high-rate regime, paving the way
for new avenues in exploratory Earth science and improved understanding of
seismicity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Inductive Invariant Based Verification of Neural Network
  Controlled Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Stavros Tripakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural networks into safety-critical systems has shown
great potential in recent years. However, the challenge of effectively
verifying the safety of Neural Network Controlled Systems (NNCS) persists. This
paper introduces a novel approach to NNCS safety verification, leveraging the
inductive invariant method. Verifying the inductiveness of a candidate
inductive invariant in the context of NNCS is hard because of the scale and
nonlinearity of neural networks. Our compositional method makes this
verification process manageable by decomposing the inductiveness proof
obligation into smaller, more tractable subproblems. Alongside the high-level
method, we present an algorithm capable of automatically verifying the
inductiveness of given candidates by automatically inferring the necessary
decomposition predicates. The algorithm significantly outperforms the baseline
method and shows remarkable reductions in execution time in our case studies,
shortening the verification time from hours (or timeout) to seconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate and Weighted Data Reconstruction Attack in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongcun Song, Ziqi Wang, Enrique Zuazua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed learning paradigm that enables
multiple clients to collaborate on building a machine learning model without
sharing their private data. Although FL is considered privacy-preserved by
design, recent data reconstruction attacks demonstrate that an attacker can
recover clients' training data based on the parameters shared in FL. However,
most existing methods fail to attack the most widely used horizontal Federated
Averaging (FedAvg) scenario, where clients share model parameters after
multiple local training steps. To tackle this issue, we propose an
interpolation-based approximation method, which makes attacking FedAvg
scenarios feasible by generating the intermediate model updates of the clients'
local training processes. Then, we design a layer-wise weighted loss function
to improve the data quality of reconstruction. We assign different weights to
model updates in different layers concerning the neural network structure, with
the weights tuned by Bayesian optimization. Finally, experimental results
validate the superiority of our proposed approximate and weighted attack (AWA)
method over the other state-of-the-art methods, as demonstrated by the
substantial improvement in different evaluation metrics for image data
reconstructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Pre-trained Human Language Models: Is it Better with Human
  Context as Groups, Individual Traits, or Both? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human context into language models is the next frontier for
human-centered natural language processing. Currently, two pre-training methods
exist: group-wise attributes (e.g., over-45-year-olds) or individual traits.
Group attributes are coarse -- not all 45-year-olds write the same way -- while
modeling individual traits allows for a more personalized representation, but
requires more complex modeling and data. So far, it is unclear which
pre-training approach benefits what tasks. We compare pre-training models with
human context via 1) group attributes, 2) individual users, and 3) a combined
approach on 5 user- and document-level tasks. We find that pre-training with
both group and individual features significantly improves the two user-level
regression tasks like age estimation and personality assessment. Pre-training
on individual users significantly improves the three document-level
classification tasks like stance and topic detection. It even does well for
downstream tasks without historical user data. Our results suggest both
approaches have specific use cases, opening new avenues for human-centered
language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit
  Encodings for Efficient DNN Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Deep Neural Network (DNN) quantization methods using integer,
fixed-point, or floating-point data types struggle to capture diverse DNN
parameter distributions at low precision, and often require large silicon
overhead and intensive quantization-aware training. In this study, we introduce
Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by
posits that dynamically adapts to DNN weight/activation distributions by
parameterizing LP bit fields. We also develop a novel genetic-algorithm based
framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters
while reducing representational divergence between quantized and full-precision
models through a novel global-local contrastive objective. Additionally, we
design a unified mixed-precision LP accelerator (LPA) architecture comprising
of processing elements (PEs) incorporating LP in the computational datapath.
Our algorithm-hardware co-design demonstrates on average <1% drop in top-1
accuracy across various CNN and ViT models. It also achieves ~ 2x improvements
in performance per unit area and 2.2x gains in energy efficiency compared to
state-of-the-art quantization accelerators using different data types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 61st IEEE/ACM Design Automation Conference (DAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothQuant: Accurate and Efficient Post-Training Quantization for Large
  Language Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10438v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10438v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, existing methods cannot maintain accuracy and hardware efficiency at
the same time. We propose SmoothQuant, a training-free, accuracy-preserving,
and general-purpose post-training quantization (PTQ) solution to enable 8-bit
weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that
weights are easy to quantize while activations are not, SmoothQuant smooths the
activation outliers by offline migrating the quantization difficulty from
activations to weights with a mathematically equivalent transformation.
SmoothQuant enables an INT8 quantization of both weights and activations for
all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,
Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x
speedup and 2x memory reduction for LLMs with negligible loss in accuracy.
SmoothQuant enables serving 530B LLM within a single node. Our work offers a
turn-key solution that reduces hardware costs and democratizes LLMs. Code is
available at https://github.com/mit-han-lab/smoothquant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023. First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating counterfactuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15328v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15328v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juha Karvanen, Santtu Tikka, Matti Vihola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual inference considers a hypothetical intervention in a parallel
world that shares some evidence with the factual world. If the evidence
specifies a conditional distribution on a manifold, counterfactuals may be
analytically intractable. We present an algorithm for simulating values from a
counterfactual distribution where conditions can be set on both discrete and
continuous variables. We show that the proposed algorithm can be presented as a
particle filter leading to asymptotically valid inference. The algorithm is
applied to fairness analysis in credit-scoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The opportunities and risks of large language models in mental health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global rates of mental health concerns are rising and there is increasing
realization that existing models of mental healthcare will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health-related tasks. In this review, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs application
to mental health and encourage adoption of strategies to mitigate these risks.
The urgent need for mental health support must be balanced with responsible
development, testing, and deployment of mental health LLMs. Especially critical
is ensuring that mental health LLMs are fine-tuned for mental health, enhance
mental health equity, adhere to ethical standards, and that people, including
those with lived experience with mental health concerns, are involved in all
stages from development through deployment. Prioritizing these efforts will
minimize potential harms to mental health and maximize the likelihood that LLMs
will positively impact mental health globally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Synthetic Human Group Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16772v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16772v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Jui Chang, Danrui Li, Deep Patel, Parth Goel, Honglu Zhou, Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by Unity Engine, M3Act features
multiple semantic groups, highly diverse and photorealistic images, and a
comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments. The results suggest our synthetic dataset can significantly
improve the performance of several downstream methods and replace real-world
datasets to reduce cost. Notably, M3Act improves the state-of-the-art MOTRv2 on
DanceTrack dataset, leading to a hop on the leaderboard from 10th to 2nd place.
Moreover, M3Act opens new research for controllable 3D group activity
generation. We define multiple metrics and propose a competitive baseline for
the novel task. Our code and data are available at our project page:
http://cjerry1243.github.io/M3Act.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually Pre-train Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An optimal control perspective on diffusion-based generative modeling <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Berner, Lorenz Richter, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs), such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we can formulate
diffusion-based generative modeling as a minimization of the Kullback-Leibler
divergence between suitable measures in path space. Finally, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other
diffusion-based sampling approaches on multiple numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Independent Communication in Multi-Agent Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper appearing on AAMAS 2024 with the same
  title. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A randomized algorithm for nonconvex minimization with inexact
  evaluations and complexity guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyao Li, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimization of a smooth nonconvex function with inexact oracle
access to gradient and Hessian (without assuming access to the function value)
to achieve approximate second-order optimality. A novel feature of our method
is that if an approximate direction of negative curvature is chosen as the
step, we choose its sense to be positive or negative with equal probability. We
allow gradients to be inexact in a relative sense and relax the coupling
between inexactness thresholds for the first- and second-order optimality
conditions. Our convergence analysis includes both an expectation bound based
on martingale analysis and a high-probability bound based on concentration
inequalities. We apply our algorithm to empirical risk minimization problems
and obtain improved gradient sample complexity over existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Borrowing Treasures from Neighbors: In-Context Learning for Multimodal
  Learning with Missing Modalities and Data Scarcity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning with missing modalities is an increasingly
relevant challenge arising in various applications such as healthcare. This
paper extends the current research into missing modalities to the low-data
regime, i.e., a downstream task has both missing modalities and limited sample
size issues. This problem setting is particularly challenging and also
practical as it is often expensive to get full-modality data and sufficient
annotated training samples. We propose to use retrieval-augmented in-context
learning to address these two crucial issues by unleashing the potential of a
transformer's in-context learning ability. Diverging from existing methods,
which primarily belong to the parametric paradigm and often require sufficient
training samples, our work exploits the value of the available full-modality
data, offering a novel perspective on resolving the challenge. The proposed
data-dependent framework exhibits a higher degree of sample efficiency and is
empirically demonstrated to enhance the classification model's performance on
both full- and missing-modality data in the low-data regime across various
multimodal learning tasks. When only 1% of the training data are available, our
proposed method demonstrates an average improvement of 6.1% over a recent
strong baseline across various datasets and missing states. Notably, our method
also reduces the performance gap between full-modality and missing-modality
data compared with the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Rewired Message-Passing Neural Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Data Splitting in Distributed Optimization for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed optimization problem has become increasingly relevant
recently. It has a lot of advantages such as processing a large amount of data
in less time compared to non-distributed methods. However, most distributed
approaches suffer from a significant bottleneck - the cost of communications.
Therefore, a large amount of research has recently been directed at solving
this problem. One such approach uses local data similarity. In particular,
there exists an algorithm provably optimally exploiting the similarity
property. But this result, as well as results from other works solve the
communication bottleneck by focusing only on the fact that communication is
significantly more expensive than local computing and does not take into
account the various capacities of network devices and the different
relationship between communication time and local computing expenses. We
consider this setup and the objective of this study is to achieve an optimal
ratio of distributed data between the server and local machines for any costs
of communications and local computations. The running times of the network are
compared between uniform and optimal distributions. The superior theoretical
performance of our solutions is experimentally validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activations and Gradients Compression for Model-Parallel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially private multivariate medians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical tools which satisfy rigorous privacy guarantees are necessary for
modern data analysis. It is well-known that robustness against contamination is
linked to differential privacy. Despite this fact, using multivariate medians
for differentially private and robust multivariate location estimation has not
been systematically studied. We develop novel finite-sample performance
guarantees for differentially private multivariate depth-based medians, which
are essentially sharp. Our results cover commonly used depth functions, such as
the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.
We show that under Cauchy marginals, the cost of heavy-tailed location
estimation outweighs the cost of privacy. We demonstrate our results
numerically using a Gaussian contamination model in dimensions up to d = 100,
and compare them to a state-of-the-art private mean estimation algorithm. As a
by-product of our investigation, we prove concentration inequalities for the
output of the exponential mechanism about the maximizer of the population
objective function. This bound applies to objective functions that satisfy a
mild regularity condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part
  II: Regularization and application of the pseudo-2D model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian parameter inference is useful to improve Li-ion battery diagnostics
and can help formulate battery aging models. However, it is computationally
intensive and cannot be easily repeated for multiple cycles, multiple operating
conditions, or multiple replicate cells. To reduce the computational cost of
Bayesian calibration, numerical solvers for physics-based models can be
replaced with faster surrogates. A physics-informed neural network (PINN) is
developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For
the P2D surrogate, additional training regularization was needed as compared to
the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and
P2D surrogate models are exercised for parameter inference and compared to data
obtained from a direct numerical solution of the governing equations. A
parameter inference study highlights the ability to use these PINNs to
calibrate scaling parameters for the cathode Li diffusion and the anode
exchange current density. By realizing computational speed-ups of 2250x for the
P2D model, as compared to using standard integrating methods, the PINN
surrogates enable rapid state-of-health diagnostics. In the low-data
availability scenario, the testing error was estimated to 2mV for the SPM
surrogate and 10mV for the P2D surrogate which could be mitigated with
additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part I:
  Implementation and multi-fidelity hierarchies for the single-particle model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To plan and optimize energy storage demands that account for Li-ion battery
aging dynamics, techniques need to be developed to diagnose battery internal
states accurately and rapidly. This study seeks to reduce the computational
resources needed to determine a battery's internal states by replacing
physics-based Li-ion battery models -- such as the single-particle model (SPM)
and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)
surrogate. The surrogate model makes high-throughput techniques, such as
Bayesian calibration, tractable to determine battery internal parameters from
voltage responses. This manuscript is the first of a two-part series that
introduces PINN surrogates of Li-ion battery models for parameter inference
(i.e., state-of-health diagnostics). In this first part, a method is presented
for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical
training, where several neural nets are trained with multiple physics-loss
fidelities is shown to significantly improve the surrogate accuracy when only
training on the governing equation residuals. The implementation is made
available in a companion repository (https://github.com/NREL/pinnstripes). The
techniques used to develop a PINN surrogate of the SPM are extended in Part II
for the PINN surrogate for the P2D battery model, and explore the Bayesian
calibration capabilities of both surrogates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling the Spectral Properties of the Hodge Laplacian: Not All
  Small Eigenvalues Are Equal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent P. Grande, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich spectral information of the graph Laplacian has been instrumental in
graph theory, machine learning, and graph signal processing for applications
such as graph classification, clustering, or eigenmode analysis. Recently, the
Hodge Laplacian has come into focus as a generalisation of the ordinary
Laplacian for higher-order graph models such as simplicial and cellular
complexes. Akin to the traditional analysis of graph Laplacians, many authors
analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to
important topological properties such as homology. However, small eigenvalues
of the Hodge Laplacian can carry different information depending on whether
they are related to curl or gradient eigenmodes, and thus may not be
comparable. We therefore introduce the notion of persistent eigenvector
similarity and provide a method to track individual harmonic, curl, and
gradient eigenvectors/-values through the so-called persistence filtration,
leveraging the full information contained in the Hodge-Laplacian spectrum
across all possible scales of a point cloud. Finally, we use our insights (a)
to introduce a novel form of Hodge spectral clustering and (b) to classify
edges and higher-order simplices based on their relationship to the smallest
harmonic, curl, and gradient eigenvectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Pre-training for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in IEEE Transactions on Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, they show poor asymptotic performance and
struggle with out-of-distribution tasks because they rely on sequence models,
such as recurrent neural networks or transformers, to process experiences
rather than summarize them using general-purpose RL components such as value
functions. In contrast, traditional RL algorithms are data-inefficient as they
do not use domain knowledge, but they do converge to an optimal policy in the
limit. We propose RL$^3$, a principled hybrid approach that incorporates
action-values, learned per task through traditional RL, in the inputs to
meta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,
compared to RL$^2$, while maintaining data-efficiency in the short term, and
generalizes better to out-of-distribution tasks. Experiments are conducted on
both custom and benchmark discrete domains from the meta-RL literature that
exhibit a range of short-term, long-term, and complex dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization for Sparse Deep Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. S. Hotegni, M. Berkemeier, S. Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neural Nets and the Representation of Human Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Freiesleben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For: Philosophy of Science for Machine Learning: Core Issues and New
  Perspectives, edited by Juan Duran and Giorgia Pozzi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Conic Proxies for AC Optimal Power Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant interest in the development of
machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).
Although significant progress has been achieved in predicting high-quality
primal solutions, no existing learning-based approach can provide valid dual
bounds for AC-OPF. This paper addresses this gap by training optimization
proxies for a convex relaxation of AC-OPF. Namely, the paper considers a
second-order cone (SOC) relaxation of AC-OPF, and proposes \revision{a novel
architecture} that embeds a fast, differentiable (dual) feasibility recovery,
thus providing valid dual bounds. The paper combines this new architecture with
a self-supervised learning scheme, which alleviates the need for costly
training data generation. Extensive numerical experiments on medium- and
large-scale power grids demonstrate the efficiency and scalability of the
proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to PSCC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonic Control Lyapunov Barrier Functions for Constrained Optimal
  Control with Reach-Avoid Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Ruikun Zhou, Haocheng Chang, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces harmonic control Lyapunov barrier functions (harmonic
CLBF) that aid in constrained control problems such as reach-avoid problems.
Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to
encode the properties of control Lyapunov barrier functions (CLBFs). As a
result, they can be initiated at the start of an experiment rather than trained
based on sample trajectories. The control inputs are selected to maximize the
inner product of the system dynamics with the steepest descent direction of the
harmonic CLBF. Numerical results are presented with four different systems
under different reach-avoid environments. Harmonic CLBFs show a significantly
low risk of entering unsafe regions and a high probability of entering the goal
region.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Feature and Model Importance in Android Malware Detection:
  An Implemented Survey and Experimental Comparison of ML-Based Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Muzaffar, Hani Ragab Hassen, Hind Zantout, Michael A Lones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of Android means it is a common target for malware. Over the
years, various studies have found that machine learning models can effectively
discriminate malware from benign applications. However, as the operating system
evolves, so does malware, bringing into question the findings of these previous
studies, many of which report very high accuracies using small, outdated, and
often imbalanced datasets. In this paper, we reimplement 18 representative past
works and reevaluate them using a balanced, relevant, and up-to-date dataset
comprising 124,000 applications. We also carry out new experiments designed to
fill holes in existing knowledge, and use our findings to identify the most
effective features and models to use for Android malware detection within a
contemporary environment. We show that high detection accuracies (up to 96.8%)
can be achieved using features extracted through static analysis alone,
yielding a modest benefit (1%) from using far more expensive dynamic analysis.
API calls and opcodes are the most productive static and TCP network traffic
provide the most predictive dynamic features. Random forests are generally the
most effective model, outperforming more complex deep learning approaches.
Whilst directly combining static and dynamic features is generally ineffective,
ensembling models separately leads to performances comparable to the best
models but using less brittle features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Randomization via Entropy Maximization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varying dynamics parameters in simulation is a popular Domain Randomization
(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).
Nevertheless, DR heavily hinges on the choice of the sampling distribution of
the dynamics parameters, since high variability is crucial to regularize the
agent's behavior but notoriously leads to overly conservative policies when
randomizing excessively. In this paper, we propose a novel approach to address
sim-to-real transfer, which automatically shapes dynamics distributions during
training in simulation without requiring real-world data. We introduce DOmain
RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization
problem that directly maximizes the entropy of the training distribution while
retaining generalization capabilities. In achieving this, DORAEMON gradually
increases the diversity of sampled dynamics parameters as long as the
probability of success of the current policy is sufficiently high. We
empirically validate the consistent benefits of DORAEMON in obtaining highly
adaptive and generalizable policies, i.e. solving the task at hand across the
widest range of dynamics parameters, as opposed to representative baselines
from the DR literature. Notably, we also demonstrate the Sim2Real applicability
of DORAEMON through its successful zero-shot transfer in a robotic manipulation
setup under unknown real-world parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to Prompt Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RetroBridge: Modeling Retrosynthesis with Markov Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilia Igashov, Arne Schneuing, Marwin Segler, Michael Bronstein, Bruno Correia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis planning is a fundamental challenge in chemistry which aims at
designing reaction pathways from commercially available starting materials to a
target molecule. Each step in multi-step retrosynthesis planning requires
accurate prediction of possible precursor molecules given the target molecule
and confidence estimates to guide heuristic search algorithms. We model
single-step retrosynthesis planning as a distribution learning problem in a
discrete state space. First, we introduce the Markov Bridge Model, a generative
framework aimed to approximate the dependency between two intractable discrete
distributions accessible via a finite sample of coupled data points. Our
framework is based on the concept of a Markov bridge, a Markov process pinned
at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does
not need a tractable noise distribution as a sampling proxy and directly
operates on the input product molecules as samples from the intractable prior
distribution. We then address the retrosynthesis planning problem with our
novel framework and introduce RetroBridge, a template-free retrosynthesis
modeling approach that achieves state-of-the-art results on standard evaluation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan Karfa, Ramesh Karri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System Verilog Assertion (SVA) formulation -- a critical yet complex task is
a prerequisite in the Formal Property Verification (FPV) process.
Traditionally, SVA formulation involves expert-driven interpretation of
specifications, which is timeconsuming and prone to human error. However,
LLM-informed automatic assertion generation is gaining interest. We designeda
novel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA
assertions from natural language specifications. ChIRAAG constitutes the
systematic breakdown of design specifications into a standardized format,
further generating assertions from formatted specifications using LLM.
Furthermore, we developed testbenches to verify/validate the LLM-generated
assertions. Automatic feedback of log files from the simulation tool to the LLM
ensures that the framework can generate correc SVAs automatically. Only 33% of
LLM-generated raw assertions had errors. Our results on OpenTitan designs shows
that LLMs can streamline and assist engineers in the assertion generation
process, reshaping verification workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures and 2 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCSD: A Federated Learning Based Approach for Code-Smell Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadi Alawadi, Khalid Alkharabsheh, Fahed Alkhabbas, Victor Kebande, Feras M. Awaysheh, Fabio Palomba, Mohammed Awad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a Federated Learning Code Smell Detection (FedCSD)
approach that allows organizations to collaboratively train federated ML models
while preserving their data privacy. These assertions have been supported by
three experiments that have significantly leveraged three manually validated
datasets aimed at detecting and examining different code smell scenarios. In
experiment 1, which was concerned with a centralized training experiment,
dataset two achieved the lowest accuracy (92.30%) with fewer smells, while
datasets one and three achieved the highest accuracy with a slight difference
(98.90% and 99.5%, respectively). This was followed by experiment 2, which was
concerned with cross-evaluation, where each ML model was trained using one
dataset, which was then evaluated over the other two datasets. Results from
this experiment show a significant drop in the model's accuracy (lowest
accuracy: 63.80\%) where fewer smells exist in the training dataset, which has
a noticeable reflection (technical debt) on the model's performance. Finally,
the last and third experiments evaluate our approach by splitting the dataset
into 10 companies. The ML model was trained on the company's site, then all
model-updated weights were transferred to the server. Ultimately, an accuracy
of 98.34% was achieved by the global model that has been trained using 10
companies for 100 training rounds. The results reveal a slight difference in
the global model's accuracy compared to the highest accuracy of the centralized
model, which can be ignored in favour of the global model's comprehensive
knowledge, lower training cost, preservation of data privacy, and avoidance of
the technical debt problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, Journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCau: A Proactive Stop Policy for Communication and Computation
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afsaneh Mahmoudi, Hossein S. Ghadikolaei, José Mairton Barros Da Silva Júnior, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates efficient distributed training of a Federated
Learning~(FL) model over a wireless network of wireless devices. The
communication iterations of the distributed training algorithm may be
substantially deteriorated or even blocked by the effects of the devices'
background traffic, packet losses, congestion, or latency. We abstract the
communication-computation impacts as an `iteration cost' and propose a
cost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an
iteration-termination method that trade-offs the training performance and
networking costs. We apply our approach when clients use the slotted-ALOHA, the
carrier-sense multiple access with collision avoidance~(CSMA/CA), and the
orthogonal frequency-division multiple access~(OFDMA) protocols. We show that,
given a total cost budget, the training performance degrades as either the
background communication traffic or the dimension of the training problem
increases. Our results demonstrate the importance of proactively designing
optimal cost-efficient stopping criteria to avoid unnecessary
communication-computation costs to achieve only a marginal FL training
improvement. We validate our method by training and testing FL over the MNIST
dataset. Finally, we apply our approach to existing communication efficient FL
methods from the literature, achieving further efficiency. We conclude that
cost-efficient stopping criteria are essential for the success of practical FL
over wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transport meets Variational Inference: Controlled Monte Carlo Diffusions <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01050v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01050v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Vargas, Shreyas Padhy, Denis Blessing, Nikolas Nüsken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connecting optimal transport and variational inference, we present a
principled and systematic framework for sampling and generative modelling
centred around divergences on path space. Our work culminates in the
development of the \emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for
Bayesian computation, a score-based annealing technique that crucially adapts
both forward and backward dynamics in a diffusion model. On the way, we clarify
the relationship between the EM-algorithm and iterative proportional fitting
(IPF) for Schr{\"o}dinger bridges, deriving as well a regularised objective
that bypasses the iterative bottleneck of standard IPF-updates. Finally, we
show that CMCD has a strong foundation in the Jarzinsky and Crooks identities
from statistical physics, and that it convincingly outperforms competing
approaches across a wide array of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on New Frontiers in Learning, Control, and Dynamical Systems
  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,
  USA, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A Dataset and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Implicit GNN Solver for Poisson-like problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Nastorg, Michele Alessandro Bucci, Thibault Faney, Jean-Marc Gratien, Guillaume Charpiat, Marc Schoenauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach
for solving the ubiquitous Poisson PDE problems with mixed boundary conditions.
By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep
network, thus avoiding the empirical tuning of the number of required Message
Passing layers to attain the solution. Its original architecture explicitly
takes into account the boundary conditions, a critical prerequisite for
physical applications, and is able to adapt to any initially provided solution.
$\Psi$-GNN is trained using a "physics-informed" loss, and the training process
is stable by design, and insensitive to its initialization. Furthermore, the
consistency of the approach is theoretically proven, and its flexibility and
generalization efficiency are experimentally demonstrated: the same learned
model can accurately handle unstructured meshes of various sizes, as well as
different boundary conditions. To the best of our knowledge, $\Psi$-GNN is the
first physics-informed GNN-based method that can handle various unstructured
domains, boundary conditions and initial solutions while also providing
convergence guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble learning for Physics Informed Neural Networks: a Gradient
  Boosting approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Fang, Sifan Wang, Paris Perdikaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the popularity of physics-informed neural networks (PINNs) is steadily
rising, to this date, PINNs have not been successful in simulating multi-scale
and singular perturbation problems. In this work, we present a new training
paradigm referred to as "gradient boosting" (GB), which significantly enhances
the performance of physics informed neural networks (PINNs). Rather than
learning the solution of a given PDE using a single neural network directly,
our algorithm employs a sequence of neural networks to achieve a superior
outcome. This approach allows us to solve problems presenting great challenges
for traditional PINNs. Our numerical experiments demonstrate the effectiveness
of our algorithm through various benchmarks, including comparisons with finite
element methods and PINNs. Furthermore, this work also unlocks the door to
employing ensemble learning techniques in PINNs, providing opportunities for
further improvement in solving PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian data-driven discovery of partial differential equations with
  variable coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoxue Chen, Yifan Du, Liyao Mars Gao, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of Partial Differential Equations (PDEs) is an essential task
for applied science and engineering. However, data-driven discovery of PDEs is
generally challenging, primarily stemming from the sensitivity of the
discovered equation to noise and the complexities of model selection. In this
work, we propose an advanced Bayesian sparse learning algorithm for PDE
discovery with variable coefficients, predominantly when the coefficients are
spatially or temporally dependent. Specifically, we apply threshold Bayesian
group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a
Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This
approach not only enhances the robustness of point estimation with valid
uncertainty quantification but also relaxes the computational burden from
Bayesian inference through the integration of coefficient thresholds as an
approximate MCMC method. Moreover, from the quantified uncertainties, we
propose a Bayesian total error bar criteria for model selection, which
outperforms classic metrics including the root mean square and the Akaike
information criterion. The capability of this method is illustrated by the
discovery of several classical benchmark PDEs with spatially or temporally
varying coefficients from solution data obtained from the reference
simulations. In the experiments, we show that the tBGL-SS method is more robust
than the baseline methods under noisy environments and provides better model
selection criteria along the regularization path.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Laplace Approximation with the Fisher Metric <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laplace's method approximates a target density with a Gaussian distribution
at its mode. It is computationally efficient and asymptotically exact for
Bayesian inference due to the Bernstein-von Mises theorem, but for complex
targets and finite-data posteriors it is often too crude an approximation. A
recent generalization of the Laplace Approximation transforms the Gaussian
approximation according to a chosen Riemannian geometry providing a richer
approximation family, while still retaining computational efficiency. However,
as shown here, its properties depend heavily on the chosen metric, indeed the
metric adopted in previous work results in approximations that are overly
narrow as well as being biased even at the limit of infinite data. We correct
this shortcoming by developing the approximation family further, deriving two
alternative variants that are exact at the limit of infinite data, extending
the theoretical analysis of the method, and demonstrating practical
improvements in a range of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024, with additional fixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DISL dataset features a collection of $514,506$ unique Solidity files
that have been deployed to Ethereum mainnet. It caters to the need for a large
and diverse dataset of real-world smart contracts. DISL serves as a resource
for developing machine learning systems and for benchmarking software
engineering tools designed for smart contracts. By aggregating every verified
smart contract from Etherscan up to January 15, 2024, DISL surpasses existing
datasets in size and recency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders Are Robust Neural Architecture Search Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Hu, Xiangxiang Chu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Lightweight and Gradient-Stable Neural Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.04088v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.04088v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyao Yu, Yin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance resource efficiency and model deployability of neural networks, we
propose a neural-layer architecture based on Householder weighting and
absolute-value activating, called Householder-absolute neural layer or simply
Han-layer. Compared to a fully connected layer with $d$-neurons and $d$
outputs, a Han-layer reduces the number of parameters and the corresponding
computational complexity from $O(d^2)$ to $O(d)$. {The Han-layer structure
guarantees that the Jacobian of the layer function is always orthogonal, thus
ensuring gradient stability (i.e., free of gradient vanishing or exploding
issues) for any Han-layer sub-networks.} Extensive numerical experiments show
that one can strategically use Han-layers to replace fully connected (FC)
layers, reducing the number of model parameters while maintaining or even
improving the generalization performance. We will also showcase the
capabilities of the Han-layer architecture on a few small stylized models, and
discuss its current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain Networks and Intelligence: A Graph Neural Network Based Approach
  to Resting State fMRI Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful
tool for investigating the relationship between brain function and cognitive
processes as it allows for the functional organization of the brain to be
captured without relying on a specific task or stimuli. In this paper, we
present a novel modeling architecture called BrainRGIN for predicting
intelligence (fluid, crystallized, and total intelligence) using graph neural
networks on rsfMRI derived static functional network connectivity matrices.
Extending from the existing graph convolution networks, our approach
incorporates a clustering-based embedding and graph isomorphism network in the
graph convolutional layer to reflect the nature of the brain sub-network
organization and efficient network expression, in combination with TopK pooling
and attention-based readout functions. We evaluated our proposed architecture
on a large dataset, specifically the Adolescent Brain Cognitive Development
Dataset, and demonstrated its effectiveness in predicting individual
differences in intelligence. Our model achieved lower mean squared errors and
higher correlation scores than existing relevant graph architectures and other
traditional machine learning models for all of the intelligence prediction
tasks. The middle frontal gyrus exhibited a significant contribution to both
fluid and crystallized intelligence, suggesting their pivotal role in these
cognitive processes. Total composite scores identified a diverse set of brain
regions to be relevant which underscores the complex nature of total
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All Rivers Run to the Sea: Private Learning with Asymmetric Flows <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data privacy is of great concern in cloud machine-learning service platforms,
when sensitive data are exposed to service providers. While private computing
environments (e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protection, their computing
performance still falls short compared to cloud GPUs. To achieve privacy
protection with high computing performance, we propose Delta, a new private
training and inference framework, with comparable model performance as
non-private centralized training. Delta features two asymmetric data flows: the
main information-sensitive flow and the residual flow. The main part flows into
a small model while the residuals are offloaded to a large model. Specifically,
Delta embeds the information-sensitive representations into a low-dimensional
space while pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small model in a private
environment. On the other hand, the residual part is sent to fast cloud GPUs,
and processed by a large model. To further enhance privacy and reduce the
communication cost, Delta applies a random binary quantization technique along
with a DP-based technique to the residuals before sharing them with the public
platform. We theoretically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity in the private
environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet
datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong
privacy protection, fast training, and inference without significantly
compromising the model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance <span class="highlight-title">Protein</span>
  Classification in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein classification tasks are essential in drug discovery. Real-world
protein structures are dynamic, which will determine the properties of
proteins. However, the existing machine learning methods, like ProNet (Wang et
al., 2022a), only access limited conformational characteristics and protein
side-chain features, leading to impractical protein structure and inaccuracy of
protein classes in their predictions. In this paper, we propose novel semantic
data augmentation methods, Novel Augmentation of New Node Attributes (NaNa),
and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate
backbone chemical and side-chain biophysical information into protein
classification tasks and a co-embedding residual learning framework.
Specifically, we leverage molecular biophysical, secondary structure, chemical
bonds, and ionic features of proteins to facilitate protein classification
tasks. Furthermore, our semantic augmentation methods and the co-embedding
residual learning framework can improve the performance of GIN (Xu et al.,
2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%
and 11.33% respectively. Our code is available at
https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Generation with $K^2$-trees <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19125v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19125v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Dongwoo Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs from a target distribution is a significant challenge
across many domains, including drug discovery and social network analysis. In
this work, we introduce a novel graph generation method leveraging $K^2$-tree
representation, originally designed for lossless graph compression. The
$K^2$-tree representation {encompasses inherent hierarchy while enabling
compact graph generation}. In addition, we make contributions by (1) presenting
a sequential $K^2$-treerepresentation that incorporates pruning, flattening,
and tokenization processes and (2) introducing a Transformer-based architecture
designed to generate the sequence by incorporating a specialized tree
positional encoding scheme. Finally, we extensively evaluate our algorithm on
four general and two molecular graph datasets to confirm its superiority for
graph generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Scalable Representation for Graph Generation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Seul Lee, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a surge of interest in employing neural networks for
graph generation, a fundamental statistical learning problem with critical
applications like molecule design and community analysis. However, most
approaches encounter significant limitations when generating large-scale
graphs. This is due to their requirement to output the full adjacency matrices
whose size grows quadratically with the number of nodes. In response to this
challenge, we introduce a new, simple, and scalable graph representation named
gap encoded edge list (GEEL) that has a small representation size that aligns
with the number of edges. In addition, GEEL significantly reduces the
vocabulary size by incorporating the gap encoding and bandwidth restriction
schemes. GEEL can be autoregressively generated with the incorporation of node
positional encoding, and we further extend GEEL to deal with attributed graphs
by designing a new grammar. Our findings reveal that the adoption of this
compact representation not only enhances scalability but also bolsters
performance by simplifying the graph generation process. We conduct a
comprehensive evaluation across ten non-attributed and two molecular graph
generation tasks, demonstrating the effectiveness of GEEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive consumption of marijuana can induce substantial psychological
and social consequences. In this investigation, we propose an elucidative
framework termed high-order graph attention neural networks (HOGANN) for the
classification of Marijuana addiction, coupled with an analysis of localized
brain network communities exhibiting abnormal activities among chronic
marijuana users. HOGANN integrates dynamic intrinsic functional brain networks,
estimated from resting-state functional magnetic resonance imaging (rs-fMRI),
using long short-term memory (LSTM) to capture temporal network dynamics. We
employ a high-order attention module for information fusion and message passing
among neighboring nodes, enhancing the network community analysis. Our model is
validated across two distinct data cohorts, yielding substantially higher
classification accuracy than benchmark algorithms. Furthermore, we discern the
most pertinent subnetworks and cognitive regions affected by persistent
marijuana consumption, indicating adverse effects on functional brain networks,
particularly within the dorsal attention and frontoparietal networks.
Intriguingly, our model demonstrates superior performance in cohorts exhibiting
prolonged dependence, implying that prolonged marijuana usage induces more
pronounced alterations in brain networks. The model proficiently identifies
craving brain maps, thereby delineating critical brain regions for analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained
vision-language models (VLMs) by constructing trainable prompts only for
composed state-object pairs. Relying on learning the joint representation of
seen compositions, these methods ignore the explicit modeling of the state and
object, thus limiting the exploitation of pre-trained knowledge and
generalization to unseen compositions. With a particular focus on the
universality of the solution, in this work, we propose a novel paradigm for
CZSL models that establishes three identification branches (i.e., Multi-Path)
to jointly model the state, object, and composition. The presented Troika is
our implementation that aligns the branch-specific prompt representations with
decomposed visual features. To calibrate the bias between semantically similar
multi-modal representations, we further devise a Cross-Modal Traction module
into Troika that shifts the prompt representation towards the current visual
content. We conduct extensive experiments on three popular benchmarks, where
our method significantly outperforms existing methods in both closed-world and
open-world settings. The code will be available at
https://github.com/bighuang624/Troika.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction Error Estimation in Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Krupkin, Johanna Hardin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, error estimates of classification Random Forests are
quantitatively assessed. Based on the initial theoretical framework built by
Bates et al. (2023), the true error rate and expected error rate are
theoretically and empirically investigated in the context of a variety of error
estimation methods common to Random Forests. We show that in the classification
case, Random Forests' estimates of prediction error is closer on average to the
true error rate instead of the average prediction error. This is opposite the
findings of Bates et al. (2023) which are given for logistic regression. We
further show that our result holds across different error estimation strategies
such as cross-validation, bagging, and data splitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2104.00673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
judgement, revealing that existing calibration methods aimed at mitigating
biases are insufficient for effectively aligning LLM evaluators. Inspired by
the use of preference data in RLHF, we formulate the evaluation as a ranking
problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided
search method that employs LLMs to conduct pairwise comparisons and efficiently
ranks candidate texts. PairS achieves state-of-the-art performance on
representative evaluation tasks and demonstrates significant improvements over
direct scoring. Furthermore, we provide insights into the role of pairwise
preference in quantifying the transitivity of LLMs and demonstrate how PairS
benefits from calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: https://github.com/xf-zhao/LoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  Transformer <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang, Sooyoung Yoon, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many mesh-based graph neural network (GNN) models have been
proposed for modeling complex high-dimensional physical systems. Remarkable
achievements have been made in significantly reducing the solving time compared
to traditional numerical solvers. These methods are typically designed to i)
reduce the computational cost in solving physical dynamics and/or ii) propose
techniques to enhance the solution accuracy in fluid and rigid body dynamics.
However, it remains under-explored whether they are effective in addressing the
challenges of flexible body dynamics, where instantaneous collisions occur
within a very short timeframe. In this paper, we present Hierarchical Contact
Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn
long-range dependencies (occurred by collisions) among spatially distant
positions of a body -- two close positions in a higher-level mesh correspond to
two distant positions in a lower-level mesh. HCMT enables long-range
interactions, and the hierarchical mesh structure quickly propagates collision
effects to faraway positions. To this end, it consists of a contact mesh
Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).
Lastly, we propose a flexible body dynamics dataset, consisting of trajectories
that reflect experimental settings frequently used in the display industry for
product designs. We also compare the performance of several baselines using
well-known benchmark datasets. Our results show that HCMT provides significant
performance improvements over existing methods. Our code is available at
https://github.com/yuyudeep/hcmt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPI++: Efficient Prediction-Powered Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios N. Angelopoulos, John C. Duchi, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PPI++: a computationally lightweight methodology for estimation
and inference based on a small labeled dataset and a typically much larger
dataset of machine-learning predictions. The methods automatically adapt to the
quality of available predictions, yielding easy-to-compute confidence sets --
for parameters of any dimensionality -- that always improve on classical
intervals using only the labeled data. PPI++ builds on prediction-powered
inference (PPI), which targets the same problem setting, improving its
computational and statistical efficiency. Real and synthetic experiments
demonstrate the benefits of the proposed adaptations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/aangelopoulos/ppi_py</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Opioid Use Disorder Risk Modelling through Behavioral and
  Genetic Feature Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sybille Légitime, Kaustubh Prabhu, Devin McConnell, Bing Wang, Dipak K. Dey, Derek Aguiar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opioids are an effective analgesic for acute and chronic pain, but also carry
a considerable risk of addiction leading to millions of opioid use disorder
(OUD) cases and tens of thousands of premature deaths in the United States
yearly. Estimating OUD risk prior to prescription could improve the efficacy of
treatment regimens, monitoring programs, and intervention strategies, but risk
estimation is typically based on self-reported data or questionnaires. We
develop an experimental design and computational methods that combine genetic
variants associated with OUD with behavioral features extracted from GPS and
Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility
and genetic data do not exist for the same cohort, we develop algorithms to (1)
generate mobility features from empirical distributions and (2) synthesize
mobility and genetic samples assuming an expected level of disease
co-occurrence. We show that integrating genetic and mobility modalities
improves risk modelling using classification accuracy, area under the
precision-recall and receiver operator characteristic curves, and $F_1$ score.
Interpreting the fitted models suggests that mobility features have more
influence on OUD risk, although the genetic contribution was significant,
particularly in linear models. While there exist concerns with respect to
privacy, security, bias, and generalizability that must be evaluated in
clinical trials before being implemented in practice, our framework provides
preliminary evidence that behavioral and genetic features may improve OUD risk
estimation to assist with personalized clinical decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages (including References section), 8 figures. Under review by
  PLOS One</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opinion Market Model: Stemming Far-Right Opinion Spread using Positive
  Interventions <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pio Calderon, Rohit Ram, Marian-Andrei Rizoiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online extremism has severe societal consequences, including normalizing hate
speech, user radicalization, and increased social divisions. Various mitigation
strategies have been explored to address these consequences. One such strategy
uses positive interventions: controlled signals that add attention to the
opinion ecosystem to boost certain opinions. To evaluate the effectiveness of
positive interventions, we introduce the Opinion Market Model (OMM), a two-tier
online opinion ecosystem model that considers both inter-opinion interactions
and the role of positive interventions. The size of the opinion attention
market is modeled in the first tier using the multivariate discrete-time Hawkes
process; in the second tier, opinions cooperate and compete for market share,
given limited attention using the market share attraction model. We demonstrate
the convergence of our proposed estimation scheme on a synthetic dataset. Next,
we test OMM on two learning tasks, applying to two real-world datasets to
predict attention market shares and uncover latent relationships between online
items. The first dataset comprises Facebook and Twitter discussions containing
moderate and far-right opinions about bushfires and climate change. The second
dataset captures popular VEVO artists' YouTube and Twitter attention volumes.
OMM outperforms the state-of-the-art predictive models on both datasets and
captures latent cooperation-competition relations. We uncover (1) self- and
cross-reinforcement between far-right and moderate opinions on the bushfires
and (2) pairwise artist relations that correlate with real-world interactions
such as collaborations and long-lasting feuds. Lastly, we use OMM as a testbed
for positive interventions and show how media coverage modulates the spread of
far-right opinions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in the 18th AAAI International Conference on Web and Social
  Media (ICWSM'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfIE: Self-Interpretation of Large Language Model Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Chen, Carl Vondrick, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do large language models (LLMs) obtain their answers? The ability to
explain and control an LLM's reasoning process is key for reliability,
transparency, and future model developments. We propose SelfIE
(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret
their own embeddings in natural language by leveraging their ability to respond
to inquiries about a given passage. Capable of interpreting open-world concepts
in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such
as making ethical decisions, internalizing prompt injection, and recalling
harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up
new avenues to control LLM reasoning. We propose Supervised Control, which
allows editing open-ended concepts while only requiring gradient computation of
individual layer. We extend RLHF to hidden embeddings and propose Reinforcement
Control that erases harmful knowledge in LLM without supervision targets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, Yogesh Balaji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite tremendous progress in generating high-quality images using diffusion
models, synthesizing a sequence of animated frames that are both photorealistic
and temporally coherent is still in its infancy. While off-the-shelf
billion-scale datasets for image generation are available, collecting similar
video data of the same scale is still challenging. Also, training a video
diffusion model is computationally much more expensive than its image
counterpart. In this work, we explore finetuning a pretrained image diffusion
model with video data as a practical solution for the video synthesis task. We
find that naively extending the image noise prior to video noise prior in video
diffusion leads to sub-optimal performance. Our carefully designed video noise
prior leads to substantially better performance. Extensive experimental
validation shows that our model, Preserve Your Own Correlation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It
also achieves SOTA video generation quality on the small-scale UCF-101
benchmark with a $10\times$ smaller model using significantly less computation
than the prior art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023. Project webpage:
  https://research.nvidia.com/labs/dir/pyoco</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Optimal Algorithms for Constrained k-Center Clustering with
  Instance-level Background Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu, Minhui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Center-based clustering has attracted significant research interest from both
theory and practice. In many practical applications, input data often contain
background knowledge that can be used to improve clustering results. In this
work, we build on widely adopted $k$-center clustering and model its input
background knowledge as must-link (ML) and cannot-link (CL) constraint sets.
However, most clustering problems including $k$-center are inherently
$\mathcal{NP}$-hard, while the more complex constrained variants are known to
suffer severer approximation and computation barriers that significantly limit
their applicability. By employing a suite of techniques including reverse
dominating sets, linear programming (LP) integral polyhedron, and LP duality,
we arrive at the first efficient approximation algorithm for constrained
$k$-center with the best possible ratio of 2. We also construct competitive
baseline algorithms and empirically evaluate our approximation algorithm
against them on a variety of real datasets. The results validate our
theoretical findings and demonstrate the great advantages of our algorithm in
terms of clustering cost, clustering quality, and running time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omega: Optimistic EMA Gradients <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Ramirez, Rohan Sukumaran, Quentin Bertrand, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic min-max optimization has gained interest in the machine learning
community with the advancements in GANs and adversarial training. Although game
optimization is fairly well understood in the deterministic setting, some
issues persist in the stochastic regime. Recent work has shown that stochastic
gradient descent-ascent methods such as the optimistic gradient are highly
sensitive to noise or can fail to converge. Although alternative strategies
exist, they can be prohibitively expensive. We introduce Omega, a method with
optimistic-like updates that mitigates the impact of noise by incorporating an
EMA of historic gradients in its update rule. We also explore a variation of
this algorithm that incorporates momentum. Although we do not provide
convergence guarantees, our experiments on stochastic games show that Omega
outperforms the optimistic gradient method when applied to linear players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral at the LatinX in AI workshop @ ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring CausalWorld: Enhancing robotic manipulation via knowledge
  transfer and curriculum learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a learning-based tri-finger robotic arm manipulating
task, which requires complex movements and coordination among the fingers. By
employing reinforcement learning, we train an agent to acquire the necessary
skills for proficient manipulation. To enhance the efficiency and effectiveness
of the learning process, two knowledge transfer strategies, fine-tuning and
curriculum learning, were utilized within the soft actor-critic architecture.
Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to
new tasks. Several variations like model transfer, policy transfer, and
across-task transfer were implemented and evaluated. To eliminate the need for
pretraining, curriculum learning decomposes the advanced task into simpler,
progressive stages, mirroring how humans learn. The number of learning stages,
the context of the sub-tasks, and the transition timing were found to be the
critical design parameters. The key factors of two learning strategies and
corresponding effects were explored in context-aware and context-unaware
scenarios, enabling us to identify the scenarios where the methods demonstrate
optimal performance, derive conclusive insights, and contribute to a broader
range of learning-based engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Negative Sampling on Graphs for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung-Kien Nguyen, Yuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is a fundamental task for graph analysis with important
applications on the Web, such as social network analysis and recommendation
systems, etc. Modern graph link prediction methods often employ a contrastive
approach to learn robust node representations, where negative sampling is
pivotal. Typical negative sampling methods aim to retrieve hard examples based
on either predefined heuristics or automatic adversarial approaches, which
might be inflexible or difficult to control. Furthermore, in the context of
link prediction, most previous methods sample negative nodes from existing
substructures of the graph, missing out on potentially more optimal samples in
the latent space. To address these issues, we investigate a novel strategy of
multi-level negative sampling that enables negative node generation with
flexible and controllable ``hardness'' levels from the latent space. Our
method, called Conditional Diffusion-based Multi-level Negative Sampling
(DMNS), leverages the Markov chain property of diffusion models to generate
negative nodes in multiple levels of variable hardness and reconcile them for
effective graph link prediction. We further demonstrate that DMNS follows the
sub-linear positivity principle for robust negative sampling. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of
DMNS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the TheWebConf 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manufacturing Service Capability Prediction with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqing Li, Xiaorui Liu, Binil Starly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current landscape, the predominant methods for identifying
manufacturing capabilities from manufacturers rely heavily on keyword matching
and semantic matching. However, these methods often fall short by either
overlooking valuable hidden information or misinterpreting critical data.
Consequently, such approaches result in an incomplete identification of
manufacturers' capabilities. This underscores the pressing need for data-driven
solutions to enhance the accuracy and completeness of manufacturing capability
identification. To address the need, this study proposes a Graph Neural
Network-based method for manufacturing service capability identification over a
knowledge graph. To enhance the identification performance, this work
introduces a novel approach that involves aggregating information from the
graph nodes' neighborhoods as well as oversampling the graph data, which can be
effectively applied across a wide range of practical scenarios. Evaluations
conducted on a Manufacturing Service Knowledge Graph and subsequent ablation
studies demonstrate the efficacy and robustness of the proposed approach. This
study not only contributes a innovative method for inferring manufacturing
service capabilities but also significantly augments the quality of
Manufacturing Service Knowledge Graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal and Semantic Evaluation Metrics for Foundation Models in
  Post-Hoc Analysis of Robotic Sub-tasks <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Salfity, Selma Wanna, Minkyu Choi, Mitch Pryor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in Task and Motion Planning (TAMP) show that training control
policies on language-supervised robot trajectories with quality labeled data
markedly improves agent task success rates. However, the scarcity of such data
presents a significant hurdle to extending these methods to general use cases.
To address this concern, we present an automated framework to decompose
trajectory data into temporally bounded and natural language-based descriptive
sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)
including both Large Language Models (LLMs) and Vision Language Models (VLMs).
Our framework provides both time-based and language-based descriptions for
lower-level sub-tasks that comprise full trajectories. To rigorously evaluate
the quality of our automatic labeling framework, we contribute an algorithm
SIMILARITY to produce two novel metrics, temporal similarity and semantic
similarity. The metrics measure the temporal alignment and semantic fidelity of
language descriptions between two sub-task decompositions, namely an FM
sub-task decomposition prediction and a ground-truth sub-task decomposition. We
present scores for temporal similarity and semantic similarity above 90%,
compared to 30% of a randomized baseline, for multiple robotic environments,
demonstrating the effectiveness of our proposed framework. Our results enable
building diverse, large-scale, language-supervised datasets for improved
robotic TAMP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures. IROS 2024 Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Image Compression with Quantization Rectifier <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Luo, Bo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural image compression has been shown to outperform traditional image
codecs in terms of rate-distortion performance. However, quantization
introduces errors in the compression process, which can degrade the quality of
the compressed image. Existing approaches address the train-test mismatch
problem incurred during quantization, the random impact of quantization on the
expressiveness of image features is still unsolved. This paper presents a novel
quantization rectifier (QR) method for image compression that leverages image
feature correlation to mitigate the impact of quantization. Our method designs
a neural network architecture that predicts unquantized features from the
quantized ones, preserving feature expressiveness for better image
reconstruction quality. We develop a soft-to-predictive training technique to
integrate QR into existing neural image codecs. In evaluation, we integrate QR
into state-of-the-art neural image codecs and compare enhanced models and
baselines on the widely-used Kodak benchmark. The results show consistent
coding efficiency improvement by QR with a negligible increase in the running
time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Machine Learning (ICML)
  Neural Compression Workshop 2023, Honolulu, Hawaii, USA. PMLR 202, 2023.
  Copyright 2023 by the authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling
  Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an active learning algorithm for learning dynamics that leverages
side information by explicitly incorporating prior domain knowledge into the
sampling process. Our proposed algorithm guides the exploration toward regions
that demonstrate high empirical discrepancy between the observed data and an
imperfect prior model of the dynamics derived from side information. Through
numerical experiments, we demonstrate that this strategy explores regions of
high discrepancy and accelerates learning while simultaneously reducing model
uncertainty. We rigorously prove that our active learning algorithm yields a
consistent estimate of the underlying dynamics by providing an explicit rate of
convergence for the maximum predictive variance. We demonstrate the efficacy of
our approach on an under-actuated pendulum system and on the half-cheetah
MuJoCo environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from
  Learned Hallucination <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a self-supervised learning method to safely learn a
motion planner for ground robots to navigate environments with dense and
dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict
obstacles, classical motion planners may not be able to keep up with limited
onboard computation. For learning-based planners, high-quality demonstrations
are difficult to acquire for imitation learning while reinforcement learning
becomes inefficient due to the high probability of collision during
exploration. To safely and efficiently provide training data, the Learning from
Hallucination (LfH) approaches synthesize difficult navigation environments
based on past successful navigation experiences in relatively easy or
completely open ones, but unfortunately cannot address dynamic obstacles. In
our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and
learn a novel latent distribution and sample dynamic obstacles from it, so the
generated training data can be used to learn a motion planner to navigate in
dynamic environments. Dyna-LfLH is evaluated on a ground robot in both
simulated and physical environments and achieves up to 25% better success rate
compared to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification for Gradient-based Explanations in Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Mulye, Matias Valdenegro-Toro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation methods help understand the reasons for a model's prediction.
These methods are increasingly involved in model debugging, performance
optimization, and gaining insights into the workings of a model. With such
critical applications of these methods, it is imperative to measure the
uncertainty associated with the explanations generated by these methods. In
this paper, we propose a pipeline to ascertain the explanation uncertainty of
neural networks by combining uncertainty estimation methods and explanation
methods. We use this pipeline to produce explanation distributions for the
CIFAR-10, FER+, and California Housing datasets. By computing the coefficient
of variation of these distributions, we evaluate the confidence in the
explanation and determine that the explanations generated using Guided
Backpropagation have low uncertainty associated with them. Additionally, we
compute modified pixel insertion/deletion metrics to evaluate the quality of
the generated explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Occurring of Object Detection and Identification towards unlabeled
  object discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binay Kumar Singh, Niels Da Vitoria Lobo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel deep learning based approach for
identifying co-occurring objects in conjunction with base objects in multilabel
object categories. Nowadays, with the advancement in computer vision based
techniques we need to know about co-occurring objects with respect to base
object for various purposes. The pipeline of the proposed work is composed of
two stages: in the first stage of the proposed model we detect all the bounding
boxes present in the image and their corresponding labels, then in the second
stage we perform co-occurrence matrix analysis. In co-occurrence matrix
analysis, we set base classes based on the maximum occurrences of the labels
and build association rules and generate frequent patterns. These frequent
patterns will show base classes and their corresponding co-occurring classes.
We performed our experiments on two publicly available datasets: Pascal VOC and
MS-COCO. The experimental results on public benchmark dataset is reported in
Sec 4. Further we extend this work by considering all frequently objects as
unlabeled and what if they are occluded as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of the Capabilities of Large Language Models for
  Vulnerability Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza Sanjida Alam, Earl T. Barr, Wei Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated great potential for code
generation and other software engineering tasks. Vulnerability detection is of
crucial importance to maintaining the security, integrity, and trustworthiness
of software systems. Precise vulnerability detection requires reasoning about
the code, making it a good case study for exploring the limits of LLMs'
reasoning capabilities. Although recent work has applied LLMs to vulnerability
detection using generic prompting techniques, their full capabilities for this
task and the types of errors they make when explaining identified
vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code
generation and commonly used as coding assistants, and evaluated their
capabilities for vulnerability detection. We systematically searched for the
best-performing prompts, incorporating techniques such as in-context learning
and chain-of-thought, and proposed three of our own prompting methods. Our
results show that while our prompting methods improved the models' performance,
LLMs generally struggled with vulnerability detection. They reported 0.5-0.63
Balanced Accuracy and failed to distinguish between buggy and fixed versions of
programs in 76% of cases on average. By comprehensively analyzing and
categorizing 287 instances of model reasoning, we found that 57% of LLM
responses contained errors, and the models frequently predicted incorrect
locations of buggy code and misidentified bug types. LLMs only correctly
localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted
correctly by 70-100% of human participants. These findings suggest that despite
their potential for other tasks, LLMs may fail to properly comprehend critical
code structures and security-related concepts. Our data and code are available
at https://figshare.com/s/78fe02e56e09ec49300b.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sanity Checks for Explanation Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Valdenegro-Toro, Mihir Mulye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations for machine learning models can be hard to interpret or be
wrong. Combining an explanation method with an uncertainty estimation method
produces explanation uncertainty. Evaluating explanation uncertainty is
difficult. In this paper we propose sanity checks for uncertainty explanation
methods, where a weight and data randomization tests are defined for
explanations with uncertainty, allowing for quick tests to combinations of
uncertainty and explanation methods. We experimentally show the validity and
effectiveness of these tests on the CIFAR10 and California Housing datasets,
noting that Ensembles seem to consistently pass both tests with Guided
Backpropagation, Integrated Gradients, and LIME explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADGL: Context-Aware Deep Graph Learning for Predicting <span class="highlight-title">Drug</span>-<span class="highlight-title">Drug</span>
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process
of drug development. DDIs occur when one drug's properties are affected by the
inclusion of other drugs. Detecting favorable DDIs has the potential to pave
the way for creating and advancing innovative medications applicable in
practical settings. However, existing DDI prediction models continue to face
challenges related to generalization in extreme cases, robust feature
extraction, and real-life application possibilities. We aim to address these
challenges by leveraging the effectiveness of context-aware deep graph learning
by introducing a novel framework named CADGL. Based on a customized variational
graph autoencoder (VGAE), we capture critical structural and physio-chemical
information using two context preprocessors for feature extraction from two
different perspectives: local neighborhood and molecular context, in a
heterogeneous graphical structure. Our customized VGAE consists of a graph
encoder, a latent information encoder, and an MLP decoder. CADGL surpasses
other state-of-the-art DDI prediction models, excelling in predicting
clinically valuable novel DDIs, supported by rigorous case studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures; In review in IEEE/ACM Transactions on
  Computational Biology and Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Intersection of Signal Processing and Machine Learning: A Use
  Case-Driven Analysis Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sulaiman Aburakhia, Abdallah Shami, George K. Karagiannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in sensing, measurement, and computing technologies have
significantly expanded the potential for signal-based applications, leveraging
the synergy between signal processing and Machine Learning (ML) to improve both
performance and reliability. This fusion represents a critical point in the
evolution of signal-based systems, highlighting the need to bridge the existing
knowledge gap between these two interdisciplinary fields. Despite many attempts
in the existing literature to bridge this gap, most are limited to specific
applications and focus mainly on feature extraction, often assuming extensive
prior knowledge in signal processing. This assumption creates a significant
obstacle for a wide range of readers. To address these challenges, this paper
takes an integrated article approach. It begins with a detailed tutorial on the
fundamentals of signal processing, providing the reader with the necessary
background knowledge. Following this, it explores the key stages of a standard
signal processing-based ML pipeline, offering an in-depth review of feature
extraction techniques, their inherent challenges, and solutions. Differing from
existing literature, this work offers an application-independent review and
introduces a novel classification taxonomy for feature extraction techniques.
Furthermore, it aims at linking theoretical concepts with practical
applications, and demonstrates this through two specific use cases: a
spectral-based method for condition monitoring of rolling bearings and a
wavelet energy analysis for epilepsy detection using EEG signals. In addition
to theoretical contributions, this work promotes a collaborative research
culture by providing a public repository of relevant Python and MATLAB signal
processing codes. This effort is intended to support collaborative research
efforts and ensure the reproducibility of the results presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Stroke Segmentation Using Deep Learning Models: A Comparative
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Soliman, Yousif Yousif, Ahmed Ibrahim, Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke segmentation plays a crucial role in the diagnosis and treatment of
stroke patients by providing spatial information about affected brain regions
and the extent of damage. Segmenting stroke lesions accurately is a challenging
task, given that conventional manual techniques are time consuming and prone to
errors. Recently, advanced deep models have been introduced for general medical
image segmentation, demonstrating promising results that surpass many state of
the art networks when evaluated on specific datasets. With the advent of the
vision Transformers, several models have been introduced based on them, while
others have aimed to design better modules based on traditional convolutional
layers to extract long-range dependencies like Transformers. The question of
whether such high-level designs are necessary for all segmentation cases to
achieve the best results remains unanswered. In this study, we selected four
types of deep models that were recently proposed and evaluated their
performance for stroke segmentation: a pure Transformer-based architecture
(DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention
mechanisms in their design, an advanced hybrid model that incorporates CNNs
with Transformers (FCT), and the well-known self-adaptive nnUNet framework with
its configuration based on given data. We examined their performance on two
publicly available datasets, and found that the nnUNet achieved the best
results with the simplest design among all. Revealing the robustness issue of
Transformers to such variabilities serves as a potential reason for their
weaker performance. Furthermore, nnUNet's success underscores the significant
impact of preprocessing and postprocessing techniques in enhancing segmentation
results, surpassing the focus solely on architectural designs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Samples Are All You Need For Social Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar JafariNodeh, Amir Ajorlou, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of social learning, where a group of
agents embedded in a social network are interested in learning an underlying
state of the world. Agents have incomplete, noisy, and heterogeneous sources of
information, providing them with recurring private observations of the
underlying state of the world. Agents can share their learning experience with
their peers by taking actions observable to them, with values from a finite
feasible set of states. Actions can be interpreted as samples from the beliefs
which agents may form and update on what the true state of the world is.
Sharing samples, in place of full beliefs, is motivated by the limited
communication, cognitive, and information-processing resources available to
agents especially in large populations. Previous work (Salhab et al.) poses the
question as to whether learning with probability one is still achievable if
agents are only allowed to communicate samples from their beliefs. We provide a
definite positive answer to this question, assuming a strongly connected
network and a ``collective distinguishability'' assumption, which are both
required for learning even in full-belief-sharing settings. In our proposed
belief update mechanism, each agent's belief is a normalized weighted geometric
interpolation between a fully Bayesian private belief -- aggregating
information from the private source -- and an ensemble of empirical
distributions of the samples shared by her neighbors over time. By carefully
constructing asymptotic almost-sure lower/upper bounds on the frequency of
shared samples matching the true state/or not, we rigorously prove the
convergence of all the beliefs to the true state, with probability one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Quality-Diversity for Crystal Structure Prediction <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Janmohamed, Marta Wolinska, Shikha Surana, Thomas Pierrot, Aron Walsh, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal structures are indispensable across various domains, from batteries
to solar cells, and extensive research has been dedicated to predicting their
properties based on their atomic configurations. However, prevailing Crystal
Structure Prediction methods focus on identifying the most stable solutions
that lie at the global minimum of the energy function. This approach overlooks
other potentially interesting materials that lie in neighbouring local minima
and have different material properties such as conductivity or resistance to
deformation. By contrast, Quality-Diversity algorithms provide a promising
avenue for Crystal Structure Prediction as they aim to find a collection of
high-performing solutions that have diverse characteristics. However, it may
also be valuable to optimise for the stability of crystal structures alongside
other objectives such as magnetism or thermoelectric efficiency. Therefore, in
this work, we harness the power of Multi-Objective Quality-Diversity algorithms
in order to find crystal structures which have diverse features and achieve
different trade-offs of objectives. We analyse our approach on 5 crystal
systems and demonstrate that it is not only able to re-discover known real-life
structures, but also find promising new ones. Moreover, we propose a method for
illuminating the objective space to gain an understanding of what trade-offs
can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted GECCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less Is More -- On the Importance of Sparsification for Transformers and
  Graph Neural Networks for TSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Attila Lischka, Jiaming Wu, Rafael Basso, Morteza Haghir Chehreghani, Balázs Kulcsár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the recent studies tackling routing problems like the Traveling
Salesman Problem (TSP) with machine learning use a transformer or Graph Neural
Network (GNN) based encoder architecture. However, many of them apply these
encoders naively by allowing them to aggregate information over the whole TSP
instances. We, on the other hand, propose a data preprocessing method that
allows the encoders to focus on the most relevant parts of the TSP instances
only. In particular, we propose graph sparsification for TSP graph
representations passed to GNNs and attention masking for TSP instances passed
to transformers where the masks correspond to the adjacency matrices of the
sparse TSP graph representations. Furthermore, we propose ensembles of
different sparsification levels allowing models to focus on the most promising
parts while also allowing information flow between all nodes of a TSP instance.
In the experimental studies, we show that for GNNs appropriate sparsification
and ensembles of different sparsification levels lead to substantial
performance increases of the overall architecture. We also design a new,
state-of-the-art transformer encoder with ensembles of attention masking. These
transformers increase model performance from a gap of $0.16\%$ to $0.10\%$ for
TSP instances of size 100 and from $0.02\%$ to $0.00\%$ for TSP instances of
size 50.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximation with Random Shallow ReLU Networks with Applications to
  Model Reference Adaptive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski, Tyler Lekang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are regularly employed in adaptive control of nonlinear
systems and related methods o reinforcement learning. A common architecture
uses a neural network with a single hidden layer (i.e. a shallow network), in
which the weights and biases are fixed in advance and only the output layer is
trained. While classical results show that there exist neural networks of this
type that can approximate arbitrary continuous functions over bounded regions,
they are non-constructive, and the networks used in practice have no
approximation guarantees. Thus, the approximation properties required for
control with neural networks are assumed, rather than proved. In this paper, we
aim to fill this gap by showing that for sufficiently smooth functions, ReLU
networks with randomly generated weights and biases achieve $L_{\infty}$ error
of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It
suffices to generate the weights uniformly over a sphere and the biases
uniformly over an interval. We show how the result can be used to get
approximations of required accuracy in a model reference adaptive control
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review for Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Generalization of Cancer Clinical Trial Eligibility
  Classifiers Across Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Yang, Ashley Gilliam, Ethan B Ludmir, Kirk Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical trials are pivotal in medical research, and NLP can enhance their
success, with application in recruitment. This study aims to evaluate the
generalizability of eligibility classification across a broad spectrum of
clinical trials. Starting with phase 3 cancer trials, annotated with seven
eligibility exclusions, then to determine how well models can generalize to
non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility
criteria data for five types of trials: (1) additional phase 3 cancer trials,
(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes
trials, and (5) observational trials for any disease, comprising 2,490
annotated eligibility criteria across seven exclusion types. Our results show
that models trained on the extensive cancer dataset can effectively handle
criteria commonly found in non-cancer trials, such as autoimmune diseases.
However, they struggle with criteria disproportionately prevalent in cancer
trials, like prior malignancy. We also experiment with few-shot learning,
demonstrating that a limited number of disease-specific examples can partially
overcome this performance gap. We are releasing this new dataset of annotated
eligibility statements to promote the development of cross-disease
generalization in clinical trial classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the potential of prototype-based soft-labels data distillation
  for imbalanced data classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu-Andrei Rosu, Mihaela-Elena Breaban, Henri Luchian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims at synthesizing a dataset by a small number of
artificially generated data items, which, when used as training data, reproduce
or approximate a machine learning (ML) model as if it were trained on the
entire original dataset. Consequently, data distillation methods are usually
tied to a specific ML algorithm. While recent literature deals mainly with
distillation of large collections of images in the context of neural network
models, tabular data distillation is much less represented and mainly focused
on a theoretical perspective. The current paper explores the potential of a
simple distillation technique previously proposed in the context of
Less-than-one shot learning. The main goal is to push further the performance
of prototype-based soft-labels distillation in terms of classification
accuracy, by integrating optimization steps in the distillation process. The
analysis is performed on real-world data sets with various degrees of
imbalance. Experimental studies trace the capability of the method to distill
the data, but also the opportunity to act as an augmentation method, i.e. to
generate new data that is able to increase model accuracy when used in
conjunction with - as opposed to instead of - the original data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Language Plans in Demonstrations Through Counterfactual
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding the common-sense reasoning of Large Language Models in physical
domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior
works have focused on leveraging LLMs directly for planning in symbolic spaces,
this work uses LLMs to guide the search of task structures and constraints
implicit in multi-step demonstrations. Specifically, we borrow from
manipulation planning literature the concept of mode families, which group
robot configurations by specific motion constraints, to serve as an abstraction
layer between the high-level language representations of an LLM and the
low-level physical trajectories of a robot. By replaying a few human
demonstrations with synthetic perturbations, we generate coverage over the
demonstrations' state space with additional successful executions as well as
counterfactuals that fail the task. Our explanation-based learning framework
trains an end-to-end differentiable neural network to predict successful
trajectories from failures and as a by-product learns classifiers that ground
low-level states and images in mode families without dense labeling. The
learned grounding classifiers can further be used to translate language plans
into reactive policies in the physical domain in an interpretable manner. We
show our approach improves the interpretability and reactivity of imitation
learning through 2D navigation and simulated and real robot manipulation tasks.
Website: https://sites.google.com/view/grounding-plans
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Gradient Langevin Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ``The right to be forgotten'' ensured by laws for user data privacy becomes
increasingly important. Machine unlearning aims to efficiently remove the
effect of certain data points on the trained model parameters so that it can be
approximately the same as if one retrains the model from scratch. This work
proposes stochastic gradient Langevin unlearning, the first unlearning
framework based on noisy stochastic gradient descent (SGD) with privacy
guarantees for approximate unlearning problems under convexity assumption. Our
results show that mini-batch gradient updates provide a superior
privacy-complexity trade-off compared to the full-batch counterpart. There are
numerous algorithmic benefits of our unlearning approach, including complexity
saving compared to retraining, and supporting sequential and batch unlearning.
To examine the privacy-utility-complexity trade-off of our method, we conduct
experiments on benchmark datasets compared against prior works. Our approach
achieves a similar utility under the same privacy constraint while using $2\%$
and $10\%$ of the gradient computations compared with the state-of-the-art
gradient-based approximate unlearning methods for mini-batch and full-batch
settings, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2401.10371</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end
  Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Xie, Henglu Wei, Zhenyi Liu, Xiaoyu Wang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To advance research in learning-based defogging algorithms, various synthetic
fog datasets have been developed. However, existing datasets created using the
Atmospheric Scattering Model (ASM) or real-time rendering engines often
struggle to produce photo-realistic foggy images that accurately mimic the
actual imaging process. This limitation hinders the effective generalization of
models from synthetic to real data. In this paper, we introduce an end-to-end
simulation pipeline designed to generate photo-realistic foggy images. This
pipeline comprehensively considers the entire physically-based foggy scene
imaging process, closely aligning with real-world image capture methods. Based
on this pipeline, we present a new synthetic fog dataset named SynFog, which
features both sky light and active lighting conditions, as well as three levels
of fog density. Experimental results demonstrate that models trained on SynFog
exhibit superior performance in visual perception and detection accuracy
compared to others when applied to real-world foggy images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep
  Learning and Explainable AI Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs),
the utmost importance lies in guaranteeing resilient and lucid security
measures. This study highlights the necessity of implementing a Zero Trust
Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs),
hence departing from conventional perimeter defences that may expose
vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous
and continuous process of authenticating all network entities and
communications. The accuracy of our methodology in detecting and identifying
unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio
Frequency (RF) signals within a Deep Learning framework, a unique method.
Precise identification is crucial in Zero Trust Architecture (ZTA), as it
determines network access. In addition, the use of eXplainable Artificial
Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local
Interpretable Model-agnostic Explanations (LIME) contributes to the improvement
of the model's transparency and interpretability. Adherence to Zero Trust
Architecture (ZTA) standards guarantees that the classifications of unmanned
aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security
within the UAV field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning: Role of State Aggregation and Trajectory
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Jia, Alexander Rakhlin, Ayush Sekhari, Chen-Yu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the problem of offline reinforcement learning with value function
realizability but without Bellman completeness. Previous work by Xie and Jiang
(2021) and Foster et al. (2022) left open the question whether a bounded
concentrability coefficient along with trajectory-based offline data admits a
polynomial sample complexity. In this work, we provide a negative answer to
this question for the task of offline policy evaluation. In addition to
addressing this question, we provide a rather complete picture for offline
policy evaluation with only value function realizability. Our primary findings
are threefold: 1) The sample complexity of offline policy evaluation is
governed by the concentrability coefficient in an aggregated Markov Transition
Model jointly determined by the function class and the offline data
distribution, rather than that in the original MDP. This unifies and
generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The
concentrability coefficient in the aggregated Markov Transition Model may grow
exponentially with the horizon length, even when the concentrability
coefficient in the original MDP is small and the offline data is admissible
(i.e., the data distribution equals the occupancy measure of some policy), 3)
Under value function realizability, there is a generic reduction that can
convert any hard instance with admissible data to a hard instance with
trajectory data, implying that trajectory data offers no extra benefits over
admissible data. These three pieces jointly resolve the open problem, though
each of them could be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study in Dataset Pruning for Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Federico Raue, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image Super-Resolution (SR), relying on large datasets for training is a
double-edged sword. While offering rich training material, they also demand
substantial computational and storage resources. In this work, we analyze
dataset pruning as a solution to these challenges. We introduce a novel
approach that reduces a dataset to a core-set of training samples, selected
based on their loss values as determined by a simple pre-trained SR model. By
focusing the training on just 50% of the original dataset, specifically on the
samples characterized by the highest loss values, we achieve results comparable
to or even surpassing those obtained from training on the entire dataset.
Interestingly, our analysis reveals that the top 5% of samples with the highest
loss values negatively affect the training process. Excluding these samples and
adjusting the selection to favor easier samples further enhances training
outcomes. Our work opens new perspectives to the untapped potential of dataset
pruning in image SR. It suggests that careful selection of training data based
on loss-value metrics can lead to better SR models, challenging the
conventional wisdom that more data inevitably leads to better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning on Blockchain Data: A Systematic Mapping Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Palaiokrassas, Sarah Bouraga, Leandros Tassiulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Blockchain technology has drawn growing attention in the literature
and in practice. Blockchain technology generates considerable amounts of data
and has thus been a topic of interest for Machine Learning (ML).
  Objective: The objective of this paper is to provide a comprehensive review
of the state of the art on machine learning applied to blockchain data. This
work aims to systematically identify, analyze, and classify the literature on
ML applied to blockchain data. This will allow us to discover the fields where
more effort should be placed in future research.
  Method: A systematic mapping study has been conducted to identify the
relevant literature. Ultimately, 159 articles were selected and classified
according to various dimensions, specifically, the domain use case, the
blockchain, the data, and the machine learning models.
  Results: The majority of the papers (49.7%) fall within the Anomaly use case.
Bitcoin (47.2%) was the blockchain that drew the most attention. A dataset
consisting of more than 1.000.000 data points was used by 31.4% of the papers.
And Classification (46.5%) was the ML task most applied to blockchain data.
  Conclusion: The results confirm that ML applied to blockchain data is a
relevant and a growing topic of interest both in the literature and in
practice. Nevertheless, some open challenges and gaps remain, which can lead to
future research directions. Specifically, we identify novel machine learning
algorithms, the lack of a standardization framework, blockchain scalability
issues and cross-chain interactions as areas worth exploring in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous, Subject-Specific Attribute Control in T2I Models by
  Identifying Semantic Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advances in text-to-image (T2I) diffusion models have
substantially elevated the quality of their generated images. However,
achieving fine-grained control over attributes remains a challenge due to the
limitations of natural language prompts (such as no continuous set of
intermediate descriptions existing between ``person'' and ``old person''). Even
though many methods were introduced that augment the model or generation
process to enable such control, methods that do not require a fixed reference
image are limited to either enabling global fine-grained attribute expression
control or coarse attribute expression control localized to specific subjects,
not both simultaneously. We show that there exist directions in the commonly
used token-level CLIP text embeddings that enable fine-grained subject-specific
control of high-level attributes in text-to-image models. Based on this
observation, we introduce one efficient optimization-free and one robust
optimization-based method to identify these directions for specific attributes
from contrastive text prompts. We demonstrate that these directions can be used
to augment the prompt text input with fine-grained control over attributes of
specific subjects in a compositional manner (control over multiple attributes
of a single subject) without having to adapt the diffusion model. Project page:
https://compvis.github.io/attribute-control. Code is available at
https://github.com/CompVis/attribute-control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://compvis.github.io/attribute-control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkits are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 37 pages, 8 figures, 11 tables; Code at
  https://github.com/ldkong1205/Calib3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Rectified Flow: Advancing Diffusion Language Generation with
  Probabilistic Flows <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have demonstrated success in controlling sentence attributes
($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the
diffusion language model. A key component that drives theimpressive performance
for generating high-quality samples from noise is iteratively denoise for
thousands of steps. While beneficial, the complexity of starting from the noise
and the learning steps has limited its implementation to many NLP real-world
applications. This paper proposes Language Rectified Flow ({\ours}). Our method
is based on the reformulation of the standard probabilistic flow models.
Language rectified flow learns (neural) ordinary differential equation models
to transport between the source distribution and the target distribution, hence
providing a unified and effective solution to generative modeling and domain
transfer. From the source distribution, our language rectified flow yields fast
simulation and effectively decreases the inference time. Experiments on three
challenging fine-grained control tasks and multiple high-quality text editing
show that our method consistently outperforms its baselines. Extensive
experiments and ablation studies demonstrate that our method can be general,
effective, and beneficial for many NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be Yourself: Bounded Attention for Multi-Subject Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Dahary, Or Patashnik, Kfir Aberman, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have an unprecedented ability to generate
diverse and high-quality images. However, they often struggle to faithfully
capture the intended semantics of complex input prompts that include multiple
subjects. Recently, numerous layout-to-image extensions have been introduced to
improve user control, aiming to localize subjects represented by specific
tokens. Yet, these methods often produce semantically inaccurate images,
especially when dealing with multiple semantically or visually similar
subjects. In this work, we study and analyze the causes of these limitations.
Our exploration reveals that the primary issue stems from inadvertent semantic
leakage between subjects in the denoising process. This leakage is attributed
to the diffusion model's attention layers, which tend to blend the visual
features of different subjects. To address these issues, we introduce Bounded
Attention, a training-free method for bounding the information flow in the
sampling process. Bounded Attention prevents detrimental leakage among subjects
and enables guiding the generation to promote each subject's individuality,
even with complex multi-subject conditioning. Through extensive
experimentation, we demonstrate that our method empowers the generation of
multiple subjects that better align with given prompts and layouts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://omer11a.github.io/bounded-attention/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Relative Representations for Goal-Oriented Semantic
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Fiorellino, Claudio Battiloro, Emilio Calvanese Strinati, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In future 6G wireless networks, semantic and effectiveness aspects of
communications will play a fundamental role, incorporating meaning and
relevance into transmissions. However, obstacles arise when devices employ
diverse languages, logic, or internal representations, leading to semantic
mismatches that might jeopardize understanding. In latent space communication,
this challenge manifests as misalignment within high-dimensional
representations where deep neural networks encode data. This paper presents a
novel framework for goal-oriented semantic communication, leveraging relative
representations to mitigate semantic mismatches via latent space alignment. We
propose a dynamic optimization strategy that adapts relative representations,
communication parameters, and computation resources for energy-efficient,
low-latency, goal-oriented semantic communications. Numerical results
demonstrate our methodology's effectiveness in mitigating mismatches among
devices, while optimizing energy consumption, delay, and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution
  Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Ben Sahel, Yonina C. Eldar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of fluorescent molecules to create long sequences of low-density,
diffraction-limited images enables highly-precise molecule localization.
However, this methodology requires lengthy imaging times, which limits the
ability to view dynamic interactions of live cells on short time scales. Many
techniques have been developed to reduce the number of frames needed for
localization, from classic iterative optimization to deep neural networks.
Particularly, deep algorithm unrolling utilizes both the structure of iterative
sparse recovery algorithms and the performance gains of supervised deep
learning. However, the robustness of this approach is highly dependant on
having sufficient training data. In this paper we introduce deep unrolled
self-supervised learning, which alleviates the need for such data by training a
sequence-specific, model-based autoencoder that learns only from given
measurements. Our proposed method exceeds the performance of its supervised
counterparts, thus allowing for robust, dynamic imaging well below the
diffraction limit without any labeled training samples. Furthermore, the
suggested model-based autoencoder scheme can be utilized to enhance
generalization in any sparse recovery framework, without the need for external
training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David Harwath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VoiceCraft, a token infilling neural codec language model, that
achieves state-of-the-art performance on both speech editing and zero-shot
text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft
employs a Transformer decoder architecture and introduces a token rearrangement
procedure that combines causal masking and delayed stacking to enable
generation within an existing sequence. On speech editing tasks, VoiceCraft
produces edited speech that is nearly indistinguishable from unedited
recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,
our model outperforms prior SotA models including VALLE and the popular
commercial model XTTS-v2. Crucially, the models are evaluated on challenging
and realistic datasets, that consist of diverse accents, speaking styles,
recording conditions, and background noise and music, and our model performs
consistently well compared to other models and real recordings. In particular,
for speech editing evaluation, we introduce a high quality, challenging, and
realistic dataset named RealEdit. We encourage readers to listen to the demos
at https://jasonppy.github.io/VoiceCraft_web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data, code, and model weights are available at
  https://github.com/jasonppy/VoiceCraft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint chest X-ray diagnosis and clinical visual attention prediction
  with multi-stage cooperative learning: enhancing interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Qiu, Hassan Rivaz, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning has become the state-of-the-art for computer-assisted
diagnosis, interpretability of the automatic decisions is crucial for clinical
deployment. While various methods were proposed in this domain, visual
attention maps of clinicians during radiological screening offer a unique asset
to provide important insights and can potentially enhance the quality of
computer-assisted diagnosis. With this paper, we introduce a novel
deep-learning framework for joint disease diagnosis and prediction of
corresponding visual saliency maps for chest X-ray scans. Specifically, we
designed a novel dual-encoder multi-task UNet, which leverages both a
DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based
encoder to extract diverse features for saliency map prediction, and a
multi-scale feature-fusion classifier to perform disease classification. To
tackle the issue of asynchronous training schedules of individual tasks in
multi-task learning, we proposed a multi-stage cooperative learning strategy,
with contrastive learning for feature encoder pretraining to boost performance.
Experiments show that our proposed method outperformed existing techniques for
chest X-ray diagnosis and the quality of visual saliency map prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Mixing Laws: Optimizing Data Mixtures by Predicting Language
  Modeling Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining data of large language models composes multiple domains (e.g.,
web texts, academic papers, codes), whose mixture proportions crucially impact
the competence of outcome models. While existing endeavors rely on heuristics
or qualitative strategies to tune the proportions, we discover the quantitative
predictability of model performance regarding the mixture proportions in
function forms, which we refer to as the data mixing laws. Fitting such
functions on sample mixtures unveils model performance on unseen mixtures
before actual runs, thus guiding the selection of an ideal data mixture.
Furthermore, we propose nested use of the scaling laws of training steps, model
sizes, and our data mixing law to enable predicting the performance of large
models trained on massive data under various mixtures with only small-scale
training. Moreover, experimental results verify that our method effectively
optimizes the training mixture of a 1B model trained for 100B tokens in
RedPajama, reaching a performance comparable to the one trained for 48% more
steps on the default mixture. Extending the application of data mixing laws to
continual training accurately predicts the critical mixture proportion that
avoids catastrophic forgetting and outlooks the potential for dynamic data
schedules
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backpropagation through space, time, and the brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective learning in neuronal networks requires the adaptation of individual
synapses given their relative contribution to solving a task. However, physical
neuronal systems -- whether biological or artificial -- are constrained by
spatio-temporal locality. How such networks can perform efficient credit
assignment, remains, to a large extent, an open question. In Machine Learning,
the answer is almost universally given by the error backpropagation algorithm,
through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely
on biologically implausible assumptions, in particular with respect to
spatiotemporal (non-)locality, while forward-propagation models such as
real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.
We introduce Generalized Latent Equilibrium (GLE), a computational framework
for fully local spatio-temporal credit assignment in physical, dynamical
networks of neurons. We start by defining an energy based on neuron-local
mismatches, from which we derive both neuronal dynamics via stationarity and
parameter dynamics via gradient descent. The resulting dynamics can be
interpreted as a real-time, biologically plausible approximation of BPTT in
deep cortical networks with continuous-time neuronal dynamics and continuously
active, local synaptic plasticity. In particular, GLE exploits the ability of
biological neurons to phase-shift their output rate with respect to their
membrane potential, which is essential in both directions of information
propagation. For the forward computation, it enables the mapping of
time-continuous inputs to neuronal space, performing an effective
spatiotemporal convolution. For the backward computation, it permits the
temporal inversion of feedback signals, which consequently approximate the
adjoint states necessary for useful parameter updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) provides a privacy-preserving mechanism for
distributed training of machine learning models on networked devices (e.g.,
mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the
edge by creating models without sharing the actual data across the network.
Existing research works typically focus on generic aspects of non-IID data and
heterogeneity in client's system characteristics, but they often neglect the
issue of insufficient data for model development, which can arise from uneven
class label distribution and highly variable data volumes across edge nodes. In
this work, we propose FLIGAN, a novel approach to address the issue of data
incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs)
to adeptly capture complex data distributions and generate synthetic data that
closely resemble the real-world data. Then, we use synthetic data to enhance
the robustness and completeness of datasets across nodes. Our methodology
adheres to FL's privacy requirements by generating synthetic data in a
federated manner without sharing the actual data in the process. We incorporate
techniques such as classwise sampling and node grouping, designed to improve
the federated GAN's performance, enabling the creation of high-quality
synthetic datasets and facilitating efficient FL training. Empirical results
from our experiments demonstrate that FLIGAN significantly improves the model
accuracy, especially in scenarios with high class imbalances, achieving up to a
20% increase in model accuracy over traditional FL baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOD: From Heuristics to Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Franc, Jakub Paplham, Daniel Prusa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of designing reliable prediction models that
abstain from predictions when faced with uncertain or out-of-distribution
samples - a recently proposed problem known as Selective Classification in the
presence of Out-of-Distribution data (SCOD). We make three key contributions to
SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes
classifier for in-distribution (ID) data and a selector represented as a
stochastic linear classifier in a 2D space, using i) the conditional risk of
the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution
(OOD) data as input. This contrasts with suboptimal strategies from current OOD
detection methods and the Softmax Information Retaining Combination (SIRC),
specifically developed for SCOD. Secondly, we establish that in a
distribution-free setting, the SCOD problem is not Probably Approximately
Correct learnable when relying solely on an ID data sample. Third, we introduce
POSCOD, a simple method for learning a plugin estimate of the optimal SCOD
strategy from both an ID data sample and an unlabeled mixture of ID and OOD
data. Our empirical results confirm the theoretical findings and demonstrate
that our proposed method, POSCOD, out performs existing OOD methods in
effectively addressing the SCOD problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Space Models as Foundation Models: A Control Theoretic Overview 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a growing interest in integrating linear
state-space models (SSM) in deep neural network architectures of foundation
models. This is exemplified by the recent success of Mamba, showing better
performance than the state-of-the-art Transformer architectures in language
tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a
latent space in order to learn a compressed representation of the data. The
same goal has been pursued by control theorists using SSMs to efficiently model
dynamical systems. Therefore, SSMs can be naturally connected to deep sequence
modeling, offering the opportunity to create synergies between the
corresponding research areas. This paper is intended as a gentle introduction
to SSM-based architectures for control theorists and summarizes the latest
research developments. It provides a systematic review of the most successful
SSM proposals and highlights their main features from a control theoretic
perspective. Additionally, we present a comparative analysis of these models,
evaluating their performance on a standardized benchmark designed for assessing
a model's efficiency at learning long sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Robust Score-Based Diffusion Posterior Sampling for
  Plug-and-Play Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Xu, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a great number of tasks in science and engineering, the goal is to infer
an unknown image from a small number of measurements collected from a known
forward model describing certain sensing or imaging modality. Due to resource
constraints, this task is often extremely ill-posed, which necessitates the
adoption of expressive prior information to regularize the solution space.
Score-based diffusion models, due to its impressive empirical success, have
emerged as an appealing candidate of an expressive prior in image
reconstruction. In order to accommodate diverse tasks at once, it is of great
interest to develop efficient, consistent and robust algorithms that
incorporate {\em unconditional} score functions of an image prior distribution
in conjunction with flexible choices of forward models.
  This work develops an algorithmic framework for employing score-based
diffusion models as an expressive data prior in general nonlinear inverse
problems. Motivated by the plug-and-play framework in the imaging community, we
introduce a diffusion plug-and-play method (\textsf{DPnP}) that alternatively
calls two samplers, a proximal consistency sampler based solely on the
likelihood function of the forward model, and a denoising diffusion sampler
based solely on the score functions of the image prior. The key insight is that
denoising under white Gaussian noise can be solved {\em rigorously} via both
stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using
the unconditional score functions. We establish both asymptotic and
non-asymptotic performance guarantees of \textsf{DPnP}, and provide numerical
experiments to illustrate its promise in solving both linear and nonlinear
image reconstruction tasks. To the best of our knowledge, \textsf{DPnP} is the
first provably-robust posterior sampling method for nonlinear inverse problems
using unconditional diffusion priors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Latent Graph Generative Modeling with Diffusion Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning graph generative models over latent spaces has received less
attention compared to models that operate on the original data space and has so
far demonstrated lacklustre performance. We present GLAD a latent space graph
generative model. Unlike most previous latent space graph generative models,
GLAD operates on a discrete latent space that preserves to a significant extent
the discrete nature of the graph structures making no unnatural assumptions
such as latent space continuity. We learn the prior of our discrete latent
space by adapting diffusion bridges to its structure. By operating over an
appropriately constructed latent space we avoid relying on decompositions that
are often used in models that operate in the original data space. We present
experiments on a series of graph benchmark datasets which clearly show the
superiority of the discrete latent space and obtain state of the art graph
generative performance, making GLAD the first latent space graph generative
model with competitive performance. Our source code is published at:
\url{https://github.com/v18nguye/GLAD}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proprioception Is All You Need: Terrain Classification for Boreal
  Forests <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in field robotics highlighted the importance of resiliency
against different types of terrains. Boreal forests, in particular, are home to
many mobility-impeding terrains that should be considered for off-road
autonomous navigation. Also, being one of the largest land biomes on Earth,
boreal forests are an area where autonomous vehicles are expected to become
increasingly common. In this paper, we address this issue by introducing
BorealTC, a publicly available dataset for proprioceptive-based terrain
classification (TC). Recorded with a Husky A200, our dataset contains 116 min
of Inertial Measurement Unit (IMU), motor current, and wheel odometry data,
focusing on typical boreal forest terrains, notably snow, ice, and silty loam.
Combining our dataset with another dataset from the state-of-the-art, we
evaluate both a Convolutional Neural Network (CNN) and the novel state space
model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that
while CNN outperforms Mamba on each separate dataset, Mamba achieves greater
accuracy when trained on a combination of both. In addition, we demonstrate
that Mamba's learning capacity is greater than a CNN for increasing amounts of
data. We show that the combination of two TC datasets yields a latent space
that can be interpreted with the properties of the terrains. We also discuss
the implications of merging datasets on classification. Our source code and
dataset are publicly available online:
https://github.com/norlab-ulaval/BorealTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Off-Policy Prediction for Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Kuipers, Renukanandan Tumu, Shuo Yang, Milad Kazemi, Rahul Mangharam, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy
using only data collected under a nominal (behavioural) policy, is a paramount
problem in data-driven analysis of safety-critical systems where the deployment
of a new policy may be unsafe. To achieve dependable off-policy predictions,
recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal
prediction framework to derive prediction regions with probabilistic guarantees
under the target process. Existing COPP methods can account for the
distribution shifts induced by policy switching, but are limited to
single-agent systems and scalar outcomes (e.g., rewards). In this work, we
introduce MA-COPP, the first conformal prediction method to solve OPP problems
involving multi-agent systems, deriving joint prediction regions for all
agents' trajectories when one or more "ego" agents change their policies.
Unlike the single-agent scenario, this setting introduces higher complexity as
the distribution shifts affect predictions for all agents, not just the ego
agents, and the prediction task involves full multi-dimensional trajectories,
not just reward values. A key contribution of MA-COPP is to avoid enumeration
or exhaustive search of the output space of agent trajectories, which is
instead required by existing COPP methods to construct the prediction region.
We achieve this by showing that an over-approximation of the true JPR can be
constructed, without enumeration, from the maximum density ratio of the JPR
trajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems
from the PettingZoo library and the F1TENTH autonomous racing environment,
achieving nominal coverage in higher dimensions and various shift settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 63rd IEEE Conference on Decision and Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INPC: Implicit Neural Point Clouds for Radiance Field Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hahlbohm, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Marcus Magnor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new approach for reconstruction and novel-view synthesis of
unbounded real-world scenes. In contrast to previous methods using either
volumetric fields, grid-based models, or discrete point cloud proxies, we
propose a hybrid scene representation, which implicitly encodes a point cloud
in a continuous octree-based probability field and a multi-resolution hash
grid. In doing so, we combine the benefits of both worlds by retaining
favorable behavior during optimization: Our novel implicit point cloud
representation and differentiable bilinear rasterizer enable fast rendering
while preserving fine geometric detail without depending on initial priors like
structure-from-motion point clouds. Our method achieves state-of-the-art image
quality on several common benchmark datasets. Furthermore, we achieve fast
inference at interactive frame rates, and can extract explicit point clouds to
further enhance performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://fhahlbohm.github.io/inpc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Remote Estimation of Multiple Markov Sources Under
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiping Luo, Nikolaos Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies semantic-aware communication for remote estimation of
multiple Markov sources over a lossy and rate-constrained channel. Unlike most
existing studies that treat all source states equally, we exploit the semantics
of information and consider that the remote actuator has different tolerances
for the estimation errors of different states. We aim to find an optimal
scheduling policy that minimizes the long-term state-dependent costs of
estimation errors under a transmission frequency constraint. We theoretically
show the structure of the optimal policy by leveraging the average-cost
Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic
programming. By exploiting the optimal structural results, we develop a novel
policy search algorithm, termed intersection search plus relative value
iteration (Insec-RVI), that can find the optimal policy using only a few
iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an
online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on
the Lyapunov optimization theorem. We also design an efficient average-cost
Q-learning algorithm to estimate the optimal policy without knowing a priori
the channel and source statistics. Numerical results show that continuous
transmission is inefficient, and remarkably, our semantic-aware policies can
attain the optimum by strategically utilizing fewer transmissions by exploiting
the timing of the important information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can ChatGPT predict article retraction based on Twitter mentions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Er-Te Zheng, Hui-Zhen Fu, Zhichao Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting problematic research articles timely is a vital task. This study
explores whether Twitter mentions of retracted articles can signal potential
problems with the articles prior to retraction, thereby playing a role in
predicting future retraction of problematic articles. A dataset comprising
3,505 retracted articles and their associated Twitter mentions is analyzed,
alongside 3,505 non-retracted articles with similar characteristics obtained
using the Coarsened Exact Matching method. The effectiveness of Twitter
mentions in predicting article retraction is evaluated by four prediction
methods, including manual labelling, keyword identification, machine learning
models, and ChatGPT. Manual labelling results indicate that there are indeed
retracted articles with their Twitter mentions containing recognizable evidence
signaling problems before retraction, although they represent only a limited
share of all retracted articles with Twitter mention data (approximately 16%).
Using the manual labelling results as the baseline, ChatGPT demonstrates
superior performance compared to other methods, implying its potential in
assisting human judgment for predicting article retraction. This study uncovers
both the potential and limitation of social media events as an early warning
system for article retraction, shedding light on a potential application of
generative artificial intelligence in promoting research integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Qu, Daniel Gomm, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs
with time-varying interactions, face a significant challenge in explainability
due to their complex model structure. Counterfactual explanations, crucial for
understanding model decisions, examine how input graph changes affect outcomes.
This paper introduces two novel counterfactual explanation methods for TGNNs:
GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer
for Dynamic Graphs). They treat explanations as a search problem, seeking input
graph alterations that alter model predictions. GreeDy uses a simple, greedy
approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm.
Experiments show both methods effectively generate clear explanations. Notably,
CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher
success rate in finding significant counterfactual inputs. This highlights
CoDy's potential in clarifying TGNN decision-making, increasing their
transparency and trustworthiness in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLM Agents Have Regret? A Case Study in Online Learning and Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly employed for
(interactive) decision-making, via the development of LLM-based autonomous
agents. Despite their emerging successes, the performance of LLM agents in
decision-making has not been fully investigated through quantitative metrics,
especially in the multi-agent setting when they interact with each other, a
typical scenario in real-world LLM-agent applications. To better understand the
limits of LLM agents in these interactive environments, we propose to study
their interactions in benchmark decision-making settings in online learning and
game theory, through the performance metric of \emph{regret}. We first
empirically study the {no-regret} behaviors of LLMs in canonical
(non-stationary) online learning problems, as well as the emergence of
equilibria when LLM agents interact through playing repeated games. We then
provide some theoretical insights into the no-regret behaviors of LLM agents,
under certain assumptions on the supervised pre-training and the rationality
model of human decision-makers who generate the data. Notably, we also identify
(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To
promote the no-regret behaviors, we propose a novel \emph{unsupervised}
training loss of \emph{regret-loss}, which, in contrast to the supervised
pre-training loss, does not require the labels of (optimal) actions. We then
establish the statistical guarantee of generalization bound for regret-loss
minimization, followed by the optimization guarantee that minimizing such a
loss may automatically lead to known no-regret learning algorithms. Our further
experiments demonstrate the effectiveness of our regret-loss, especially in
addressing the above ``regrettable'' cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of a model-free entropy-regularized inverse reinforcement
  learning algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Titouan Renard, Andreas Schlaginhaufen, Tingting Ni, Maryam Kamgarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a dataset of expert demonstrations, inverse reinforcement learning
(IRL) aims to recover a reward for which the expert is optimal. This work
proposes a model-free algorithm to solve entropy-regularized IRL problem. In
particular, we employ a stochastic gradient descent update for the reward and a
stochastic soft policy iteration update for the policy. Assuming access to a
generative model, we prove that our algorithm is guaranteed to recover a reward
for which the expert is $\varepsilon$-optimal using
$\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP).
Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the
optimal policy corresponding to the recovered reward is $\varepsilon$-close to
the expert policy in total variation distance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak Convergence Analysis of Online Neural Actor-Critic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Chun-Hei Lam, Justin Sirignano, Ziheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that a single-layer neural network trained with the online actor
critic algorithm converges in distribution to a random ordinary differential
equation (ODE) as the number of hidden units and the number of training steps
$\rightarrow \infty$. In the online actor-critic algorithm, the distribution of
the data samples dynamically changes as the model is updated, which is a key
challenge for any convergence analysis. We establish the geometric ergodicity
of the data samples under a fixed actor policy. Then, using a Poisson equation,
we prove that the fluctuations of the model updates around the limit
distribution due to the randomly-arriving data samples vanish as the number of
parameter updates $\rightarrow \infty$. Using the Poisson equation and weak
convergence techniques, we prove that the actor neural network and critic
neural network converge to the solutions of a system of ODEs with random
initial conditions. Analysis of the limit ODE shows that the limit critic
network will converge to the true value function, which will provide the actor
an asymptotically unbiased estimate of the policy gradient. We then prove that
the limit actor network will converge to a stationary point.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A
  User-Centric Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Ji, Xiping Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)
are an emerging indoor wireless communication paradigm, which combines the
advantages of the capacious optical spectra of LiFi and ubiquitous coverage of
WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource
management for such hybrid networks. The existing LB methods are mostly
network-centric, relying on a central unit to make a solution for the users all
at once. Consequently, the solution needs to be updated for all users at the
same pace, regardless of their moving status. This would affect the network
performance in two aspects: i) when the update frequency is low, it would
compromise the connectivity of fast-moving users; ii) when the update frequency
is high, it would cause unnecessary handovers as well as hefty feedback costs
for slow-moving users. Motivated by this, we investigate user-centric LB which
allows users to update their solutions at different paces. The research is
developed upon our previous work on adaptive target-condition neural network
(ATCNN), which can conduct LB for individual users in quasi-static channels. In
this paper, a deep neural network (DNN) model is designed to enable an adaptive
update interval for each individual user. This new model is termed as
mobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a
user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is
proposed to handle resource management and mobility management simultaneously.
Results show that at the same level of average update interval, MS-ATCNN can
achieve a network throughput up to 215\% higher than conventional LB methods
such as game theory, especially for a larger number of users. In addition,
MS-ATCNN costs an ultra low runtime at the level of 100s $\mu$s, which is two
to three orders of magnitude lower than game theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, 3 tables, submitted to IEEE TWC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple-Source Localization from a Single-Snapshot Observation Using
  Graph Bayesian Optimization <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Zhang, Zijian Zhang, Zhiqian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the significance of its various applications, source localization has
garnered considerable attention as one of the most important means to confront
diffusion hazards. Multi-source localization from a single-snapshot observation
is especially relevant due to its prevalence. However, the inherent
complexities of this problem, such as limited information, interactions among
sources, and dependence on diffusion models, pose challenges to resolution.
Current methods typically utilize heuristics and greedy selection, and they are
usually bonded with one diffusion model. Consequently, their effectiveness is
constrained. To address these limitations, we propose a simulation-based method
termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results
for its sample efficiency. A surrogate function models uncertainty from the
limited information. It takes sets of nodes as the input instead of individual
nodes. BOSouL can incorporate any diffusion model in the data acquisition
process through simulations. Empirical studies demonstrate that its performance
is robust across graph structures and diffusion models. The code is available
at https://github.com/XGraph-Team/BOSouL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the AAAI Conference on Artificial Intelligence, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Yang, Marie Siew, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Workshop on Foundation Models for
  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at
  CPS-IoT Week 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster-Based Normalization Layer for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Faye, Hanane Azzag, Mustapha Lebbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning faces significant challenges during the training of neural
networks, including internal covariate shift, label shift, vanishing/exploding
gradients, overfitting, and computational complexity. While conventional
normalization methods, such as Batch Normalization, aim to tackle some of these
issues, they often depend on assumptions that constrain their adaptability.
Mixture Normalization faces computational hurdles in its pursuit of handling
multiple Gaussian distributions. This paper introduces Cluster-Based
Normalization (CB-Norm) in two variants - Supervised Cluster-Based
Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization
(UCB-Norm) - proposing a groundbreaking one-step normalization approach.
CB-Norm leverages a Gaussian mixture model to specifically address challenges
related to gradient stability and learning acceleration. For SCB-Norm, a
supervised variant, the novel mechanism involves introducing predefined data
partitioning, termed clusters, to normalize activations based on the assigned
cluster. This cluster-driven approach creates a space that conforms to a
Gaussian mixture model. On the other hand, UCB-Norm, an unsupervised
counterpart, dynamically clusters neuron activations during training, adapting
to task-specific challenges without relying on predefined data partitions
(clusters). This dual approach ensures flexibility in addressing diverse
learning scenarios. CB-Norm innovatively uses a one-step normalization
approach, where parameters of each mixture component (cluster in activation
space) serve as weights for deep neural networks. This adaptive clustering
process tackles both clustering and resolution of deep neural network tasks
concurrently during training, signifying a notable advancement in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iso-Diffusion: Improving Diffusion Probabilistic Models Using the
  Isotropy of the Additive Gaussian Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilum Fernando, Dhananjaya jayasundara, Roshan Godaliyadda, Chaminda Bandara, Parakrama Ekanayake, Vijitha Herath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in
the realm of generative AI. Despite their high performance, there is room for
improvement, especially in terms of sample fidelity by utilizing statistical
properties that impose structural integrity, such as isotropy. Minimizing the
mean squared error between the additive and predicted noise alone does not
impose constraints on the predicted noise to be isotropic. Thus, we were
motivated to utilize the isotropy of the additive noise as a constraint on the
objective function to enhance the fidelity of DDPMs. Our approach is simple and
can be applied to any DDPM variant. We validate our approach by presenting
experiments conducted on four synthetic 2D datasets as well as on unconditional
image generation. As demonstrated by the results, the incorporation of this
constraint improves the fidelity metrics, Precision and Density for the 2D
datasets as well as for the unconditional image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks (AAs) pose a significant threat to the reliability and
robustness of deep neural networks. While the impact of these attacks on model
predictions has been extensively studied, their effect on the learned
representations and concepts within these models remains largely unexplored. In
this work, we perform an in-depth analysis of the influence of AAs on the
concepts learned by convolutional neural networks (CNNs) using eXplainable
artificial intelligence (XAI) techniques. Through an extensive set of
experiments across various network architectures and targeted AA techniques, we
unveil several key findings. First, AAs induce substantial alterations in the
concept composition within the feature space, introducing new concepts or
modifying existing ones. Second, the adversarial perturbation itself can be
linearly decomposed into a set of latent vector components, with a subset of
these being responsible for the attack's success. Notably, we discover that
these components are target-specific, i.e., are similar for a given target
class throughout different AA techniques and starting classes. Our findings
provide valuable insights into the nature of AAs and their impact on learned
representations, paving the way for the development of more robust and
interpretable deep learning models, as well as effective defenses against
adversarial threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin Menten, Tamara Mueller, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical atlases are widely used for population analysis. Conditional
atlases target a particular sub-population defined via certain conditions (e.g.
demographics or pathologies) and allow for the investigation of fine-grained
anatomical differences - such as morphological changes correlated with age.
Existing approaches use either registration-based methods that are unable to
handle large anatomical variations or generative models, which can suffer from
training instabilities and hallucinations. To overcome these limitations, we
use latent diffusion models to generate deformation fields, which transform a
general population atlas into one representing a specific sub-population. By
generating a deformation field and registering the conditional atlas to a
neighbourhood of images, we ensure structural plausibility and avoid
hallucinations, which can occur during direct image synthesis. We compare our
method to several state-of-the-art atlas generation methods in experiments
using 5000 brain as well as whole-body MR images from UK Biobank. Our method
generates highly realistic atlases with smooth transformations and high
anatomical fidelity, outperforming the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data Generation and Joint Learning for Robust Code-Mixed
  Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kartik, Sanjana Soni, Anoop Kunchukuttan, Tanmoy Chakraborty, Md Shad Akhtar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread online communication in a modern multilingual world has
provided opportunities to blend more than one language (aka code-mixed
language) in a single utterance. This has resulted a formidable challenge for
the computational models due to the scarcity of annotated data and presence of
noise. A potential solution to mitigate the data scarcity problem in
low-resource setup is to leverage existing data in resource-rich language
through translation. In this paper, we tackle the problem of code-mixed
(Hinglish and Bengalish) to English machine translation. First, we
synthetically develop HINMIX, a parallel corpus of Hinglish to English, with
~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation
based joint-training model that learns to handle noise in the real-world
code-mixed text by parameter sharing across clean and noisy words. Further, we
show the adaptability of RCMT in a zero-shot setup for Bengalish to English
translation. Our evaluation and comprehensive analyses qualitatively and
quantitatively demonstrate the superiority of RCMT over state-of-the-art
code-mixed and robust translation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, to be published in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepKnowledge: Generalisation-Driven Deep Learning Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sondess Missaoui, Simos Gerasimou, Nikolaos Matragkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their unprecedented success, DNNs are notoriously fragile to small
shifts in data distribution, demanding effective testing techniques that can
assess their dependability. Despite recent advances in DNN testing, there is a
lack of systematic testing approaches that assess the DNN's capability to
generalise and operate comparably beyond data in their training distribution.
We address this gap with DeepKnowledge, a systematic testing methodology for
DNN-based systems founded on the theory of knowledge generalisation, which aims
to enhance DNN robustness and reduce the residual risk of 'black box' models.
Conforming to this theory, DeepKnowledge posits that core computational DNN
units, termed Transfer Knowledge neurons, can generalise under domain shift.
DeepKnowledge provides an objective confidence measurement on testing
activities of DNN given data distribution shifts and uses this information to
instrument a generalisation-informed test adequacy criterion to check the
transfer knowledge capacity of a test set. Our empirical evaluation of several
DNNs, across multiple datasets and state-of-the-art adversarial generation
techniques demonstrates the usefulness and effectiveness of DeepKnowledge and
its ability to support the engineering of more dependable DNNs. We report
improvements of up to 10 percentage points over state-of-the-art coverage
criteria for detecting adversarial attacks on several benchmarks, including
MNIST, SVHN, and CIFAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Domain Incremental Learning <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasushi Esaki, Satoshi Koide, Takuro Kutsuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain incremental learning (DIL) has been discussed in previous studies on
deep neural network models for classification. In DIL, we assume that samples
on new domains are observed over time. The models must classify inputs on all
domains. In practice, however, we may encounter a situation where we need to
perform DIL under the constraint that the samples on the new domain are
observed only infrequently. Therefore, in this study, we consider the extreme
case where we have only one sample from the new domain, which we call one-shot
DIL. We first empirically show that existing DIL methods do not work well in
one-shot DIL. We have analyzed the reason for this failure through various
investigations. According to our analysis, we clarify that the difficulty of
one-shot DIL is caused by the statistics in the batch normalization layers.
Therefore, we propose a technique regarding these statistics and demonstrate
the effectiveness of our technique through experiments on open datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE International Joint Conference on Neural Networks
  (IJCNN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on generalization bounds for losses with finite moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the truncation method from Alquier [1] to derive
high-probability PAC-Bayes bounds for unbounded losses with heavy tails.
Assuming that the $p$-th moment is bounded, the resulting bounds interpolate
between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p
\to \infty$ and the loss is essentially bounded. Moreover, the paper derives a
high-probability PAC-Bayes bound for losses with a bounded variance. This bound
has an exponentially better dependence on the confidence parameter and the
dependency measure than previous bounds in the literature. Finally, the paper
extends all results to guarantees in expectation and single-draw PAC-Bayes. In
order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded
losses from [2] in these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages: 5 of main text, 1 of references, and 3 of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rene Winchenbach, Nils Thuerey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning physical simulations has been an essential and central aspect of
many recent research efforts in machine learning, particularly for
Navier-Stokes-based fluid mechanics. Classic numerical solvers have
traditionally been computationally expensive and challenging to use in inverse
problems, whereas Neural solvers aim to address both concerns through machine
learning. We propose a general formulation for continuous convolutions using
separable basis functions as a superset of existing methods and evaluate a
large set of basis functions in the context of (a) a compressible 1D SPH
simulation, (b) a weakly compressible 2D SPH simulation, and (c) an
incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries
included in the basis functions are key aspects of stability and accuracy. Our
broad evaluation shows that Fourier-based continuous convolutions outperform
all other architectures regarding accuracy and generalization. Finally, using
these Fourier-based networks, we show that prior inductive biases, such as
window functions, are no longer necessary. An implementation of our approach,
as well as complete datasets and solver implementations, is available at
https://github.com/tum-pbs/SFBC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Learning Representation
  (ICLR) 2024, 54 pages, 39 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Graph Representation Learning with Attention-Driven Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Mingkun Xu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning has become a crucial task in machine learning
and data mining due to its potential for modeling complex structures such as
social networks, chemical compounds, and biological systems. Spiking neural
networks (SNNs) have recently emerged as a promising alternative to traditional
neural networks for graph learning tasks, benefiting from their ability to
efficiently encode and process temporal and spatial information. In this paper,
we propose a novel approach that integrates attention mechanisms with SNNs to
improve graph representation learning. Specifically, we introduce an attention
mechanism for SNN that can selectively focus on important nodes and
corresponding features in a graph during the learning process. We evaluate our
proposed method on several benchmark datasets and show that it achieves
comparable performance compared to existing graph learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for Recommendation <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Loss Function-based Support Vector Machine for Binary
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Liping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss
SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the
degree of penalty for the correctly classified samples within the margin. This
oversight affects the generalization ability of the SVM classifier to some
extent. To address this limitation, from the perspective of confidence margin,
we propose a novel Slide loss function ($\ell_s$) to construct the support
vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal
stationary point, and utilizing the property of Lipschitz continuity, we derive
the first-order optimality conditions for $\ell_s$-SVM. Based on this, we
define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To
efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method
of multipliers with the working set ($\ell_s$-ADMM), and provide the
convergence analysis. The numerical experiments on real world datasets confirm
the robustness and effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim-to-Real Gap with Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Texture Loss for CT denoising with GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed collaborative anomalous sound detection by embedding sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift evolution of machine learning has led to emergence of various
definitions of privacy due to the threats it poses to privacy, including the
concept of local differential privacy (LDP). Although widely embraced and
utilized across numerous domains, this conventional approach to measure privacy
still exhibits certain limitations, spanning from failure to prevent
inferential disclosure to lack of consideration for the adversary's background
knowledge. In this comprehensive study, we introduce Bayesian privacy and delve
into the intricate relationship between local differential privacy and its
Bayesian counterparts, unveiling novel insights into utility-privacy
trade-offs. We introduce a framework that encapsulates both attack and defense
strategies, highlighting their interplay and effectiveness. Our theoretical
contributions are anchored in the rigorous definitions and relationships
between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),
encapsulated by equations $\epsilon_{p,a} \leq
\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +
\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP
established under uniform prior distribution. These relationships fortify our
understanding of the privacy guarantees provided by various mechanisms, leading
to the realization that a mechanism satisfying $\xi$-LDP also confers
$\xi$-MBP, and vice versa. Our work not only lays the groundwork for future
empirical exploration but also promises to enhance the design of
privacy-preserving algorithms that do not compromise on utility, thereby
fostering the development of trustworthy machine learning solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antigen-Specific Antibody Design via Direct Energy-based Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibody design, a crucial task with significant implications across various
disciplines such as therapeutics and biology, presents considerable challenges
due to its intricate nature. In this paper, we tackle antigen-specific antibody
design as a protein sequence-structure co-design problem, considering both
rationality and functionality. Leveraging a pre-trained conditional diffusion
model that jointly models sequences and structures of
complementarity-determining regions (CDR) in antibodies with equivariant neural
networks, we propose direct energy-based preference optimization to guide the
generation of antibodies with both rational structures and considerable binding
affinities to given antigens. Our method involves fine-tuning the pre-trained
diffusion model using a residue-level decomposed energy preference.
Additionally, we employ gradient surgery to address conflicts between various
types of energy, such as attraction and repulsion. Experiments on RAbD
benchmark show that our approach effectively optimizes the energy of generated
antibodies and achieves state-of-the-art performance in designing high-quality
antibodies with low total energy and high binding affinity, demonstrating the
superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) heavily depends on label quality for its performance.
However, the label distribution among individual clients is always both noisy
and heterogeneous. The high loss incurred by client-specific samples in
heterogeneous label noise poses challenges for distinguishing between
client-specific and noisy label samples, impacting the effectiveness of
existing label noise learning approaches. To tackle this issue, we propose
FedFixer, where the personalized model is introduced to cooperate with the
global model to effectively select clean client-specific samples. In the dual
models, updating the personalized model solely at a local level can lead to
overfitting on noisy data due to limited samples, consequently affecting both
the local and global models' performance. To mitigate overfitting, we address
this concern from two perspectives. Firstly, we employ a confidence regularizer
to alleviate the impact of unconfident predictions caused by label noise.
Secondly, a distance regularizer is implemented to constrain the disparity
between the personalized and global models. We validate the effectiveness of
FedFixer through extensive experiments on benchmark datasets. The results
demonstrate that FedFixer can perform well in filtering noisy label samples on
different clients, especially in highly heterogeneous label noise scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Federated Learning by Selecting Beneficial Herd of Local
  Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning framework in
communication network systems. However, the systems' Non-Independent and
Identically Distributed (Non-IID) data negatively affect the convergence
efficiency of the global model, since only a subset of these data samples are
beneficial for model convergence. In pursuit of this subset, a reliable
approach involves determining a measure of validity to rank the samples within
the dataset. In this paper, We propose the BHerd strategy which selects a
beneficial herd of local gradients to accelerate the convergence of the FL
model. Specifically, we map the distribution of the local dataset to the local
gradients and use the Herding strategy to obtain a permutation of the set of
gradients, where the more advanced gradients in the permutation are closer to
the average of the set of gradients. These top portion of the gradients will be
selected and sent to the server for global aggregation. We conduct experiments
on different datasets, models and scenarios by building a prototype system, and
experimental results demonstrate that our BHerd strategy is effective in
selecting beneficial local gradients to mitigate the effects brought by the
Non-IID dataset, thus accelerating model convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Meta-Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure Leskovec, Christopher Re, Sebastian Thrun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models like ChatGPT demonstrate a remarkable capacity to learn
new concepts during inference without any fine-tuning. However, visual models
trained to detect new objects during inference have been unable to replicate
this ability, and instead either perform poorly or require meta-training and/or
fine-tuning on similar objects. In this work, we propose a meta-learning
algorithm that emulates Large Language Models by learning new visual concepts
during inference without fine-tuning. Our approach leverages a frozen
pre-trained feature extractor, and analogous to in-context learning, recasts
visual meta-learning as sequence modeling over datapoints with known labels and
a test datapoint with an unknown label. On 8 out of 11 meta-learning
benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or
matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these
benchmarks. Our code is available at https://github.com/cfifty/CAML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying statistical learning theory to deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cédric Gerbelot, Avetik Karagulyan, Stefani Karp, Kavya Ravichandran, Menachem Stern, Nathan Srebro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although statistical learning theory provides a robust framework to
understand supervised learning, many theoretical aspects of deep learning
remain unclear, in particular how different architectures may lead to inductive
bias when trained using gradient based methods. The goal of these lectures is
to provide an overview of some of the main questions that arise when attempting
to understand deep learning from a learning theory perspective. After a brief
reminder on statistical learning theory and stochastic optimization, we discuss
implicit bias in the context of benign overfitting. We then move to a general
description of the mirror descent algorithm, showing how we may go back and
forth between a parameter space and the corresponding function space for a
given learning problem, as well as how the geometry of the learning problem may
be represented by a metric tensor. Building on this framework, we provide a
detailed study of the implicit bias of gradient descent on linear diagonal
networks for various regression tasks, showing how the loss function, scale of
parameters at initialization and depth of the network may lead to various forms
of implicit bias, in particular transitioning between kernel or feature
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model
  Conversions between Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Converting deep learning models between frameworks is a common step to
maximize model compatibility across devices and leverage optimization features
that may be exclusively provided in one deep learning framework. However, this
conversion process may be riddled with bugs, making the converted models either
undeployable or problematic, considerably degrading their prediction
correctness.
  In this paper we propose an automated approach for fault localization and
repair, Fix-Con, during model conversion between deep learning frameworks.
Fix-Con is capable of detecting and fixing faults introduced in model input,
parameters, hyperparameters, and the model graph during conversion.
  Fix-Con uses a set of fault types (mined from surveying conversion issues
reported \nick{in code repositories and forums}) to localize potential
conversion faults in the converted target model and then repair them
appropriately, e.g., replacing the parameters of the target model with those
from the source model. This is done iteratively for every image in the dataset,
comparing output label differences between the source model and the converted
target model until all differences are resolved. We evaluate the effectiveness
of Fix-Con in fixing model conversion bugs of three widely used image
recognition models converted across four different deep learning frameworks.
Overall, Fix-Con was able to fix $462$ out of $755$ detected conversion faults,
either completely repairing or significantly improving the performance of $14$
out of the $15$ erroneous conversion cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 3 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive evaluation of Mal-API-2019 dataset by machine learning in
  malware detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglin Li, Haibei Zhu, Houze Liu, Jintong Song, Qishuo Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study conducts a thorough examination of malware detection using machine
learning techniques, focusing on the evaluation of various classification
models using the Mal-API-2019 dataset. The aim is to advance cybersecurity
capabilities by identifying and mitigating threats more effectively. Both
ensemble and non-ensemble machine learning methods, such as Random Forest,
XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special
emphasis is placed on the importance of data pre-processing techniques,
particularly TF-IDF representation and Principal Component Analysis, in
improving model performance. Results indicate that ensemble methods,
particularly Random Forest and XGBoost, exhibit superior accuracy, precision,
and recall compared to others, highlighting their effectiveness in malware
detection. The paper also discusses limitations and potential future
directions, emphasizing the need for continuous adaptation to address the
evolving nature of malware. This research contributes to ongoing discussions in
cybersecurity and provides practical insights for developing more robust
malware detection systems in the digital era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fault Localization for Buggy Deep Learning Framework Conversions in
  Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06157v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06157v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 100%.
To mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Covariance-Aware Private Mean Estimation Without Private Covariance
  Estimation <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.13329v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.13329v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present two sample-efficient differentially private mean estimators for
$d$-dimensional (sub)Gaussian distributions with unknown covariance.
Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with
mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that
$\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is
the Mahalanobis distance. All previous estimators with the same guarantee
either require strong a priori bounds on the covariance matrix or require
$\Omega(d^{3/2})$ samples.
  Each of our estimators is based on a simple, general approach to designing
differentially private mechanisms, but with novel technical steps to make the
estimator private and sample-efficient. Our first estimator samples a point
with approximately maximum Tukey depth using the exponential mechanism, but
restricted to the set of points of large Tukey depth. Its accuracy guarantees
hold even for data sets that have a small amount of adversarial corruption.
Proving that this mechanism is private requires a novel analysis. Our second
estimator perturbs the empirical mean of the data set with noise calibrated to
the empirical covariance, without releasing the covariance itself. Its sample
complexity guarantees hold more generally for subgaussian distributions, albeit
with a slightly worse dependence on the privacy parameter. For both estimators,
careful preprocessing of the data is required to satisfy differential privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages. Appeared in NeurIPS 2021. Updated version contains improved
  analysis of Tukey depth mechanism: robustness guarantees, tighter error
  analysis, and techniques for faster implementation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaNN: Assessing the Impact of Computational Environment Parameters on
  the Performance of Image Recognition Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06208v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06208v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
  In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 100% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning Using Three-Operator ADMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04152v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04152v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashi Kant, José Mairton B. da Silva Jr., Gabor Fodor, Bo Göransson, Mats Bengtsson, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as an instance of distributed machine
learning paradigm that avoids the transmission of data generated on the users'
side. Although data are not transmitted, edge devices have to deal with limited
communication bandwidths, data heterogeneity, and straggler effects due to the
limited computational resources of users' devices. A prominent approach to
overcome such difficulties is FedADMM, which is based on the classical
two-operator consensus alternating direction method of multipliers (ADMM). The
common assumption of FL algorithms, including FedADMM, is that they learn a
global model using data only on the users' side and not on the edge server.
However, in edge learning, the server is expected to be near the base station
and have direct access to rich datasets. In this paper, we argue that
leveraging the rich data on the edge server is much more beneficial than
utilizing only user datasets. Specifically, we show that the mere application
of FL with an additional virtual user node representing the data on the edge
server is inefficient. We propose FedTOP-ADMM, which generalizes FedADMM and is
based on a three-operator ADMM-type technique that exploits a smooth cost
function on the edge server to learn a global model parallel to the edge
devices. Our numerical experiments indicate that FedTOP-ADMM has substantial
gain up to 33\% in communication efficiency to reach a desired test accuracy
with respect to FedADMM, including a virtual user on the edge server.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IEEE Journal of Selected Topics in Signal Processing,
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Crowd-Aware Multi-Agent Path Finding through Local
  Broadcasting with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Pham, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF) in crowded environments presents a
challenging problem in motion planning, aiming to find collision-free paths for
all agents in the system. MAPF finds a wide range of applications in various
domains, including aerial swarms, autonomous warehouse robotics, and
self-driving vehicles. Current approaches to MAPF generally fall into two main
categories: centralized and decentralized planning. Centralized planning
suffers from the curse of dimensionality when the number of agents or states
increases and thus does not scale well in large and complex environments. On
the other hand, decentralized planning enables agents to engage in real-time
path planning within a partially observable environment, demonstrating implicit
coordination. However, they suffer from slow convergence and performance
degradation in dense environments. In this paper, we introduce CRAMP, a novel
crowd-aware decentralized reinforcement learning approach to address this
problem by enabling efficient local communication among agents via Graph Neural
Networks (GNNs), facilitating situational awareness and decision-making
capabilities in congested environments. We test CRAMP on simulated environments
and demonstrate that our method outperforms the state-of-the-art decentralized
methods for MAPF on various metrics. CRAMP improves the solution quality up to
59% measured in makespan and collision count, and up to 35% improvement in
success rate in comparison to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SO(2)-<span class="highlight-title">Equivariant</span> Downwash Models for Close Proximity Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Smith, A. Shankar, J. Gielis, J. Blumenkamp, A. Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multirotors flying in close proximity induce aerodynamic wake effects on each
other through propeller downwash. Conventional methods have fallen short of
providing adequate 3D force-based models that can be incorporated into robust
control paradigms for deploying dense formations. Thus, learning a model for
these downwash patterns presents an attractive solution. In this paper, we
present a novel learning-based approach for modelling the downwash forces that
exploits the latent geometries (i.e. symmetries) present in the problem. We
demonstrate that when trained with only 5 minutes of real-world flight data,
our geometry-aware model outperforms state-of-the-art baseline models trained
with more than 15 minutes of data. In dense real-world flights with two
vehicles, deploying our model online improves 3D trajectory tracking by nearly
36% on average (and vertical tracking by 56%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Node-Aligned Graph-to-Graph (NAG2G): Elevating Template-Free Deep
  Learning Approaches in Single-Step Retrosynthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Yao, Wentao Guo, Zhen Wang, Shang Xiang, Wentan Liu, <span class="highlight-author">Guolin Ke</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-step retrosynthesis (SSR) in organic chemistry is increasingly
benefiting from deep learning (DL) techniques in computer-aided synthesis
design. While template-free DL models are flexible and promising for
retrosynthesis prediction, they often ignore vital 2D molecular information and
struggle with atom alignment for node generation, resulting in lower
performance compared to the template-based and semi-template-based methods. To
address these issues, we introduce Node-Aligned Graph-to-Graph (NAG2G), a
transformer-based template-free DL model. NAG2G combines 2D molecular graphs
and 3D conformations to retain comprehensive molecular details and incorporates
product-reactant atom mapping through node alignment which determines the order
of the node-by-node graph outputs process in an auto-regressive manner. Through
rigorous benchmarking and detailed case studies, we have demonstrated that
NAG2G stands out with its remarkable predictive accuracy on the expansive
datasets of USPTO-50k and USPTO-FULL. Moreover, the model's practical utility
is underscored by its successful prediction of synthesis pathways for multiple
drug candidate molecules. This not only proves NAG2G's robustness but also its
potential to revolutionize the prediction of complex chemical synthesis
processes for future synthetic route design tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive
  Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn
new classes with scarce samples while preserving knowledge of old ones.
Existing FSCIL methods usually fine-tune the entire backbone, leading to
overfitting and hindering the potential to learn new classes. On the other
hand, recent prompt-based CIL approaches alleviate forgetting by training
prompts with sufficient data in each task. In this work, we propose a novel
framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages
task-invariant prompts to capture shared knowledge by reducing specific
information from the attention aspect. Additionally, self-adaptive
task-specific prompts in ASP provide specific information and transfer
knowledge from old classes to new classes with an Information Bottleneck
learning objective. In summary, ASP prevents overfitting on base task and does
not require enormous data in few-shot incremental tasks. Extensive experiments
on three benchmark datasets validate that ASP consistently outperforms
state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning
new classes and mitigating forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAC Privacy Preserving Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qipan Xu, Youlong Ding, Xinxi Zhang, Jie Gao, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data privacy protection is garnering increased attention among researchers.
Diffusion models (DMs), particularly with strict differential privacy, can
potentially produce images with both high privacy and visual quality. However,
challenges arise such as in ensuring robust protection in privatizing specific
data attributes, areas where current models often fall short. To address these
challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model
leverages diffusion principles and ensure Probably Approximately Correct (PAC)
privacy. We enhance privacy protection by integrating a private classifier
guidance into the Langevin Sampling Process. Additionally, recognizing the gap
in measuring the privacy of models, we have developed a novel metric to gauge
privacy levels. Our model, assessed with this new metric and supported by
Gaussian matrix computations for the PAC bound, has shown superior performance
in privacy protection over existing leading private generative models according
to benchmark tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment
  Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09362v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09362v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Elahimanesh, Shayan Salehi, Sara Zahedi Movahed, Lisa Alazraki, Ruoyu Hu, Abbas Edalat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wake of the post-pandemic era, marked by social isolation and surging
rates of depression and anxiety, conversational agents based on digital
psychotherapy can play an influential role compared to traditional therapy
sessions. In this work, we develop a voice-capable chatbot in Farsi to guide
users through Self-Attachment (SAT), a novel, self-administered, holistic
psychological technique based on attachment theory. Our chatbot uses a dynamic
array of rule-based and classification-based modules to comprehend user input
throughout the conversation and navigates a dialogue flowchart accordingly,
recommending appropriate SAT exercises that depend on the user's emotional and
mental state. In particular, we collect a dataset of over 6,000 utterances and
develop a novel sentiment-analysis module that classifies user sentiment into
12 classes, with accuracy above 92%. To keep the conversation novel and
engaging, the chatbot's responses are retrieved from a large dataset of
utterances created with the aid of Farsi GPT-2 and a reinforcement learning
approach, thus requiring minimal human annotation. Our chatbot also offers a
question-answering module, called SAT Teacher, to answer users' questions about
the principles of Self-Attachment. Finally, we design a cross-platform
application as the bot's user interface. We evaluate our platform in a ten-day
human study with N=52 volunteers from the non-clinical population, who have had
over 2,000 dialogues in total with the chatbot. The results indicate that the
platform was engaging to most users (75%), 72% felt better after the
interactions, and 74% were satisfied with the SAT Teacher's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Graph Neural Networks on Real Processing-In-Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI-MAKE 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-training for Speech with Flow Matching <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-MCMC: Sampling from Flat Basins with Ease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05401v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05401v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian deep learning counts on the quality of posterior distribution
estimation. However, the posterior of deep neural networks is highly
multi-modal in nature, with local modes exhibiting varying generalization
performance. Given a practical budget, targeting at the original posterior can
lead to suboptimal performance, as some samples may become trapped in "bad"
modes and suffer from overfitting. Leveraging the observation that "good" modes
with low generalization error often reside in flat basins of the energy
landscape, we propose to bias sampling on the posterior toward these flat
regions. Specifically, we introduce an auxiliary guiding variable, the
stationary distribution of which resembles a smoothed posterior free from sharp
modes, to lead the MCMC sampler to flat basins. By integrating this guiding
variable with the model parameter, we create a simple joint distribution that
enables efficient sampling with minimal computational overhead. We prove the
convergence of our method and further show that it converges faster than
several existing flatness-aware methods in the strongly convex setting.
Empirical results demonstrate that our method can successfully sample from flat
basins of the posterior, and outperforms all compared baselines on multiple
benchmarks including classification, calibration, and out-of-distribution
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Meta-Learning Perspective on Transformers for Causal Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinbo Wu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture has become prominent in developing large causal
language models. However, mechanisms to explain its capabilities are not well
understood. Focused on the training process, here we establish a meta-learning
view of the Transformer architecture when trained for the causal language
modeling task, by explicating an inner optimization process within the
Transformer. Further, within the inner optimization, we discover and
theoretically analyze a special characteristic of the norms of learned token
representations within Transformer-based causal language models. Our analysis
is supported by experiments in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards White Box Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Satkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces semantic features as a candidate conceptual framework
for building inherently interpretable neural networks. A proof of concept model
for informative subproblem of MNIST consists of 4 such layers with the total of
5K learnable parameters. The model is well-motivated, inherently interpretable,
requires little hyperparameter tuning and achieves human-level adversarial test
accuracy - with no form of adversarial training! These results and the general
nature of the approach warrant further research on semantic features. The code
is available at https://github.com/314-Foundation/white-box-nn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, independent research, v2 changes: more adequate
  title; added: related research in Introduction, Ablation Study, Discussion,
  examples in Further Research, Appendix C; minor wording changes (including
  abstract)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Carbon Footprint Reduction for Sustainable Data Centers in Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning workloads significantly increase energy consumption,
sustainable data centers with low carbon emissions are becoming a top priority
for governments and corporations worldwide. This requires a paradigm shift in
optimizing power consumption in cooling and IT loads, shifting flexible loads
based on the availability of renewable energy in the power grid, and leveraging
battery storage from the uninterrupted power supply in data centers, using
collaborative agents. The complex association between these optimization
strategies and their dependencies on variable external factors like weather and
the power grid carbon intensity makes this a hard problem. Currently, a
real-time controller to optimize all these goals simultaneously in a dynamic
real-world setting is lacking. We propose a Data Center Carbon Footprint
Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that
optimizes data centers for the multiple objectives of carbon footprint
reduction, energy consumption, and energy cost. The results show that the
DC-CFR MARL agents effectively resolved the complex interdependencies in
optimizing cooling, load shifting, and energy storage in real-time for various
locations under real-world dynamic weather and grid carbon intensity
conditions. DC-CFR significantly outperformed the industry standard ASHRAE
controller with a considerable reduction in carbon emissions (14.5%), energy
usage (14.4%), and energy cost (13.7%) when evaluated over one year across
multiple geographical regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separable Hamiltonian Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Yu Khoo, Dawen Wu, Jonathan Sze Choong Low, Stéphane Bressan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamiltonian neural networks (HNNs) are state-of-the-art models that regress
the vector field of a dynamical system under the learning bias of Hamilton's
equations. A recent observation is that embedding a bias regarding the additive
separability of the Hamiltonian reduces the regression complexity and improves
regression performance. We propose separable HNNs that embed additive
separability within HNNs using observational, learning, and inductive biases.
We show that the proposed models are more effective than the HNN at regressing
the Hamiltonian and the vector field, and have the capability to interpret the
kinetic and potential energy of the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering modular solutions that generalize compositionally <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wołczyk, Alexandra Proca, Johannes von Oswald, Razvan Pascanu, João Sacramento, Angelika Steger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many complex tasks can be decomposed into simpler, independent parts.
Discovering such underlying compositional structure has the potential to enable
compositional generalization. Despite progress, our most powerful systems
struggle to compose flexibly. It therefore seems natural to make models more
modular to help capture the compositional nature of many tasks. However, it is
unclear under which circumstances modular systems can discover hidden
compositional structure. To shed light on this question, we study a
teacher-student setting with a modular teacher where we have full control over
the composition of ground truth modules. This allows us to relate the problem
of compositional generalization to that of identification of the underlying
modules. In particular we study modularity in hypernetworks representing a
general class of multiplicative interactions. We show theoretically that
identification up to linear transformation purely from demonstrations is
possible without having to learn an exponential number of module combinations.
We further demonstrate empirically that under the theoretically identified
conditions, meta-learning from finite data can discover modular policies that
generalize compositionally in a number of complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024; Code available at
  https://github.com/smonsays/modular-hyperteacher</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CacheGen: KV Cache Compression and Streaming for Fast Language Model
  Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07240v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07240v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) take on complex tasks, their inputs are
supplemented with longer contexts that incorporate domain knowledge or
user-specific information. Yet using long contexts poses a challenge for
responsive LLM systems, as nothing can be generated until the whole context is
processed by the LLM. While the context-processing delay can be reduced by
reusing the KV cache of a context across different inputs, fetching the KV
cache, which contains large tensors, over the network can cause extra network
delays.
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen
uses a custom tensor encoder, which embraces KV cache's distributional
properties, to encode a KV cache into more compact bitstream representations
with negligible encoding/decoding overhead. This reduces the bandwidth demand
to fetch the KV cache. Second, to maintain low context-loading delay and high
generation quality, CacheGen adapts the streaming strategies to cope with
changes in available bandwidth. When available bandwidth drops, CacheGen may
raise the compression level for a part of the context or choose to recompute
its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes
and four datasets (662 contexts in total). Compared to the recent systems that
reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the
total delay in fetching and processing contexts by 2.7-3.2x while having
negligible impact on the LLM response quality in accuracy or perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributional Robustness Bounds Generalization Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiong Wang, Haowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian methods, distributionally robust optimization methods, and
regularization methods are three pillars of trustworthy machine learning
combating distributional uncertainty, e.g., the uncertainty of an empirical
distribution compared to the true underlying distribution. This paper
investigates the connections among the three frameworks and, in particular,
explores why these frameworks tend to have smaller generalization errors.
Specifically, first, we suggest a quantitative definition for "distributional
robustness", propose the concept of "robustness measure", and formalize several
philosophical concepts in distributionally robust optimization. Second, we show
that Bayesian methods are distributionally robust in the probably approximately
correct (PAC) sense; in addition, by constructing a Dirichlet-process-like
prior in Bayesian nonparametrics, it can be proven that any regularized
empirical risk minimization method is equivalent to a Bayesian method. Third,
we show that generalization errors of machine learning models can be
characterized using the distributional uncertainty of the nominal distribution
and the robustness measures of these machine learning models, which is a new
perspective to bound generalization errors, and therefore, explain the reason
why distributionally robust machine learning models, Bayesian models, and
regularization models tend to have smaller generalization errors in a unified
manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Options and State Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Energy Management Configuration Concepts from a Set of
  Pareto-optimal Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Lanfermann, Qiqi Liu, Yaochu Jin, Sebastian Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing resource efficient energy management systems in facilities and
buildings becomes increasingly important in the transformation to a sustainable
society. However, selecting a suitable configuration based on multiple,
typically conflicting objectives, such as cost, robustness with respect to
uncertainty of grid operation, or renewable energy utilization, is a difficult
multi-criteria decision making problem. The recently developed concept
identification technique can facilitate a decision maker by sorting
configuration options into semantically meaningful groups (concepts). In this
process, the partitioning of the objectives and design parameters into
different sets (called description spaces) is a very important step. In this
study we focus on utilizing the concept identification technique for finding
relevant and viable energy management configurations from a very large data set
of Pareto-optimal solutions. The data set consists of 20000 realistic
Pareto-optimal building energy management configurations generated by a
many-objective evolutionary optimization of a high quality Digital Twin energy
management simulator. We analyze how the choice of description spaces, i.e.,
the partitioning of the objectives and parameters, impacts the type of
information that can be extracted. We show that the decision maker can
introduce constraints and biases into that process to meet expectations and
preferences. The iterative approach presented in this work allows for the
generation of valuable insights into trade-offs between specific objectives,
and constitutes a powerful and flexible tool to support the decision making
process when designing large and complex energy management systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, accepted at Energy Conversion and Management: X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Poincaré Inequality and Consistency Results for Signal Sampling on
  Large Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien Le, Luana Ruiz, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale graph machine learning is challenging as the complexity of
learning models scales with the graph size. Subsampling the graph is a viable
alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.
Existing graph sampling techniques require not only computing the spectra of
large matrices but also repeating these computations when the graph changes,
e.g., grows. In this paper, we introduce a signal sampling theory for a type of
graph limit -- the graphon. We prove a Poincar\'e inequality for graphon
signals and show that complements of node subsets satisfying this inequality
are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting
connections with spectral clustering and Gaussian elimination, we prove that
such sampling sets are consistent in the sense that unique sampling sets on a
convergent graph sequence converge to unique sampling sets on the graphon. We
then propose a related graphon signal sampling algorithm for large graphs, and
demonstrate its good empirical performance on graph machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate
  Time Series <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, Alexandre Drouin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new model for multivariate probabilistic time series
prediction, designed to flexibly address a range of tasks including
forecasting, interpolation, and their combinations. Building on copula theory,
we propose a simplified objective for the recently-introduced transformer-based
attentional copulas (TACTiS), wherein the number of distributional parameters
now scales linearly with the number of variables instead of factorially. The
new objective requires the introduction of a training curriculum, which goes
hand-in-hand with necessary changes to the original architecture. We show that
the resulting model has significantly better training dynamics and achieves
state-of-the-art performance across diverse real-world forecasting tasks, while
maintaining the flexibility of prior work, such as seamless handling of
unaligned and unevenly-sampled time series. Code is made available at
https://github.com/ServiceNow/TACTiS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures, The Twelfth International Conference on
  Learning Representations (ICLR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for
  Contrastive Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotation or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExtremeCast: Boosting Extreme Value Prediction for Global Weather
  Forecast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanghan Xu, Kang Chen, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven weather forecast based on machine learning (ML) has experienced
rapid development and demonstrated superior performance in the global
medium-range forecast compared to traditional physics-based dynamical models.
However, most of these ML models struggle with accurately predicting extreme
weather, which is closely related to the extreme value prediction. Through
mathematical analysis, we prove that the use of symmetric losses, such as the
Mean Squared Error (MSE), leads to biased predictions and underestimation of
extreme values. To address this issue, we introduce Exloss, a novel loss
function that performs asymmetric optimization and highlights extreme values to
obtain accurate extreme weather forecast. Furthermore, we introduce a
training-free extreme value enhancement strategy named ExEnsemble, which
increases the variance of pixel values and improves the forecast robustness.
Combined with an advanced global weather forecast model, extensive experiments
show that our solution can achieve state-of-the-art performance in extreme
weather prediction, while maintaining the overall forecast accuracy comparable
to the top medium-range forecast models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF
  Magnetic Marker Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfan Wu, Thomas Langerak, Otmar Hilliges, Juan Zarate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking passive magnetic markers plays a vital role in advancing healthcare
and robotics, offering the potential to significantly improve the precision and
efficiency of systems. This technology is key to developing smarter, more
responsive tools and devices, such as enhanced surgical instruments, precise
diagnostic tools, and robots with improved environmental interaction
capabilities. However, traditionally, the tracking of magnetic markers is
computationally expensive due to the requirement for iterative optimization
procedures. Moreover, these methods depend on the magnetic dipole model for
their optimization function, which can yield imprecise outcomes due to the
model's significant inaccuracies when dealing with short distances between
non-spherical magnet and sensor.Our paper introduces a novel approach that
leverages neural networks to bypass these limitations, directly inferring the
marker's position and orientation to accurately determine the magnet's 5 DoF in
a single step without initial estimation. Although our method demands an
extensive supervised training phase, we mitigate this by introducing a
computationally more efficient method to generate synthetic, yet realistic data
using Finite Element Methods simulations. The benefits of fast and accurate
inference significantly outweigh the offline training preparation. In our
evaluation, we use different cylindrical magnets, tracked with a square array
of 16 sensors. We perform the sensors' reading and position inference on a
portable, neural networks-oriented single-board computer, ensuring a compact
setup. We benchmark our prototype against vision-based ground truth data,
achieving a mean positional error of 4 mm and an orientation error of 8 degrees
within a 0.2x0.2x0.15 m working volume. These results showcase our prototype's
ability to balance accuracy and compactness effectively in tracking 5 DoF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Box Embeddings for the Description Logic EL++ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11118v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11118v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OWL ontologies, whose formal semantics are rooted in Description Logic (DL),
have been widely used for knowledge representation. Similar to Knowledge Graphs
(KGs), ontologies are often incomplete, and maintaining and constructing them
has proved challenging. While classical deductive reasoning algorithms use the
precise formal semantics of an ontology to predict missing facts, recent years
have witnessed growing interest in inductive reasoning techniques that can
derive probable facts from an ontology. Similar to KGs, a promising approach is
to learn ontology embeddings in a latent vector space, while additionally
ensuring they adhere to the semantics of the underlying DL. While a variety of
approaches have been proposed, current ontology embedding methods suffer from
several shortcomings, especially that they all fail to faithfully model
one-to-many, many-to-one, and many-to-many relations and role inclusion axioms.
To address this problem and improve ontology completion performance, we propose
a novel ontology embedding method named Box$^2$EL for the DL EL++, which
represents both concepts and roles as boxes (i.e., axis-aligned
hyperrectangles), and models inter-concept relationships using a bumping
mechanism. We theoretically prove the soundness of Box$^2$EL and conduct an
extensive experimental evaluation, achieving state-of-the-art results across a
variety of datasets on the tasks of subsumption prediction, role assertion
prediction, and approximating deductive reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated license information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Early Neuron Alignment in Two-layer ReLU Networks with Small
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hancheng Min, Enrique Mallada, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of training a two-layer ReLU network for
binary classification using gradient flow with small initialization. We
consider a training dataset with well-separated input vectors: Any pair of
input data with the same label are positively correlated, and any pair with
different labels are negatively correlated. Our analysis shows that, during the
early phase of training, neurons in the first layer try to align with either
the positive data or the negative data, depending on its corresponding weight
on the second layer. A careful analysis of the neurons' directional dynamics
allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on
the time it takes for all neurons to achieve good alignment with the input
data, where $n$ is the number of data points and $\mu$ measures how well the
data are separated. After the early alignment phase, the loss converges to zero
at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer
is approximately low-rank. Numerical experiments on the MNIST dataset
illustrate our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>iclr 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample compression schemes for balls in graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémie Chalopin, Victor Chepoi, Fionn Mc Inerney, Sébastien Ratel, Yann Vaxès
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the open problems in machine learning is whether any set-family of
VC-dimension $d$ admits a sample compression scheme of size $O(d)$. In this
paper, we study this problem for balls in graphs. For a ball $B=B_r(x)$ of a
graph $G=(V,E)$, a realizable sample for $B$ is a signed subset $X=(X^+,X^-)$
of $V$ such that $B$ contains $X^+$ and is disjoint from $X^-$. A proper sample
compression scheme of size $k$ consists of a compressor and a reconstructor.
The compressor maps any realizable sample $X$ to a subsample $X'$ of size at
most $k$. The reconstructor maps each such subsample $X'$ to a ball $B'$ of $G$
such that $B'$ includes $X^+$ and is disjoint from $X^-$.
  For balls of arbitrary radius $r$, we design proper labeled sample
compression schemes of size $2$ for trees, of size $3$ for cycles, of size $4$
for interval graphs, of size $6$ for trees of cycles, and of size $22$ for
cube-free median graphs. For balls of a given radius, we design proper labeled
sample compression schemes of size $2$ for trees and of size $4$ for interval
graphs. We also design approximate sample compression schemes of size 2 for
balls of $\delta$-hyperbolic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTSUM: Relation Triple-based Interpretable Summarization with
  Multi-level Salience Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonglae Cho, Yonggi Cho, HoonJae Lee, Myungha Jang, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present RTSUM, an unsupervised summarization framework that
utilizes relation triples as the basic unit for summarization. Given an input
document, RTSUM first selects salient relation triples via multi-level salience
scoring and then generates a concise summary from the selected relation triples
by using a text-to-text language model. On the basis of RTSUM, we also develop
a web demo for an interpretable summarizing tool, providing fine-grained
interpretations with the output summary. With support for customization
options, our tool visualizes the salience for textual units at three distinct
levels: sentences, relation triples, and phrases. The codes,are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time Series Compression using Quaternion Valued Neural Networks and
  Quaternion Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Pöppelbaum, Andreas Schwung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel quaternionic time-series compression methodology where we
divide a long time-series into segments of data, extract the min, max, mean and
standard deviation of these chunks as representative features and encapsulate
them in a quaternion, yielding a quaternion valued time-series. This
time-series is processed using quaternion valued neural network layers, where
we aim to preserve the relation between these features through the usage of the
Hamilton product. To train this quaternion neural network, we derive quaternion
backpropagation employing the GHR calculus, which is required for a valid
product and chain rule in quaternion space. Furthermore, we investigate the
connection between the derived update rules and automatic differentiation. We
apply our proposed compression method on the Tennessee Eastman Dataset, where
we perform fault classification using the compressed data in two settings: a
fully supervised one and in a semi supervised, contrastive learning setting.
Both times, we were able to outperform real valued counterparts as well as two
baseline models: one with the uncompressed time-series as the input and the
other with a regular downsampling using the mean. Further, we could improve the
classification benchmark set by SimCLR-TS from 81.43% to 83.90%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Call to Reflect on Evaluation Practices for Age Estimation:
  Comparative Analysis of the State-of-the-Art and a Unified Benchmark <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Paplham, Vojtech Franc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparing different age estimation methods poses a challenge due to the
unreliability of published results stemming from inconsistencies in the
benchmarking process. Previous studies have reported continuous performance
improvements over the past decade using specialized methods; however, our
findings challenge these claims. This paper identifies two trivial, yet
persistent issues with the currently used evaluation protocol and describes how
to resolve them. We offer an extensive comparative analysis for
state-of-the-art facial age estimation methods. Surprisingly, we find that the
performance differences between the methods are negligible compared to the
effect of other factors, such as facial alignment, facial coverage, image
resolution, model architecture, or the amount of data used for pretraining. We
use the gained insights to propose using FaRL as the backbone model and
demonstrate its effectiveness on all public datasets. We make the source code
and exact data splits public on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 Camera-Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Convergence Rate Bounds for Optimization Under Power Law Spectral
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Velikanov, Dmitry Yarotsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of optimization on quadratic problems sensitively depends on the
low-lying part of the spectrum. For large (effectively infinite-dimensional)
problems, this part of the spectrum can often be naturally represented or
approximated by power law distributions, resulting in power law convergence
rates for iterative solutions of these problems by gradient-based algorithms.
In this paper, we propose a new spectral condition providing tighter upper
bounds for problems with power law optimization trajectories. We use this
condition to build a complete picture of upper and lower bounds for a wide
range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy
Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules
of learning rate and momentum. In particular, we demonstrate how an optimally
accelerated method, its schedule, and convergence upper bound can be obtained
in a unified manner for a given shape of the spectrum. Also, we provide first
proofs of tight lower bounds for convergence rates of Steepest Descent and
Conjugate Gradients under spectral power laws with general exponents. Our
experiments show that the obtained convergence bounds and acceleration
strategies are not only relevant for exactly quadratic optimization problems,
but also fairly accurate when applied to the training of neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via
  Temporal-Viewpoint Alignment <span class="chip">ACCV'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video sequences exhibit significant nuisance variations (undesired effects)
of speed of actions, temporal locations, and subjects' poses, leading to
temporal-viewpoint misalignment when comparing two sets of frames or evaluating
the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera
viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D
skeleton sequences whose camera and subjects' poses can be easily manipulated
in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where
matching well temporal blocks (temporal chunks that make up a sequence) of
support-query sequence pairs (by factoring out nuisance variations) is
essential due to limited samples of novel classes. Given a query sequence, we
create its several views by simulating several camera locations. For a support
sequence, we match it with view-simulated query sequences, as in the popular
Dynamic Time Warping (DTW). Specifically, each support temporal block can be
matched to the query temporal block with the same or adjacent (next) temporal
index, and adjacent camera views to achieve joint local temporal-viewpoint
warping. JEANIE selects the smallest distance among matching paths with
different temporal-viewpoint warping patterns, an advantage over DTW which only
performs temporal alignment. We also propose an unsupervised FSAR akin to
clustering of sequences with JEANIE as a distance measure. JEANIE achieves
state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D
Multiview Activity II on supervised and unsupervised FSAR, and their
meta-learning inspired fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Journal of Computer Vision (IJCV). An
  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was
  distinguished by the Sang Uk Lee Best Student Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. The website is
  available at https://uni-rlhf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The autoregressive neural network architecture of the Boltzmann
  distribution of pairwise interacting spins systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indaco Biazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated
exceptional results in image and language generation tasks, contributing to the
growing popularity of generative models in both scientific and commercial
applications. This work presents an exact mapping of the Boltzmann distribution
of binary pairwise interacting systems into autoregressive form. The resulting
ARNN architecture has weights and biases of its first layer corresponding to
the Hamiltonian's couplings and external fields, featuring widely used
structures such as the residual connections and a recurrent architecture with
clear physical meanings. Moreover, its architecture's explicit formulation
enables the use of statistical physics techniques to derive new ARNNs for
specific systems. As examples, new effective ARNN architectures are derived
from two well-known mean-field systems, the Curie-Weiss and
Sherrington-Kirkpatrick models, showing superior performance in approximating
the Boltzmann distributions of the corresponding physics model compared to
other commonly used architectures. The connection established between the
physics of the system and the neural network architecture provides a means to
derive new architectures for different interacting systems and interpret
existing ones from a physical perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figure plus the Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Samplet basis pursuit: Multiresolution scattered data approximation with
  sparsity constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Baroli, Helmut Harbrecht, Michael Multerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider scattered data approximation in samplet coordinates with
$\ell_1$-regularization. The application of an $\ell_1$-regularization term
enforces sparsity of the coefficients with respect to the samplet basis.
Samplets are wavelet-type signed measures, which are tailored to scattered
data. They provide similar properties as wavelets in terms of localization,
multiresolution analysis, and data compression. By using the Riesz isometry, we
embed samplets into reproducing kernel Hilbert spaces and discuss the
properties of the resulting functions. We argue that the class of signals that
are sparse with respect to the embedded samplet basis is considerably larger
than the class of signals that are sparse with respect to the basis of kernel
translates. Vice versa, every signal that is a linear combination of only a few
kernel translates is sparse in samplet coordinates. Therefore, samplets enable
the use of well-established multiresolution techniques on general scattered
data sets.
  We propose the rapid solution of the problem under consideration by combining
soft-shrinkage with the semi-smooth Newton method. Leveraging on the sparse
representation of kernel matrices in samplet coordinates, this approach
converges faster than the fast iterative shrinkage thresholding algorithm and
is feasible for large-scale data. Numerical benchmarks are presented and
demonstrate the superiority of the multiresolution approach over the
single-scale approach. As large-scale applications, the surface reconstruction
from scattered data and the reconstruction of scattered temperature data using
a dictionary of multiple kernels are considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCOST: State-Space Models for Long Document Abstractive Summarization <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models are a low-complexity alternative to transformers for
encoding long sequences and capturing long-term dependencies. We propose
LOCOST: an encoder-decoder architecture based on state-space models for
conditional text generation with long context inputs. With a computational
complexity of $O(L \log L)$, this architecture can handle significantly longer
sequences than state-of-the-art models that are based on sparse attention
patterns. We evaluate our model on a series of long document abstractive
summarization tasks. The model reaches a performance level that is 93-96%
comparable to the top-performing sparse transformers of the same size while
saving up to 50% memory during training and up to 87% during inference.
Additionally, LOCOST effectively handles input texts exceeding 600K tokens at
inference time, setting new state-of-the-art results on full-book summarization
and opening new perspectives for long input processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 7 tables, EACL 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Understanding of Gradient Bias in Meta-Reinforcement
  Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Feng, Bo Liu, Jie Ren, Luo Mai, Rui Zhu, Haifeng Zhang, Jun Wang, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level
optimisation procedures wherein the outer-loop meta-learner guides the
inner-loop gradient-based reinforcement learner to achieve fast adaptations. In
this paper, we develop a unified framework that describes variations of GMRL
algorithms and points out that existing stochastic meta-gradient estimators
adopted by GMRL are actually \textbf{biased}. Such meta-gradient bias comes
from two sources: 1) the compositional bias incurred by the two-level problem
structure, which has an upper bound of
$\mathcal{O}\big(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}\big)$
\emph{w.r.t.} inner-loop update step $K$, learning rate $\alpha$, estimate
variance $\hat{\sigma}^{2}_{\text{In}}$ and sample size $|\tau|$, and 2) the
multi-step Hessian estimation bias $\hat{\Delta}_{H}$ due to the use of
autodiff, which has a polynomial impact
$\mathcal{O}\big((K-1)(\hat{\Delta}_{H})^{K-1}\big)$ on the meta-gradient bias.
We study tabular MDPs empirically and offer quantitative evidence that
testifies our theoretical findings on existing stochastic meta-gradient
estimators. Furthermore, we conduct experiments on Iterated Prisoner's Dilemma
and Atari games to show how other methods such as off-policy learning and
low-bias estimator can help fix the gradient bias for GMRL algorithms in
general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating and Mitigating the Side Effects of Noisy Views for
  Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17245v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17245v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering (MVC) aims at exploring category structures among
multi-view data in self-supervised manners. Multiple views provide more
information than single views and thus existing MVC methods can achieve
satisfactory performance. However, their performance might seriously degenerate
when the views are noisy in practical multi-view scenarios. In this paper, we
formally investigate the drawback of noisy views and then propose a
theoretically grounded deep MVC method (namely MVCAN) to address this issue.
Specifically, we propose a novel MVC objective that enables un-shared
parameters and inconsistent clustering predictions across multiple views to
reduce the side effects of noisy views. Furthermore, a two-level multi-view
iterative optimization is designed to generate robust learning targets for
refining individual views' representation learning. Theoretical analysis
reveals that MVCAN works by achieving the multi-view consistency,
complementarity, and noise robustness. Finally, experiments on extensive public
datasets demonstrate that MVCAN outperforms state-of-the-art methods and is
robust against the existence of noisy views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Linear Time Series Forecasting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Toner, Luke Darlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Huber Loss Minimization Approach to Byzantine Robust Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Fei Yu, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Few-Shot Learning via Diffusive Neural Network
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal modeling is foundational for smart city applications, yet it
is often hindered by data scarcity in many cities and regions. To bridge this
gap, we propose a novel generative pre-training framework, GPD, for
spatio-temporal few-shot learning with urban knowledge transfer. Unlike
conventional approaches that heavily rely on common feature extraction or
intricate few-shot learning designs, our solution takes a novel approach by
performing generative pre-training on a collection of neural network parameters
optimized with data from source cities. We recast spatio-temporal few-shot
learning as pre-training a generative diffusion model, which generates tailored
neural networks guided by prompts, allowing for adaptability to diverse data
distributions and city-specific characteristics. GPD employs a
Transformer-based denoising diffusion model, which is model-agnostic to
integrate with powerful spatio-temporal neural networks. By addressing
challenges arising from data gaps and the complexity of generalizing knowledge
across cities, our framework consistently outperforms state-of-the-art
baselines on multiple real-world datasets for tasks such as traffic speed
prediction and crowd flow prediction. The implementation of our approach is
available: https://github.com/tsinghua-fib-lab/GPD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the forecast accuracy of wind power by leveraging multiple
  hierarchical structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas English, Mahdi Abolghasemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renewable energy generation is of utmost importance for global
decarbonization. Forecasting renewable energies, particularly wind energy, is
challenging due to the inherent uncertainty in wind energy generation, which
depends on weather conditions. Recent advances in hierarchical forecasting
through reconciliation have demonstrated a significant increase in the quality
of wind energy forecasts for short-term periods. We leverage the
cross-sectional and temporal hierarchical structure of turbines in wind farms
and build cross-temporal hierarchies to further investigate how integrated
cross-sectional and temporal dimensions can add value to forecast accuracy in
wind farms. We found that cross-temporal reconciliation was superior to
individual cross-sectional reconciliation at multiple temporal aggregations.
Additionally, machine learning based forecasts that were cross-temporally
reconciled demonstrated high accuracy at coarser temporal granularities, which
may encourage adoption for short-term wind forecasts. Empirically, we provide
insights for decision-makers on the best methods for forecasting high-frequency
wind data across different forecasting horizons and levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater
  Sample Efficiency and Simplicity <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1902.05605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1902.05605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficiency is a crucial problem in deep reinforcement learning. Recent
algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency
by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the
critic per environment sample. However, this comes at the expense of a greatly
increased computational cost. To reduce this computational burden, we introduce
CrossQ: A lightweight algorithm for continuous control tasks that makes careful
use of Batch Normalization and removes target networks to surpass the current
state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1.
Notably, CrossQ does not rely on advanced bias-reduction schemes used in
current methods. CrossQ's contributions are threefold: (1) it matches or
surpasses current state-of-the-art methods in terms of sample efficiency, (2)
it substantially reduces the computational cost compared to REDQ and DroQ, (3)
it is easy to implement, requiring just a few lines of code on top of SAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024. Project page at
  http://aditya.bhatts.org/CrossQ and code release at
  https://github.com/adityab/CrossQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightIt: Illumination Modeling and Control for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:25:07.871603205Z">
            2024-03-28 05:25:07 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
